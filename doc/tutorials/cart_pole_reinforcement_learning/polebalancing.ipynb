{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f763ff4e",
   "metadata": {},
   "source": [
    "# Polebalancing using NESTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c21179",
   "metadata": {},
   "source": [
    "In this tutorial, we are going to build an agent that can successfully solve the classic pole balancing problem using reinforcement learning. We will start with a standard temporal difference learning approach and after that, use NESTML to set up a spiking neural network to perform this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0885d90c",
   "metadata": {},
   "source": [
    "# Cart Pole Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9c0bfe",
   "metadata": {},
   "source": [
    "For the cart pole environment, we mostly need three things:  \n",
    "    - A renderer to display the simulation  \n",
    "    - The physics system and  \n",
    "    - An input to be able to nudge the pole in both directions  \n",
    "\n",
    "For that, we will need the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ded29bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame as pg\n",
    "from typing import Tuple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2283c79f",
   "metadata": {},
   "source": [
    "Let's start with the renderer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1af3680-b849-48bb-a653-642b580a01aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renders the scene. IMPORTANT: Because ipycanvas uses the html canvas coordinates, the y-axis is inverted.\n",
    "class Renderer():\n",
    "    def __init__(self, width: int, height: int, origin_x: int = 0, origin_y: int = 0, SCALE: int = 1) -> None:\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.origin = (origin_x, origin_y)\n",
    "        self.SCALE = SCALE #1m = SCALE pixels\n",
    "\n",
    "        pg.display.init()\n",
    "        pg.display.set_caption(\"Pole Balancing Simulator\")\n",
    "        pg.font.init()\n",
    "        self.screen = pg.display.set_mode((width, height))\n",
    "    \n",
    "    #Translates global coordinates into screen coordinates\n",
    "    def translate(self, x: int, y: int) -> Tuple[int, int]:\n",
    "        return (x+self.origin[0], -y+self.origin[1])\n",
    "    \n",
    "    #Draws ground. offset is there to shift the ground below the car\n",
    "    def draw_ground(self, offset: int, color) -> None:\n",
    "        ground = pg.Rect(self.translate(-self.width//2, -offset * self.SCALE), (self.width, self.height-self.origin[1]-offset * self.SCALE))\n",
    "        pg.draw.rect(self.screen, color, ground)\n",
    "\n",
    "    #Draws car. pos_y is omitted because the car's center should be at y = 0\n",
    "    def draw_car(self, pos_x: float, car_color = \"blue\", wheel_color = \"black\") -> None:\n",
    "        pos_x *= self.SCALE\n",
    "        #values, hard-coded for now, in meters\n",
    "        width = 0.5 * self.SCALE\n",
    "        height = 0.25 * self.SCALE\n",
    "        wheel_radius = 0.1 * self.SCALE\n",
    "\n",
    "        car_body = pg.Rect(self.translate(pos_x - width/2, height/2), (width, height))\n",
    "        pg.draw.rect(self.screen, car_color, car_body)\n",
    "        pg.draw.circle(self.screen, wheel_color, \n",
    "                           self.translate(pos_x - width/2 + wheel_radius, -height/2), wheel_radius)\n",
    "        pg.draw.circle(self.screen, wheel_color, \n",
    "                           self.translate(pos_x + width/2 - wheel_radius, -height/2), wheel_radius)\n",
    "\n",
    "    #Draws the pole\n",
    "    def draw_pole(self, pos_x: float, theta: float, length: float, width: float = 0.1, color = \"red\") -> None:\n",
    "        pos_x *= self.SCALE\n",
    "        width = int(width * self.SCALE)\n",
    "        pole_end_x = length * np.sin(theta) * self.SCALE + pos_x\n",
    "        pole_end_y = length * np.cos(theta) * self.SCALE\n",
    "        pg.draw.line(self.screen, color, self.translate(pos_x, 0), self.translate(pole_end_x, pole_end_y), width)\n",
    "\n",
    "    #Clears the entire canvas\n",
    "    def draw_clear(self) -> None:\n",
    "        self.screen.fill(\"white\")\n",
    "\n",
    "    #Draws physical values\n",
    "    def draw_stats(self, theta: float, dw: float, a: float, x: float, episode: int) -> None:\n",
    "        font = pg.font.Font(None, 24)\n",
    "        text = font.render(str(theta)[:4] + \" | \" + str(dw)[:4] + \" | \" + str(x)[:4] + \" | \" + str(a)[:4] + \" | episode: \" + str(episode), True, (10,10,10))\n",
    "        textpos = text.get_rect(centerx=self.screen.get_width() / 2, y=10)\n",
    "        self.screen.blit(text, textpos)\n",
    "\n",
    "    #Get the \n",
    "    def get_relative_mouse_x(self, mouse_x:float) -> float:\n",
    "        return (mouse_x-self.origin[0])/self.SCALE\n",
    "    \n",
    "    def display(self) -> None:\n",
    "        pg.display.flip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6362d90",
   "metadata": {},
   "source": [
    "## Physics Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74907eaf",
   "metadata": {},
   "source": [
    "For the physics, we use the corrected version of of the original problem derived from V. Florian (CITATION NEEDED), but omit the friction forces.\n",
    "The situation is sketched here:  \n",
    "\n",
    "![alt text](cartpole_illustration.png \"Cartpole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9c94e0",
   "metadata": {},
   "source": [
    "We apply Newton's second law of motion to the cart:  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\mathbf{F} + \\mathbf{G}_c - \\mathbf{N} = m_c \\cdot \\mathbf{a}_c\n",
    "\\end{aligned}\n",
    "$$\n",
    "Where:  \n",
    "\n",
    "$\\mathbf{F} = F \\cdot \\mathbf{u_x}$ is the control force acting on the cart,  \n",
    "$\\mathbf{G}_c = m_c \\cdot g \\cdot \\mathbf{u}_y$ is the gravitational component acting on the cart,  \n",
    "$\\mathbf{N} = N_x \\cdot \\mathbf{u}_x - N_y \\cdot \\mathbf{u}_y$ is the negative reaction force that the pole is applying on the cart,  \n",
    "$\\mathbf{a}_c = \\ddot{x} \\cdot \\mathbf{u}_x$ is the accelaration of the cart,  \n",
    "$m_c$ is the cart's mass and  \n",
    "$\\mathbf{u}_x$, $\\mathbf{u}_y$, $\\mathbf{u}_z$ are the unit vectors of the frame of reference given in the illustration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde2e429",
   "metadata": {},
   "source": [
    "We can decompose this equation now into the $x$ and $y$ component:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    F - N_x = m_c \\cdot \\ddot{x}\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    m_c \\cdot g + N_y = 0\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62acf48",
   "metadata": {},
   "source": [
    "Newton's second law of motion applied to the pole gives us:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\mathbf{N} + \\mathbf{G}_p = m_p \\cdot \\mathbf{a}_p\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where $\\mathbf{G}_p = m_p \\cdot g \\cdot \\mathbf{u}_y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38770100",
   "metadata": {},
   "source": [
    "The accelaration $\\mathbf{a}_p$ of the pole's center of mass consists of three components, where $\\mathbf{r}_p = l \\cdot (\\sin{\\theta}\\cdot \\mathbf{u}_x-\\cos{\\theta}\\cdot \\mathbf{u}_y)$ denotes the vector pointing to the pole's center of mass relative to it's rotation center:  \n",
    "1. The accelaration of the cart it is attached to $\\mathbf{a}_c$,\n",
    "2. The pole's angular accelaration $\\mathbf{\\epsilon} = \\ddot{\\theta} \\cdot \\mathbf{u}_z$, which is translated into accelaration by $\\mathbf{\\epsilon} \\times \\mathbf{r}_p$.\n",
    "3. The pole's angular velocity $\\mathbf{\\omega} = \\dot{\\theta} \\cdot \\mathbf{u}_z$, for which the accelaration can be derived by  $\\mathbf{\\omega} \\times (\\mathbf{\\omega} \\times \\mathbf{r}_p)$.\n",
    "\n",
    "Thus we obtain:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\mathbf{a}_p  = \\mathbf{a}_c + \\mathbf{\\epsilon} \\times \\mathbf{r}_p + \\mathbf{\\omega} \\times (\\mathbf{\\omega} \\times \\mathbf{r}_p)\n",
    "\\end{aligned}\n",
    "$$\n",
    "Substituting $\\mathbf{r}_p = l \\cdot (\\sin{\\theta}\\cdot \\mathbf{u}_x-\\cos{\\theta}\\cdot \\mathbf{u}_y)$ and $\\mathbf{a}_p = \\ddot{x} \\cdot \\mathbf{u}_x$ as well as $\\mathbf{u}_z \\times \\mathbf{u}_x = \\mathbf{u}_y$ and $\\mathbf{u}_z \\times \\mathbf{u}_y = -\\mathbf{u}_x$:\n",
    "\\begin{aligned}\n",
    "    \\mathbf{a}_p  = \\ddot{x} \\cdot \\mathbf{u}_x + l \\cdot \\ddot{\\theta} \\cdot (\\sin{\\theta}\\cdot \\mathbf{u}_y + \\cos{\\theta}\\cdot \\mathbf{u}_x) - l \\cdot \\dot{\\theta}^2 \\cdot (\\sin{\\theta}\\cdot \\mathbf{u}_x - \\cos{\\theta}\\cdot \\mathbf{u}_y)\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895254eb",
   "metadata": {},
   "source": [
    "Inserting this quation into our equation for the forces of the pole and decomposing on the $x$ and $y$ axis we obtain:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    N_x = m_p \\cdot (\\ddot{x} + l \\cdot \\ddot{\\theta} \\cdot \\cos{\\theta} - l \\cdot \\dot{\\theta}^2 \\cdot \\sin{\\theta})\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    m_p \\cdot g - N_y = m_p \\cdot (l \\cdot \\ddot{\\theta} \\cdot \\sin{\\theta} + l \\cdot \\dot{\\theta}^2 \\cdot \\cos{\\theta})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376c6dd1",
   "metadata": {},
   "source": [
    "# TODO: FINISH EQUATION DERIVATION (SOLVE EQUATION REFERENCING?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b5f4227",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Physics():\n",
    "    \n",
    "    def __init__(self, x, theta, v = 0, a = 0, w = 0, dw = 0, g = 9.81, m_c = 1, m_p = 0.1, l = 0.5, dt = 0.02) -> None:\n",
    "        self.__dict__.update(vars())\n",
    "\n",
    "    def dw_step(self, cart_force, nudge_force) -> float:\n",
    "        numerator = self.g * np.sin(self.theta) + np.cos(self.theta) * (-cart_force - self.m_p * self.l * self.w**2 * np.sin(self.theta))/(self.m_c+self.m_p) + nudge_force * np.cos(self.theta)/(self.m_p*self.l)\n",
    "        denominator = self.l * (4/3 - (self.m_p*np.cos(self.theta)**2)/(self.m_c+self.m_p))\n",
    "\n",
    "        self.dw = numerator/denominator\n",
    "        self.w += self.dt * self.dw\n",
    "        self.theta += self.dt * self.w\n",
    "\n",
    "        return self.theta\n",
    "    \n",
    "    def a_step(self, force) -> float:\n",
    "        numerator = force + self.m_p * self.l * (self.w**2 * np.sin(self.theta) - self.dw * np.cos(self.theta))\n",
    "        denominator = self.m_c + self.m_p\n",
    "\n",
    "        self.a = numerator/denominator\n",
    "        self.v += self.dt * self.a\n",
    "        self.x += self.dt * self.v\n",
    "\n",
    "        return self.x\n",
    "\n",
    "    def update(self, force, mouse_x) -> Tuple[float, float]:\n",
    "        nudge_force = 0\n",
    "        if mouse_x is not None:\n",
    "            nudge_force = -1 if mouse_x > self.x else 1\n",
    "        return (self.dw_step(force, nudge_force), self.a_step(force))\n",
    "    \n",
    "    #get state of the system that agent can see\n",
    "    def get_state(self) -> Tuple[float,float,float,float]:\n",
    "        return (self.x, self.theta, self.v, self.w)\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        self.x = 0\n",
    "        self.theta = (np.random.rand() - 1) / 10\n",
    "        self.v = 0\n",
    "        self.a = 0\n",
    "        self.w = 0\n",
    "        self.dw = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76793d12",
   "metadata": {},
   "source": [
    "# The Agent (BOXES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5707ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Agent:\n",
    "    def __init__(self, initial_state: Tuple[float,float,float,float]) -> None:\n",
    "\n",
    "        #thresholds for discretizing the state space\n",
    "        self.x_thresholds = np.array([-2.4, -0.8, 0.8, 2.4])\n",
    "        self.theta_thresholds = np.array([-12, -6, -1, 0, 1, 6, 12])\n",
    "        self.theta_thresholds = self.theta_thresholds /180 * np.pi\n",
    "        self.v_thresholds = np.array([float(\"-inf\"), -0.5, 0.5, float(\"+inf\")]) #open intervals ignored here\n",
    "        self.w_thresholds = np.array([float(\"-inf\"), -50, 50, float(\"+inf\")]) #open intervals ignored here\n",
    "        self.w_thresholds = self.w_thresholds /180 * np.pi\n",
    "\n",
    "        self.dimensions = (len(self.x_thresholds) - 1, len(self.theta_thresholds) - 1, len(self.v_thresholds) - 1, len(self.w_thresholds) - 1)\n",
    "\n",
    "        self.boxes = np.random.rand(self.dimensions[0], \n",
    "                                    self.dimensions[1], \n",
    "                                    self.dimensions[2], \n",
    "                                    self.dimensions[3], \n",
    "                                    2) #one q-value for left and right respectively\n",
    "        box = self.get_box(initial_state)\n",
    "        self.current_box = self.boxes[box[0], box[1], box[2], box[3], :]\n",
    "\n",
    "        self.episode = 1\n",
    "    \n",
    "    def discretize(self, value, thresholds):\n",
    "        for i, limit in enumerate(thresholds):\n",
    "            if value < limit:\n",
    "                return i - 1\n",
    "        return -1\n",
    "\n",
    "    def get_box(self, state: Tuple[float,float,float,float]) -> Tuple[int,int,int,int]:\n",
    "        return (self.discretize(state[0], self.x_thresholds),\n",
    "                 self.discretize(state[1], self.theta_thresholds),\n",
    "                 self.discretize(state[2], self.v_thresholds), \n",
    "                 self.discretize(state[3], self.w_thresholds))\n",
    "    \n",
    "    def get_episode(self) -> int:\n",
    "        return self.episode\n",
    "    \n",
    "    \n",
    "    def failure_reset(self, state: Tuple[float,float,float,float]):\n",
    "        box = self.get_box(state)\n",
    "        self.current_box = self.boxes[box[0], box[1], box[2], box[3], :]\n",
    "        self.episode += 1\n",
    "\n",
    "\n",
    "class NonSpikingAgent(Agent):\n",
    "    def __init__(self, initial_state: Tuple[float,float,float,float], learning_rate, learning_decay, epsilon, epsilon_decay, discount_factor) -> None:\n",
    "        super().__init__(initial_state)\n",
    "\n",
    "        #learning paramters\n",
    "        self.learning_rate = learning_rate\n",
    "        self. learning_decay = learning_decay\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "    #returns 0 if the action is \"left\", else \"1\"\n",
    "    def choose_action(self) -> int:\n",
    "        self.action = np.random.choice([np.argmax(self.current_box), np.argmin(self.current_box)], p=[1-self.epsilon, self.epsilon])\n",
    "        return self.action\n",
    "    \n",
    "    #returns 0 if no failure occured, else 1\n",
    "    #reward is -1 on failure and 0 else\n",
    "    def update(self, next_state: Tuple[float,float,float,float]) -> int:\n",
    "        box = self.get_box(next_state)\n",
    "        if -1 in box:\n",
    "            self.current_box[self.action] += self.learning_rate * -1\n",
    "            return 1\n",
    "        \n",
    "        next_box = self.boxes[box[0], box[1], box[2], box[3], :]\n",
    "        next_q = np.max(next_box)\n",
    "        self.current_box[self.action] += self.learning_rate * (self.discount_factor * (next_q - self.current_box[self.action]))\n",
    "\n",
    "        self.current_box = next_box\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.learning_rate *= self.learning_decay\n",
    "\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2e6cd3",
   "metadata": {},
   "source": [
    "# Plot Renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9b07a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "class PlotRenderer():\n",
    "    def __init__(self, init_x = [0], init_y = [0]) -> None:\n",
    "        plt.ion()\n",
    "        #Construct lifetime plot\n",
    "        self.lifetime_fig, self.lifetime_ax = plt.subplots()\n",
    "        self.x = init_x\n",
    "        self.y = init_y\n",
    "        self.max_lifetime = 0\n",
    "        self.line, = self.lifetime_ax.plot(self.x, self.y)\n",
    "        self.lifetime_ax.set_xlabel(\"Episode\")\n",
    "        self.lifetime_ax.set_ylabel(\"Simulation Steps\")\n",
    "        self.lifetime_ax.set_title(\"Lifetime Plot\")\n",
    "\n",
    "        #Construct Heatmap for two parameters\n",
    "        self.q_value_fig, self.q_value_ax = plt.subplots()\n",
    "        self.q_value_ax.set_title(\"Q-Values for a state of (param1/param2)\")\n",
    "        self.cmap = plt.cm.coolwarm\n",
    "        \n",
    "    def average_values(self, boxes):\n",
    "        res = []\n",
    "    def update(self, x, y, boxes) -> None:\n",
    "        print(x)\n",
    "        self.x.append(x)\n",
    "        self.y.append(y)\n",
    "        self.max_lifetime = max(self.max_lifetime, y)\n",
    "        self.line.set_data(self.x, self.y)\n",
    "        self.lifetime_ax.set_xlim(self.x[0], self.x[-1])\n",
    "        self.lifetime_ax.set_ylim(0, self.max_lifetime)\n",
    "\n",
    "        if(x % 10 == 0):\n",
    "            q_values = boxes[:,:,:,:,0] - boxes[:,:,:,:,1]\n",
    "            self.q_value_ax.imshow(np.mean(q_values, axis = (2,3)), cmap=plt.cm.coolwarm, interpolation='none')\n",
    "\n",
    "        plt.draw\n",
    "        plt.pause(0.0001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd7786b",
   "metadata": {},
   "source": [
    "# Executing Non-Spiking-Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717eda26-e385-494f-bdca-9847eefe01ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "r = Renderer(1200, 800, 600, 500, 400)\n",
    "clock = pg.time.Clock()\n",
    "running = True\n",
    "\n",
    "p = Physics(0, (np.random.rand() - 1) / 10)\n",
    "\n",
    "a = NonSpikingAgent(p.get_state(), 0.5, 0.9999999999999, 1, 0.995, 0.99)\n",
    "\n",
    "plot = PlotRenderer()\n",
    "\n",
    "steps_per_episode = 0\n",
    "max_steps = 0\n",
    "\n",
    "window_size = 30\n",
    "window = np.zeros(30)\n",
    "avg_lifetime = 20000\n",
    "\n",
    "toggle_sim = False\n",
    "\n",
    "while running:\n",
    "    steps_per_episode += 1\n",
    "\n",
    "    force = 0\n",
    "    mouse_x = None\n",
    "\n",
    "    # poll for events\n",
    "    for event in pg.event.get():\n",
    "        if event.type == pg.QUIT:\n",
    "            running = False\n",
    "            pg.quit()\n",
    "            sys.exit()\n",
    "            quit()\n",
    "        elif event.type == pg.MOUSEBUTTONDOWN:\n",
    "            mouse_x = r.get_relative_mouse_x(pg.mouse.get_pos()[0])\n",
    "        elif event.type == pg.KEYDOWN:\n",
    "            toggle_sim ^= pg.key.get_pressed()[pg.K_SPACE]\n",
    "\n",
    "    # agent chooses action, simulation is uodated and reward is calculated\n",
    "    force = 10 if a.choose_action() else -10\n",
    "    theta, x = p.update(force, mouse_x)\n",
    "    failure = a.update(p.get_state())\n",
    "\n",
    "    if failure:\n",
    "        p.reset()\n",
    "        a.failure_reset(p.get_state())\n",
    "        plot.update(a.get_episode(), steps_per_episode, a.boxes)\n",
    "        window = np.roll(window, 1)\n",
    "        window[0] = steps_per_episode\n",
    "        steps_per_episode = 0\n",
    "    \n",
    "    \n",
    "    if np.mean(window) >= avg_lifetime or toggle_sim:\n",
    "        r.draw_clear()\n",
    "        r.draw_ground(0.2, \"grey\")\n",
    "        r.draw_car(x)\n",
    "        r.draw_pole(x, theta, 2*p.l, 0.02)\n",
    "        r.draw_stats(theta*180/np.pi, p.w*180/np.pi, x, p.a, a.get_episode())\n",
    "        r.display()\n",
    "\n",
    "        clock.tick(50)  # limits FPS to 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2876351",
   "metadata": {},
   "source": [
    "# TODO: clean up code, derive equations and explain renderer briefly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e05060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43c70dff",
   "metadata": {},
   "source": [
    "# Spiking version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e16ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... generate NESTML model code...\n",
    "\n",
    "from pynestml.codegeneration.nest_code_generator_utils import NESTCodeGeneratorUtils\n",
    "\n",
    "# generate and build code\n",
    "input_layer_module_name, input_layer_neuron_model_name = \\\n",
    "   NESTCodeGeneratorUtils.generate_code_for(\"../../../models/neurons/ignore_and_fire_neuron.nestml\")\n",
    "\n",
    "# ignore_and_fire\n",
    "output_layer_module_name, output_layer_neuron_model_name, output_layer_synapse_model_name = \\\n",
    "    NESTCodeGeneratorUtils.generate_code_for(\"../../../models/neurons/iaf_psc_exp_neuron.nestml\",\n",
    "                                             \"../../../models/synapses/neuromodulated_stdp_synapse.nestml\",\n",
    "                                             post_ports=[\"post_spikes\"],\n",
    "                                             mod_ports=[\"mod_spikes\"],\n",
    "                                             logging_level=\"DEBUG\",\n",
    "                                             codegen_opts={\"delay_variable\": {\"neuromodulated_stdp_synapse\": \"d\"},\n",
    "                                                           \"weight_variable\": {\"neuromodulated_stdp_synapse\": \"w\"}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99614c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest\n",
    "\n",
    "\n",
    "class SpikingAgent(Agent):\n",
    "    cycle_period = 40.   # [ms]\n",
    "    \n",
    "    def __init__(self, initial_state: Tuple[float,float,float,float], gamma) -> None:\n",
    "        super().__init__(initial_state)\n",
    "        self.gamma = gamma\n",
    "        self.construct_neural_network()\n",
    "    \n",
    "    def get_state_neuron(self, state) -> int:\n",
    "        idx = 0\n",
    "        thresholds = [self.x_thresholds, self.theta_thresholds, self.v_thresholds, self.w_thresholds]\n",
    "        for dim, val, thresh in zip(self.dimensions, state, thresholds):\n",
    "            i = self.discretize(val,thresh)\n",
    "            if i == -1: return -1\n",
    "            idx = idx * dim + i\n",
    "\n",
    "        return idx\n",
    "    \n",
    "    def construct_neural_network(self):\n",
    "        nest.ResetKernel()\n",
    "        nest.Install(input_layer_module_name)   # makes the generated NESTML model available\n",
    "        nest.Install(output_layer_module_name)   # makes the generated NESTML model available\n",
    "        \"\"\"\n",
    "        #PROBLEM: NEST likes to use two NodeCollection objects\n",
    "        #perhaps just use an equation to derive the correct index?\n",
    "        self.input_population = np.empty((len(self.x_thresholds) - 1, \n",
    "                                          len(self.theta_thresholds) - 1, \n",
    "                                          len(self.v_thresholds) - 1, \n",
    "                                          len(self.w_thresholds) - 1))\n",
    "        \n",
    "        for idx, _ in np.ndenumerate(self.input_population):\n",
    "            self.input_population[idx[0], idx[1], idx[2], idx[3]] = nest.Create(neuron_model_name)\n",
    "        \"\"\"\n",
    "        input_size = self.dimensions[0] * self.dimensions[1] * self.dimensions[2] * self.dimensions[3]\n",
    "        \n",
    "        self.volume_transmitter = nest.Create(\"volume_transmitter\")\n",
    "        nest.CopyModel(output_layer_synapse_model_name, \"stdp_dopa_nestml\",\n",
    "                        {\"volume_transmitter\": vt})\n",
    "\n",
    "        self.output_population_left = nest.Create(output_layer_neuron_model_name, 10)\n",
    "        self.output_population_right = nest.Create(output_layer_neuron_model_name, 10)\n",
    "        \n",
    "        nest.Connect(self.input_population, self.output_population_left, syn_spec={\"synapse_model\": output_layer_synapse_model_name})\n",
    "        nest.Connect(self.input_population, self.output_population_right, syn_spec={\"synapse_model\": output_layer_synapse_model_name})\n",
    "\n",
    "        self.output_population_spike_recorder_left = nest.Create(\"spike_recorder\")\n",
    "        nest.Connect(self.output_population_left, self.output_population_spike_recorder_left, record_to=\"\")\n",
    "\n",
    "        self.output_population_spike_recorder_right = nest.Create(\"spike_recorder\", record_to=\"\")\n",
    "        nest.Connect(self.output_population_right, self.output_population_spike_recorder_right)\n",
    "\n",
    "    #returns 0 if the action is \"left\", else \"1\"\n",
    "    def choose_action(self) -> int:\n",
    "        if self.Q_left > self.Q_right:\n",
    "            self.action = 0   # left\n",
    "        else:\n",
    "            self.action = 1   # right\n",
    "        \n",
    "        return self.action\n",
    "    \n",
    "    #returns 0 if the action is \"left\", else \"1\"\n",
    "    def choose_last_taken_action(self) -> int:\n",
    "        if self.Q_left_prev > self.Q_right_prev:\n",
    "            return 0   # left\n",
    "        else:\n",
    "            return 1   # right\n",
    "\n",
    "    def update(self, next_state: Tuple[float,float,float,float]):\n",
    "        box = self.get_box(next_state)  \n",
    "        \n",
    "        # make the correct input neuron fire\n",
    "        self.input_population.firing_rate = 0.\n",
    "        self.input_population[box[0], box[1], box[2], box[3]].firing_rate = 100. # XXX: value not given in Liu&Pan. Got 500 Hz as max freq from BVogler thesis. n.b. 40 ms cycle time. \n",
    "\n",
    "        # simulate for one cycle\n",
    "        nest.Simulate(SpikingAgent.cycle_period)\n",
    "        \n",
    "        # measure output layer spikes\n",
    "        #output_layer_spike_times_left = self.output_population_spike_recorder_left.get(\"events\")[\"times\"]\n",
    "        #output_layer_spike_times_right = self.output_population_spike_recorder_right.get(\"events\")[\"times\"]\n",
    "        # TODO measure the number of spikes in the last cycle\n",
    "        \"\"\"\n",
    "        n_events\n",
    "        The number of events that were collected by the recorder can be read out of the n_events entry. \n",
    "        The number of events can be reset to 0. Other values cannot be set.\n",
    "\n",
    "        record_to\n",
    "        A string (default: “memory”) containing the name of the recording backend where to write data to. \n",
    "        An empty string turns all recording of individual events off.\n",
    "        \"\"\"\n",
    "        n_output_spikes_left = self.output_population_left.n_events\n",
    "        n_output_spikes_right = self.output_population_right.n_events\n",
    "        self.output_population_left.n_events = 0\n",
    "        self.output_population_right.n_events = 0\n",
    "        \n",
    "        self.Q_left_prev = self.Q_left\n",
    "        self.Q_right_prev = self.Q_right\n",
    "        self.Q_left = self.scale_n_output_spikes_to_Q_value * n_output_spikes_left\n",
    "        self.Q_right = self.scale_n_output_spikes_to_Q_value * n_output_spikes_right\n",
    "\n",
    "        # set new dopamine concentration on the synapses\n",
    "        if failure:\n",
    "            R = 0.\n",
    "            #what would we mean by that? negative dopamine is biologically inaccurate\n",
    "            TD_left = -self.Q_left\n",
    "            TD_right = -self.Q_right\n",
    "        else:\n",
    "            R = 1.\n",
    "            TD_left = self.gamma * self.Q_left + R - self.Q_left_prev\n",
    "            TD_right = self.gamma * self.Q_right + R - self.Q_right_prev\n",
    "\n",
    "        if self.choose_last_taken_action() == 0:\n",
    "            syn = nest.GetConnections(from=self.input_population, to=self.output_population_left)  # XXX: TODO: grab only synapses from input pop to left Q value group\n",
    "            syn.n = TD_left\n",
    "        else:\n",
    "            syn = nest.GetConnections(from=self.input_population, to=self.output_population_right)  # XXX: TODO: grab only synapses from input pop to right Q value group\n",
    "            syn.n = TD_right\n",
    "        \n",
    "        return failure\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566fa9c9",
   "metadata": {},
   "source": [
    "# Executing spiking version\n",
    "\n",
    "The main loop looks like this: for every iteration of the loop (for every \"cycle\" or \"step\"):\n",
    "\n",
    "- set the rate of the input neurons to the current state of the system\n",
    "- run the SNN with this input state s_n for a period of time (cycle time, in BVogler's thesis: 40 ms)\n",
    "- obtain the Q(sn, a) values, by counting nr of spikes in output population over this cycle period\n",
    "- choose action $a_n$ on the basis of Q-values\n",
    "- run the environment for the same cycle time (40 ms) to obtain next state $s_{n+1}$\n",
    "- compute reward on the basis of the last taken action (????)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bda6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "r = Renderer(1200, 800, 600, 500, 400)\n",
    "clock = pg.time.Clock()\n",
    "running = True\n",
    "\n",
    "p = Physics(0, (np.random.rand() - 1) / 10)\n",
    "\n",
    "a = SpikingAgent(p.get_state(), 0.5, 0.9999, 1, 0.995, 0.99)\n",
    "\n",
    "plt.ion()  # turning interactive mode on\n",
    "# preparing the data\n",
    "y_plot = [0]\n",
    "x_plot = [0]\n",
    "\n",
    "# plotting the first frame\n",
    "graph = plt.plot(x_plot,y_plot)[0]\n",
    "plt.pause(1)\n",
    "\n",
    "steps_per_episode = 0\n",
    "max_steps = 0\n",
    "\n",
    "while running:\n",
    "    steps_per_episode += 1\n",
    "\n",
    "    force = 0\n",
    "    mouse_x = None\n",
    "\n",
    "    # poll for events\n",
    "    for event in pg.event.get():\n",
    "        if event.type == pg.QUIT:\n",
    "            running = False\n",
    "            pg.quit()\n",
    "            sys.exit()\n",
    "            quit()\n",
    "        elif event.type == pg.MOUSEBUTTONDOWN:\n",
    "            mouse_x = r.get_relative_mouse_x(pg.mouse.get_pos()[0])\n",
    "\n",
    "    # agent chooses action, simulation is uodated and reward is calculated\n",
    "    force = 10 if a.choose_action() else -10\n",
    "    theta, x = p.update(force, mouse_x)\n",
    "    failure = a.update(p.get_state())\n",
    "\n",
    "    if failure:\n",
    "        p.reset()\n",
    "        a.failure_reset(p.get_state())\n",
    "\n",
    "        if steps_per_episode > max_steps:\n",
    "            max_steps = steps_per_episode\n",
    "        y_plot.append(steps_per_episode)\n",
    "        x_plot.append(a.get_episode())\n",
    "        \n",
    "        # removing the older graph\n",
    "        graph.remove()\n",
    "        \n",
    "        # plotting newer graph\n",
    "        graph = plt.plot(x_plot,y_plot,color = 'g')[0]\n",
    "        plt.xlim(x_plot[0], x_plot[-1])\n",
    "        plt.ylim(0, max_steps)\n",
    "        # calling pause function to let it draw the graoh in between episodes\n",
    "        plt.pause(0.0001)\n",
    "\n",
    "        steps_per_episode = 0\n",
    "    \n",
    "    \n",
    "    if a.get_episode() > 1000:\n",
    "        r.draw_clear()\n",
    "        r.draw_ground(0.2, \"grey\")\n",
    "        r.draw_car(x)\n",
    "        r.draw_pole(x, theta, 2*p.l, 0.02)\n",
    "        r.draw_stats(theta*180/np.pi, p.w*180/np.pi, x, p.a, a.get_episode())\n",
    "        r.display()\n",
    "\n",
    "        clock.tick(50)  # limits FPS to 50\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
