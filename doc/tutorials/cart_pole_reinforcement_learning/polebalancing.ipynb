{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f763ff4e",
   "metadata": {},
   "source": [
    "# Polebalancing using NESTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c21179",
   "metadata": {},
   "source": [
    "In this tutorial, we are going to build an agent that can successfully solve the classic pole balancing problem using reinforcement learning. We will start with a standard temporal difference learning approach and after that, use NESTML to set up a spiking neural network to perform this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0885d90c",
   "metadata": {},
   "source": [
    "# Cart Pole Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9c0bfe",
   "metadata": {},
   "source": [
    "For the cart pole environment, we mostly need three things:  \n",
    "    - A renderer to display the simulation  \n",
    "    - The physics system and  \n",
    "    - An input to be able to nudge the pole in both directions  \n",
    "\n",
    "For that, we will need the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ded29bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame as pg\n",
    "from typing import Tuple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2283c79f",
   "metadata": {},
   "source": [
    "Let's start with the renderer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1af3680-b849-48bb-a653-642b580a01aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renders the scene. IMPORTANT: Because ipycanvas uses the html canvas coordinates, the y-axis is inverted.\n",
    "class Renderer():\n",
    "    def __init__(self, width: int, height: int, origin_x: int = 0, origin_y: int = 0, SCALE: int = 1) -> None:\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.origin = (origin_x, origin_y)\n",
    "        self.SCALE = SCALE #1m = SCALE pixels\n",
    "\n",
    "        pg.display.init()\n",
    "        pg.display.set_caption(\"Pole Balancing Simulator\")\n",
    "        pg.font.init()\n",
    "        self.screen = pg.display.set_mode((width, height))\n",
    "    \n",
    "    #Translates global coordinates into screen coordinates\n",
    "    def translate(self, x: int, y: int) -> Tuple[int, int]:\n",
    "        return (x+self.origin[0], -y+self.origin[1])\n",
    "    \n",
    "    #Draws ground. offset is there to shift the ground below the car\n",
    "    def draw_ground(self, offset: int, color) -> None:\n",
    "        ground = pg.Rect(self.translate(-self.width//2, -offset * self.SCALE), (self.width, self.height-self.origin[1]-offset * self.SCALE))\n",
    "        pg.draw.rect(self.screen, color, ground)\n",
    "\n",
    "    #Draws car. pos_y is omitted because the car's center should be at y = 0\n",
    "    def draw_car(self, pos_x: float, car_color = \"blue\", wheel_color = \"black\") -> None:\n",
    "        pos_x *= self.SCALE\n",
    "        #values, hard-coded for now, in meters\n",
    "        width = 0.5 * self.SCALE\n",
    "        height = 0.25 * self.SCALE\n",
    "        wheel_radius = 0.1 * self.SCALE\n",
    "\n",
    "        car_body = pg.Rect(self.translate(pos_x - width/2, height/2), (width, height))\n",
    "        pg.draw.rect(self.screen, car_color, car_body)\n",
    "        pg.draw.circle(self.screen, wheel_color, \n",
    "                           self.translate(pos_x - width/2 + wheel_radius, -height/2), wheel_radius)\n",
    "        pg.draw.circle(self.screen, wheel_color, \n",
    "                           self.translate(pos_x + width/2 - wheel_radius, -height/2), wheel_radius)\n",
    "\n",
    "    #Draws the pole\n",
    "    def draw_pole(self, pos_x: float, theta: float, length: float, width: float = 0.1, color = \"red\") -> None:\n",
    "        pos_x *= self.SCALE\n",
    "        width = int(width * self.SCALE)\n",
    "        pole_end_x = length * np.sin(theta) * self.SCALE + pos_x\n",
    "        pole_end_y = length * np.cos(theta) * self.SCALE\n",
    "        pg.draw.line(self.screen, color, self.translate(pos_x, 0), self.translate(pole_end_x, pole_end_y), width)\n",
    "\n",
    "    #Clears the entire canvas\n",
    "    def draw_clear(self) -> None:\n",
    "        self.screen.fill(\"white\")\n",
    "\n",
    "    #Draws physical values\n",
    "    def draw_stats(self, theta: float, dw: float, a: float, x: float, episode: int) -> None:\n",
    "        font = pg.font.Font(None, 24)\n",
    "        text = font.render(str(theta)[:4] + \" | \" + str(dw)[:4] + \" | \" + str(x)[:4] + \" | \" + str(a)[:4] + \" | episode: \" + str(episode), True, (10,10,10))\n",
    "        textpos = text.get_rect(centerx=self.screen.get_width() / 2, y=10)\n",
    "        self.screen.blit(text, textpos)\n",
    "\n",
    "    #Get the \n",
    "    def get_relative_mouse_x(self, mouse_x:float) -> float:\n",
    "        return (mouse_x-self.origin[0])/self.SCALE\n",
    "    \n",
    "    def display(self) -> None:\n",
    "        pg.display.flip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6362d90",
   "metadata": {},
   "source": [
    "## Physics Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74907eaf",
   "metadata": {},
   "source": [
    "For the physics, we use the corrected version of of the original problem derived from V. Florian (CITATION NEEDED), but omit the friction forces.\n",
    "The situation is sketched here:  \n",
    "\n",
    "![alt text](cartpole_illustration.png \"Cartpole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9c94e0",
   "metadata": {},
   "source": [
    "We apply Newton's second law of motion to the cart:  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\mathbf{F} + \\mathbf{G}_c - \\mathbf{N} = m_c \\cdot \\mathbf{a}_c\n",
    "\\end{aligned}\n",
    "$$\n",
    "Where:  \n",
    "\n",
    "$\\mathbf{F} = F \\cdot \\mathbf{u_x}$ is the control force acting on the cart,  \n",
    "$\\mathbf{G}_c = m_c \\cdot g \\cdot \\mathbf{u}_y$ is the gravitational component acting on the cart,  \n",
    "$\\mathbf{N} = N_x \\cdot \\mathbf{u}_x - N_y \\cdot \\mathbf{u}_y$ is the negative reaction force that the pole is applying on the cart,  \n",
    "$\\mathbf{a}_c = \\ddot{x} \\cdot \\mathbf{u}_x$ is the accelaration of the cart,  \n",
    "$m_c$ is the cart's mass and  \n",
    "$\\mathbf{u}_x$, $\\mathbf{u}_y$, $\\mathbf{u}_z$ are the unit vectors of the frame of reference given in the illustration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde2e429",
   "metadata": {},
   "source": [
    "We can decompose this equation now into the $x$ and $y$ component:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    F - N_x = m_c \\cdot \\ddot{x}\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    m_c \\cdot g + N_y = 0\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62acf48",
   "metadata": {},
   "source": [
    "Newton's second law of motion applied to the pole gives us:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\mathbf{N} + \\mathbf{G}_p = m_p \\cdot \\mathbf{a}_p\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where $\\mathbf{G}_p = m_p \\cdot g \\cdot \\mathbf{u}_y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38770100",
   "metadata": {},
   "source": [
    "The accelaration $\\mathbf{a}_p$ of the pole's center of mass consists of three components, where $\\mathbf{r}_p = l \\cdot (\\sin{\\theta}\\cdot \\mathbf{u}_x-\\cos{\\theta}\\cdot \\mathbf{u}_y)$ denotes the vector pointing to the pole's center of mass relative to it's rotation center:  \n",
    "1. The accelaration of the cart it is attached to $\\mathbf{a}_c$,\n",
    "2. The pole's angular accelaration $\\mathbf{\\epsilon} = \\ddot{\\theta} \\cdot \\mathbf{u}_z$, which is translated into accelaration by $\\mathbf{\\epsilon} \\times \\mathbf{r}_p$.\n",
    "3. The pole's angular velocity $\\mathbf{\\omega} = \\dot{\\theta} \\cdot \\mathbf{u}_z$, for which the accelaration can be derived by  $\\mathbf{\\omega} \\times (\\mathbf{\\omega} \\times \\mathbf{r}_p)$.\n",
    "\n",
    "Thus we obtain:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\mathbf{a}_p  = \\mathbf{a}_c + \\mathbf{\\epsilon} \\times \\mathbf{r}_p + \\mathbf{\\omega} \\times (\\mathbf{\\omega} \\times \\mathbf{r}_p)\n",
    "\\end{aligned}\n",
    "$$\n",
    "Substituting $\\mathbf{r}_p = l \\cdot (\\sin{\\theta}\\cdot \\mathbf{u}_x-\\cos{\\theta}\\cdot \\mathbf{u}_y)$ and $\\mathbf{a}_p = \\ddot{x} \\cdot \\mathbf{u}_x$ as well as $\\mathbf{u}_z \\times \\mathbf{u}_x = \\mathbf{u}_y$ and $\\mathbf{u}_z \\times \\mathbf{u}_y = -\\mathbf{u}_x$:\n",
    "\\begin{aligned}\n",
    "    \\mathbf{a}_p  = \\ddot{x} \\cdot \\mathbf{u}_x + l \\cdot \\ddot{\\theta} \\cdot (\\sin{\\theta}\\cdot \\mathbf{u}_y + \\cos{\\theta}\\cdot \\mathbf{u}_x) - l \\cdot \\dot{\\theta}^2 \\cdot (\\sin{\\theta}\\cdot \\mathbf{u}_x - \\cos{\\theta}\\cdot \\mathbf{u}_y)\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895254eb",
   "metadata": {},
   "source": [
    "Inserting this quation into our equation for the forces of the pole and decomposing on the $x$ and $y$ axis we obtain:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    N_x = m_p \\cdot (\\ddot{x} + l \\cdot \\ddot{\\theta} \\cdot \\cos{\\theta} - l \\cdot \\dot{\\theta}^2 \\cdot \\sin{\\theta})\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    m_p \\cdot g - N_y = m_p \\cdot (l \\cdot \\ddot{\\theta} \\cdot \\sin{\\theta} + l \\cdot \\dot{\\theta}^2 \\cdot \\cos{\\theta})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376c6dd1",
   "metadata": {},
   "source": [
    "# TODO: FINISH EQUATION DERIVATION (SOLVE EQUATION REFERENCING?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b5f4227",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Physics():\n",
    "    \n",
    "    def __init__(self, x, theta, v = 0, a = 0, w = 0, dw = 0, g = 9.81, m_c = 1, m_p = 0.1, l = 0.5, dt = 0.02) -> None:\n",
    "        self.__dict__.update(vars())\n",
    "\n",
    "    def dw_step(self, cart_force, nudge_force) -> float:\n",
    "        numerator = self.g * np.sin(self.theta) + np.cos(self.theta) * (-cart_force - self.m_p * self.l * self.w**2 * np.sin(self.theta))/(self.m_c+self.m_p) + nudge_force * np.cos(self.theta)/(self.m_p*self.l)\n",
    "        denominator = self.l * (4/3 - (self.m_p*np.cos(self.theta)**2)/(self.m_c+self.m_p))\n",
    "\n",
    "        self.dw = numerator/denominator\n",
    "        self.w += self.dt * self.dw\n",
    "        self.theta += self.dt * self.w\n",
    "\n",
    "        return self.theta\n",
    "    \n",
    "    def a_step(self, force) -> float:\n",
    "        numerator = force + self.m_p * self.l * (self.w**2 * np.sin(self.theta) - self.dw * np.cos(self.theta))\n",
    "        denominator = self.m_c + self.m_p\n",
    "\n",
    "        self.a = numerator/denominator\n",
    "        self.v += self.dt * self.a\n",
    "        self.x += self.dt * self.v\n",
    "\n",
    "        return self.x\n",
    "\n",
    "    def update(self, force, mouse_x) -> Tuple[float, float]:\n",
    "        nudge_force = 0\n",
    "        if mouse_x is not None:\n",
    "            nudge_force = -1 if mouse_x > self.x else 1\n",
    "        return (self.dw_step(force, nudge_force), self.a_step(force))\n",
    "    \n",
    "    #get state of the system that agent can see\n",
    "    def get_state(self) -> Tuple[float,float,float,float]:\n",
    "        return (self.x, self.theta, self.v, self.w)\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        self.x = 0\n",
    "        self.theta = (np.random.rand() - 1) / 10\n",
    "        self.v = 0\n",
    "        self.a = 0\n",
    "        self.w = 0\n",
    "        self.dw = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76793d12",
   "metadata": {},
   "source": [
    "# The Agent (BOXES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5707ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Agent:\n",
    "    def __init__(self, initial_state: Tuple[float,float,float,float]) -> None:\n",
    "\n",
    "        #thresholds for discretizing the state space\n",
    "        self.x_thresholds = np.array([-2.4, -0.8, 0.8, 2.4])\n",
    "        self.theta_thresholds = np.array([-12, -6, -1, 0, 1, 6, 12])\n",
    "        self.theta_thresholds = self.theta_thresholds /180 * np.pi\n",
    "        self.v_thresholds = np.array([float(\"-inf\"), -0.5, 0.5, float(\"+inf\")]) #open intervals ignored here\n",
    "        self.w_thresholds = np.array([float(\"-inf\"), -50, 50, float(\"+inf\")]) #open intervals ignored here\n",
    "        self.w_thresholds = self.w_thresholds /180 * np.pi\n",
    "\n",
    "        self.dimensions = (len(self.x_thresholds) - 1, len(self.theta_thresholds) - 1, len(self.v_thresholds) - 1, len(self.w_thresholds) - 1)\n",
    "\n",
    "        self.boxes = np.random.rand(self.dimensions[0], \n",
    "                                    self.dimensions[1], \n",
    "                                    self.dimensions[2], \n",
    "                                    self.dimensions[3], \n",
    "                                    2) #one q-value for left and right respectively\n",
    "        box = self.get_box(initial_state)\n",
    "        self.current_box = self.boxes[box[0], box[1], box[2], box[3], :]\n",
    "\n",
    "        self.episode = 1\n",
    "    \n",
    "    def discretize(self, value, thresholds):\n",
    "        for i, limit in enumerate(thresholds):\n",
    "            if value < limit:\n",
    "                return i - 1\n",
    "        return -1\n",
    "\n",
    "    def get_box(self, state: Tuple[float,float,float,float]) -> Tuple[int,int,int,int]:\n",
    "        return (self.discretize(state[0], self.x_thresholds),\n",
    "                 self.discretize(state[1], self.theta_thresholds),\n",
    "                 self.discretize(state[2], self.v_thresholds), \n",
    "                 self.discretize(state[3], self.w_thresholds))\n",
    "    \n",
    "    def get_episode(self) -> int:\n",
    "        return self.episode\n",
    "    \n",
    "    \n",
    "    def failure_reset(self, state: Tuple[float,float,float,float]):\n",
    "        box = self.get_box(state)\n",
    "        self.current_box = self.boxes[box[0], box[1], box[2], box[3], :]\n",
    "        self.episode += 1\n",
    "\n",
    "\n",
    "class NonSpikingAgent(Agent):\n",
    "    def __init__(self, initial_state: Tuple[float,float,float,float], learning_rate, learning_decay, epsilon, epsilon_decay, discount_factor) -> None:\n",
    "        super().__init__(initial_state)\n",
    "\n",
    "        #learning paramters\n",
    "        self.learning_rate = learning_rate\n",
    "        self. learning_decay = learning_decay\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "    #returns 0 if the action is \"left\", else \"1\"\n",
    "    def choose_action(self) -> int:\n",
    "        self.action = np.random.choice([np.argmax(self.current_box), np.argmin(self.current_box)], p=[1-self.epsilon, self.epsilon])\n",
    "        return self.action\n",
    "    \n",
    "    #returns 0 if no failure occured, else 1\n",
    "    #reward is -1 on failure and 0 else\n",
    "    def update(self, next_state: Tuple[float,float,float,float]) -> int:\n",
    "        box = self.get_box(next_state)\n",
    "        if -1 in box:\n",
    "            self.current_box[self.action] += self.learning_rate * -1\n",
    "            return 1\n",
    "        \n",
    "        next_box = self.boxes[box[0], box[1], box[2], box[3], :]\n",
    "        next_q = np.max(next_box)\n",
    "        self.current_box[self.action] += self.learning_rate * (self.discount_factor * (next_q - self.current_box[self.action]))\n",
    "\n",
    "        self.current_box = next_box\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.learning_rate *= self.learning_decay\n",
    "\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2e6cd3",
   "metadata": {},
   "source": [
    "# Plot Renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f9b07a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib tk\n",
    "class PlotRenderer():\n",
    "    def __init__(self, init_x = [0], init_y = [0]) -> None:\n",
    "        plt.ion()\n",
    "        #Construct lifetime plot\n",
    "        self.lifetime_fig, self.lifetime_ax = plt.subplots()\n",
    "        self.x = init_x\n",
    "        self.y = init_y\n",
    "        self.max_lifetime = 0\n",
    "        self.line, = self.lifetime_ax.plot(self.x, self.y)\n",
    "        self.lifetime_ax.set_xlabel(\"Episode\")\n",
    "        self.lifetime_ax.set_ylabel(\"Simulation Steps\")\n",
    "        self.lifetime_ax.set_title(\"Lifetime Plot\")\n",
    "\n",
    "        #Construct Heatmap for two parameters\n",
    "        self.q_value_fig, self.q_value_ax = plt.subplots()\n",
    "        self.q_value_ax.set_title(\"Q-Values for a state of (param1/param2)\")\n",
    "        self.cmap = plt.cm.coolwarm\n",
    "        \n",
    "    def update(self, x, y, boxes) -> None:\n",
    "        print(x)\n",
    "        self.x.append(x)\n",
    "        self.y.append(y)\n",
    "        self.max_lifetime = max(self.max_lifetime, y)\n",
    "        self.line.set_data(self.x, self.y)\n",
    "        self.lifetime_ax.set_xlim(self.x[0], self.x[-1])\n",
    "        self.lifetime_ax.set_ylim(0, self.max_lifetime)\n",
    "\n",
    "        if(x % 10 == 0):\n",
    "            q_values = boxes[:,:,:,:,0] - boxes[:,:,:,:,1]\n",
    "            self.q_value_ax.imshow(np.mean(q_values, axis = (1,3)), cmap=plt.cm.coolwarm, interpolation='none')\n",
    "\n",
    "        plt.draw\n",
    "        plt.pause(0.0001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd7786b",
   "metadata": {},
   "source": [
    "# Executing Non-Spiking-Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717eda26-e385-494f-bdca-9847eefe01ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "r = Renderer(1200, 800, 600, 500, 400)\n",
    "clock = pg.time.Clock()\n",
    "running = True\n",
    "\n",
    "p = Physics(0, (np.random.rand() - 1) / 10)\n",
    "\n",
    "a = NonSpikingAgent(p.get_state(), 0.5, 0.9999999999999, 1, 0.995, 0.99)\n",
    "\n",
    "plot = PlotRenderer()\n",
    "\n",
    "steps_per_episode = 0\n",
    "max_steps = 0\n",
    "\n",
    "window_size = 30\n",
    "window = np.zeros(30)\n",
    "avg_lifetime = 20000\n",
    "\n",
    "toggle_sim = False\n",
    "\n",
    "while running:\n",
    "    steps_per_episode += 1\n",
    "\n",
    "    force = 0\n",
    "    mouse_x = None\n",
    "\n",
    "    # poll for events\n",
    "    for event in pg.event.get():\n",
    "        if event.type == pg.QUIT:\n",
    "            running = False\n",
    "            pg.quit()\n",
    "            sys.exit()\n",
    "            quit()\n",
    "        elif event.type == pg.MOUSEBUTTONDOWN:\n",
    "            mouse_x = r.get_relative_mouse_x(pg.mouse.get_pos()[0])\n",
    "        elif event.type == pg.KEYDOWN:\n",
    "            toggle_sim ^= pg.key.get_pressed()[pg.K_SPACE]\n",
    "\n",
    "    # agent chooses action, simulation is updated and reward is calculated\n",
    "    force = 10 if a.choose_action() else -10\n",
    "    theta, x = p.update(force, mouse_x)\n",
    "    failure = a.update(p.get_state())\n",
    "\n",
    "    if failure:\n",
    "        p.reset()\n",
    "        a.failure_reset(p.get_state())\n",
    "        plot.update(a.get_episode(), steps_per_episode, a.boxes)\n",
    "        window = np.roll(window, 1)\n",
    "        window[0] = steps_per_episode\n",
    "        steps_per_episode = 0\n",
    "    \n",
    "    \n",
    "    if np.mean(window) >= avg_lifetime or toggle_sim:\n",
    "        r.draw_clear()\n",
    "        r.draw_ground(0.2, \"grey\")\n",
    "        r.draw_car(x)\n",
    "        r.draw_pole(x, theta, 2*p.l, 0.02)\n",
    "        r.draw_stats(theta*180/np.pi, p.w*180/np.pi, x, p.a, a.get_episode())\n",
    "        r.display()\n",
    "\n",
    "        clock.tick(50)  # limits FPS to 50\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2876351",
   "metadata": {},
   "source": [
    "# TODO: clean up code, derive equations and explain renderer briefly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e05060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43c70dff",
   "metadata": {},
   "source": [
    "# Spiking version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4705cf6",
   "metadata": {},
   "source": [
    "## Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df6e192",
   "metadata": {},
   "source": [
    "The core principle of our SNN is to simulate the physics and neuron model in sequence, where the state at the end of a physics step is the input for the SNN and the resulting action at the end of a period of SNN simulation is the input to the next physics simulation. Both cycles are set to 40ms to provide the effect that they run simultaneously.\n",
    "The model's structure consists of two layers of neurons. For each discrete state of the system, the input layer contains a single neuron corresponding to it. Neuromodulated synapses connect these to the output layer, which itself consists of two neuron groups interpreted as actions \"move left\" and \"move right\" respectively.\n",
    "\n",
    "One simulation step of the SNN works as follows:\n",
    "1. Get the current state of the cart pole and find the designated neuron that only fires when that state is reached.\n",
    "2. Set a continuous firing rate for the simulation period on that neuron.\n",
    "3. Determine which of the neuron groups in the output layer has fired more spikes at the end of the step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5831c9d7",
   "metadata": {},
   "source": [
    "# SNN Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bdcc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class SNN_Renderer():\n",
    "\n",
    "    def __init__(self, in_layer, out_layer, weights, state_dimensions, sort_for = \"POSITION\"):\n",
    "        self.__dict__.update(vars())\n",
    "\n",
    "        in_groups = self.draw_input_layer(in_layer)\n",
    "        out_groups = self.draw_output_layer(out_layer)\n",
    "        self.draw_synapses(in_groups, out_groups, weights)\n",
    "        #... is there already a visualization tool for NEST?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3519392",
   "metadata": {},
   "source": [
    "## Neuron Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc47382",
   "metadata": {},
   "source": [
    "### Ignore and Fire Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e16ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... generate NESTML model code...\n",
    "\n",
    "from pynestml.codegeneration.nest_code_generator_utils import NESTCodeGeneratorUtils\n",
    "\n",
    "# generate and build code\n",
    "input_layer_module_name, input_layer_neuron_model_name = \\\n",
    "   NESTCodeGeneratorUtils.generate_code_for(\"../../../models/neurons/ignore_and_fire_neuron.nestml\")\n",
    "\n",
    "# ignore_and_fire\n",
    "output_layer_module_name, output_layer_neuron_model_name, output_layer_synapse_model_name = \\\n",
    "    NESTCodeGeneratorUtils.generate_code_for(\"iaf_psc_exp_neuron.nestml\",\n",
    "                                             \"neuromodulated_stdp_synapse.nestml\",\n",
    "                                             post_ports=[\"post_spikes\"],\n",
    "                                             logging_level=\"DEBUG\",\n",
    "                                             codegen_opts={\"delay_variable\": {\"neuromodulated_stdp_synapse\": \"d\"},\n",
    "                                                           \"weight_variable\": {\"neuromodulated_stdp_synapse\": \"w\"}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99614c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest\n",
    "\n",
    "nest.set_verbosity(\"M_ERROR\")\n",
    "\n",
    "class SpikingAgent(Agent):\n",
    "    cycle_period = 40.   # [ms], corresponding to 2 physics steps\n",
    "    \n",
    "    def __init__(self, initial_state: Tuple[float,float,float,float], gamma) -> None:\n",
    "        super().__init__(initial_state)\n",
    "        self.gamma = gamma\n",
    "        self.construct_neural_network()\n",
    "        self.Q_left = None\n",
    "        self.Q_right = None\n",
    "        self.Q_left_prev = None\n",
    "        self.Q_right_prev = None\n",
    "        self.scale_n_output_spikes_to_Q_value = 10\n",
    "    \n",
    "    def get_state_neuron(self, state) -> int:\n",
    "        idx = 0\n",
    "        thresholds = [self.x_thresholds, self.theta_thresholds, self.v_thresholds, self.w_thresholds]\n",
    "        for dim, val, thresh in zip(self.dimensions, state, thresholds):\n",
    "            i = self.discretize(val,thresh)\n",
    "            if i == -1: \n",
    "                print(\"fail\")\n",
    "                return -1\n",
    "            idx = idx * dim + i\n",
    "\n",
    "        return idx\n",
    "    \n",
    "    def construct_neural_network(self):\n",
    "        nest.ResetKernel()\n",
    "        nest.Install(input_layer_module_name)   # makes the generated NESTML model available\n",
    "        nest.Install(output_layer_module_name)   # makes the generated NESTML model available\n",
    "\n",
    "        input_size = self.dimensions[0] * self.dimensions[1] * self.dimensions[2] * self.dimensions[3]\n",
    "        self.input_population = nest.Create(input_layer_neuron_model_name, input_size)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.volume_transmitter = nest.Create(\"volume_transmitter\")\n",
    "        nest.CopyModel(output_layer_synapse_model_name, \"stdp_dopa_nestml\",\n",
    "                        {\"volume_transmitter\": self.volume_transmitter}) #how does this work? we're just setting syn.n?\n",
    "        \"\"\"\n",
    "        \n",
    "        self.output_population_left = nest.Create(output_layer_neuron_model_name, 10)\n",
    "        self.output_population_right = nest.Create(output_layer_neuron_model_name, 10)\n",
    "        \n",
    "        self.mm = nest.Create('multimeter', 1, {'record_from': ['V_m']})\n",
    "        nest.Connect(self.mm, self.output_population_left)\n",
    "        self.sr = nest.Create(\"spike_recorder\")\n",
    "        nest.Connect(self.output_population_left, self.sr)\n",
    "\n",
    "\n",
    "        nest.Connect(self.input_population, self.output_population_left, syn_spec={\"synapse_model\": output_layer_synapse_model_name})\n",
    "        nest.Connect(self.input_population, self.output_population_right, syn_spec={\"synapse_model\": output_layer_synapse_model_name})\n",
    "\n",
    "        self.output_population_spike_recorder_left = nest.Create(\"spike_recorder\")\n",
    "        nest.Connect(self.output_population_left, self.output_population_spike_recorder_left)\n",
    "\n",
    "        self.output_population_spike_recorder_right = nest.Create(\"spike_recorder\")\n",
    "        nest.Connect(self.output_population_right, self.output_population_spike_recorder_right)\n",
    "\n",
    "    #returns 0 if the action is \"left\", else \"1\"\n",
    "    def choose_action(self, Q_left, Q_right) -> int:\n",
    "        return Q_left > Q_right\n",
    "\n",
    "    def compute_Q_values(self):\n",
    "        n_output_spikes_left = self.output_population_spike_recorder_left.n_events\n",
    "        n_output_spikes_right = self.output_population_spike_recorder_right.n_events\n",
    " \n",
    "        self.output_population_spike_recorder_left.n_events = 0\n",
    "        self.output_population_spike_recorder_right.n_events = 0\n",
    "        \n",
    "        self.Q_left_prev = self.Q_left\n",
    "        self.Q_right_prev = self.Q_right\n",
    "\n",
    "        # still need to get scale\n",
    "        self.Q_left = self.scale_n_output_spikes_to_Q_value * n_output_spikes_left\n",
    "        self.Q_right = self.scale_n_output_spikes_to_Q_value * n_output_spikes_right\n",
    "\n",
    "    # update Q_value using TD-Error with previous Q_value and reward = 0\n",
    "    # cooldown_time in case the SNN doesn't need 40ms to update\n",
    "    def failure_reset(self, cooldown_time):\n",
    "        # if for some reason the simulation terminates super fast\n",
    "        if self.Q_left_prev == None and self.Q_right_prev == None:\n",
    "            return\n",
    "        # what would we mean by that? negative dopamine is biologically inaccurate\n",
    "        # inhibitory neuromodulators?\n",
    "        if self.choose_action(self.Q_left_prev, self.Q_right_prev):\n",
    "            syn = nest.GetConnections(source=self.input_population, target=self.output_population_right)\n",
    "            syn.n = -self.Q_right\n",
    "        else:\n",
    "            syn = nest.GetConnections(source=self.input_population, target=self.output_population_left)\n",
    "            syn.n = -self.Q_left\n",
    "        nest.Simulate(cooldown_time)\n",
    "        \n",
    "        self.episode += 1\n",
    "\n",
    "    def update(self, next_state: Tuple[float,float,float,float]):\n",
    "        \n",
    "        # make the correct input neuron fire\n",
    "        self.input_population.firing_rate = 0.\n",
    "        neuron_id = self.get_state_neuron(next_state)\n",
    "\n",
    "        # if state was a failure\n",
    "        if neuron_id == -1:\n",
    "            self.failure_reset(SpikingAgent.cycle_period)\n",
    "            return -1\n",
    "        \n",
    "        self.input_population[neuron_id].firing_rate = 100. # XXX: value not given in Liu&Pan. Got 500 Hz as max freq from BVogler thesis. n.b. 40 ms cycle time. \n",
    "\n",
    "        # simulate for one cycle\n",
    "        nest.Simulate(SpikingAgent.cycle_period)\n",
    "\n",
    "        \"\"\"\n",
    "        spike_times = nest.GetStatus(self.sr, keys=\"events\")[0][\"times\"]\n",
    "\n",
    "        fig, ax = plt.subplots(nrows=1)\n",
    "        \n",
    "        ax.plot(self.mm.get(\"events\")[\"times\"], self.mm.get(\"events\")[\"V_m\"])\n",
    "        ax.scatter(spike_times, 30 * np.ones_like(spike_times), marker=\"d\", c=\"orange\", alpha=.8, zorder=99)\n",
    "        ax.grid(True)\n",
    "        ax.set_ylabel(\"V_m [mV]\")\n",
    "        ax.set_xlabel(\"Time [ms]\")\n",
    "        fig.show()\n",
    "        \"\"\"\n",
    "\n",
    "        self.compute_Q_values()\n",
    "\n",
    "        # set new dopamine concentration on the synapses\n",
    "        # PROBLEM: HOW DO WE HANDLE FAILURE? The physics simulation immediately resets after it.\n",
    "        # Perhaps run the simulation without spiking to let the weights update? (BVogler)\n",
    "\n",
    "        # update Q_value using TD-Error with previous Q_value and reward = 1\n",
    "        if self.Q_left_prev != None and self.Q_right_prev != None:\n",
    "            R = 1.\n",
    "            if self.choose_action(self.Q_left_prev, self.Q_right_prev):\n",
    "                syn = nest.GetConnections(source=self.input_population, target=self.output_population_right)\n",
    "                syn.n = self.gamma * self.Q_right + R - self.Q_right_prev\n",
    "            else:\n",
    "                syn = nest.GetConnections(source=self.input_population, target=self.output_population_left)\n",
    "                syn.n = self.gamma * self.Q_left + R - self.Q_left_prev\n",
    "        \n",
    "        # 0 if action is \"left\", else 1\n",
    "        return self.choose_action(self.Q_left, self.Q_right)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566fa9c9",
   "metadata": {},
   "source": [
    "# Executing spiking version\n",
    "\n",
    "The main loop looks like this: for every iteration of the loop (for every \"cycle\" or \"step\"):\n",
    "\n",
    "- set the rate of the input neurons to the current state of the system\n",
    "- run the SNN with this input state s_n for a period of time (cycle time, in BVogler's thesis: 40 ms)\n",
    "- obtain the Q(sn, a) values, by counting nr of spikes in output population over this cycle period\n",
    "- choose action $a_n$ on the basis of Q-values\n",
    "- run the environment for the same cycle time (40 ms) to obtain next state $s_{n+1}$\n",
    "- compute reward on the basis of the last taken action (????)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bda6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "r = Renderer(1200, 800, 600, 500, 400)\n",
    "clock = pg.time.Clock()\n",
    "running = True\n",
    "\n",
    "p = Physics(0, (np.random.rand() - 1) / 10)\n",
    "\n",
    "a = SpikingAgent(p.get_state(), 0.98)\n",
    "\n",
    "#plot = PlotRenderer()\n",
    "\n",
    "steps_per_episode = 0\n",
    "\n",
    "window_size = 30\n",
    "window = np.zeros(30)\n",
    "avg_lifetime = 20000\n",
    "\n",
    "toggle_sim = False\n",
    "\n",
    "while running:\n",
    "    steps_per_episode += 1\n",
    "    force = 0\n",
    "    mouse_x = None\n",
    "\n",
    "    # poll for events\n",
    "    for event in pg.event.get():\n",
    "        if event.type == pg.QUIT:\n",
    "            running = False\n",
    "            pg.quit()\n",
    "            sys.exit()\n",
    "            quit()\n",
    "        elif event.type == pg.MOUSEBUTTONDOWN:\n",
    "            mouse_x = r.get_relative_mouse_x(pg.mouse.get_pos()[0])\n",
    "        elif event.type == pg.KEYDOWN:\n",
    "            toggle_sim ^= pg.key.get_pressed()[pg.K_SPACE]\n",
    "\n",
    "    # Simulate SNN, choose action, simulate physics, receive state\n",
    "    # Since SNN takes 40ms, it reacts only to every 2nd physics step\n",
    "    if steps_per_episode % 2 == 1:\n",
    "        action = a.update(p.get_state())\n",
    "        if action == -1:\n",
    "            p.reset()\n",
    "            a.failure_reset(SpikingAgent.cycle_period)\n",
    "            #plot.update(a.get_episode(), steps_per_episode, a.boxes)\n",
    "            window = np.roll(window, 1)\n",
    "            window[0] = steps_per_episode\n",
    "            steps_per_episode = 0\n",
    "        else:\n",
    "            force = 10 if action else -10\n",
    "            theta, x = p.update(force, mouse_x)\n",
    "    else:\n",
    "        theta, x = p.update(0, mouse_x)\n",
    "\n",
    "    \n",
    "    \n",
    "    if np.mean(window) >= avg_lifetime or toggle_sim:\n",
    "        r.draw_clear()\n",
    "        r.draw_ground(0.2, \"grey\")\n",
    "        r.draw_car(x)\n",
    "        r.draw_pole(x, theta, 2*p.l, 0.02)\n",
    "        r.draw_stats(theta*180/np.pi, p.w*180/np.pi, x, p.a, a.get_episode())\n",
    "        r.display()\n",
    "\n",
    "        clock.tick(50)  # limits FPS to 50"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
