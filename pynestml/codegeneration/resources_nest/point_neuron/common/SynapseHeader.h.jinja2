{#-
SynapseHeader.h.jinja2

This file is part of NEST.

Copyright (C) 2004 The NEST Initiative

NEST is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 2 of the License, or
(at your option) any later version.

NEST is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with NEST.  If not, see <http://www.gnu.org/licenses/>.
#}
{%- import 'directives_cpp/FunctionDeclaration.jinja2' as function_declaration with context %}
{%- if tracing %}/* generated by {{self._TemplateReference__context.name}} */ {% endif -%}
/**
 *  {{synapseName}}.h
 *
 *  This file is part of NEST.
 *
 *  Copyright (C) 2004 The NEST Initiative
 *
 *  NEST is free software: you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License as published by
 *  the Free Software Foundation, either version 2 of the License, or
 *  (at your option) any later version.
 *
 *  NEST is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU General Public License for more details.
 *
 *  You should have received a copy of the GNU General Public License
 *  along with NEST.  If not, see <http://www.gnu.org/licenses/>.
 *
 *  Generated from NESTML {{ nestml_version }} at time: {{ now }}
**/

#ifndef {{synapseName.upper()}}_H
#define {{synapseName.upper()}}_H

// C++ includes:
#include <cmath>

// Includes from nestkernel:
#include "common_synapse_properties.h"
#include "connection.h"
#include "connector_model.h"
#include "event.h"
{%- if norm_rng %}

// Includes for random number generator
{%- if nest_version.startswith("v2") %}
#include "normal_randomdev.h"
#include "uniform_randomdev.h"
{%- else %}
#include <random>
{%- endif %}
{%- endif %}
{%- if vt_ports is defined and vt_ports|length > 0  %}
// Includes for volume transmitter
#include "volume_transmitter.h"
{%- endif %}


// Includes from sli:
#include "dictdatum.h"
#include "dictutils.h"

/** @BeginDocumentation
{{ synapse.print_comment() }}
**/

// uncomment the next line to enable printing of detailed debug information
// #define DEBUG

namespace nest
{
{%- if not (nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2")
        or nest_version.startswith("v3.3") or nest_version.startswith("v3.4") or nest_version.startswith("v3.5") or nest_version.startswith("v3.6")) %}
// Register the synapse model
void register_{{ synapseName }}( const std::string& name );
{%- endif %}

namespace {{names_namespace}}
{
{%- if synapse.get_state_symbols()|length > 0 %}
{%- for sym in synapse.get_state_symbols() %}
    const Name _{{sym.get_symbol_name()}}( "{{sym.get_symbol_name()}}" );
{%- endfor %}
{%- endif %}
{%- if synapse.get_parameter_symbols()|length > 0 %}
{%- for sym in synapse.get_parameter_symbols() %}
    const Name _{{sym.get_symbol_name()}}( "{{sym.get_symbol_name()}}" );
{%- endfor %}
{%- endif %}
}

class {{synapseName}}CommonSynapseProperties : public CommonSynapseProperties {
public:

    {{synapseName}}CommonSynapseProperties()
    : CommonSynapseProperties()
    {
{%- filter indent(width=8) %}
{%- for parameter in synapse.get_parameter_symbols() %}
{%-     set isHomogeneous = PyNestMLLexer["DECORATOR_HOMOGENEOUS"] in parameter.get_decorators() %}
{%-     if isHomogeneous %}
{%-         with variable = utils.get_parameter_variable_by_name(astnode, parameter.get_symbol_name()) %}
{%-             set variable_symbol = synapse.get_scope().resolve_to_symbol(variable.get_complete_name(), SymbolKind.VARIABLE) %}
{%-             include "directives_cpp/CommonPropertiesDictionaryMemberInitialization.jinja2" %}
{%-         endwith %}
{%-     endif %}
{%- endfor %}
{%- endfilter %}
    }

    /**
     * Get all properties and put them into a dictionary.
     */
    void get_status( DictionaryDatum& d ) const
    {
        CommonSynapseProperties::get_status( d );

{%- filter indent(width=8) %}
{%- for parameter in synapse.get_parameter_symbols() %}
{%-     set isHomogeneous = PyNestMLLexer["DECORATOR_HOMOGENEOUS"] in parameter.get_decorators() %}
{%-     if isHomogeneous %}
{%-         set variable_symbol = parameter %}
{%-         set variable = utils.get_variable_by_name(astnode, variable_symbol.get_symbol_name()) %}
{%-         include "directives_cpp/CommonPropertiesDictionaryWriter.jinja2" %}
{%-     endif %}
{%- endfor %}
{%- endfilter %}
    }


    /**
     * Set properties from the values given in dictionary.
     */
    void set_status( const DictionaryDatum& d, ConnectorModel& cm )
    {
      CommonSynapseProperties::set_status( d, cm );

{%- filter indent(width=8) %}
{%- for parameter in synapse.get_parameter_symbols() %}
{%-     set isHomogeneous = PyNestMLLexer["DECORATOR_HOMOGENEOUS"] in parameter.get_decorators() %}
{%-     if isHomogeneous %}
{%-         set variable_symbol = parameter %}
{%-         set variable = utils.get_variable_by_name(astnode, variable_symbol.get_symbol_name()) %}
{%-         include "directives_cpp/CommonPropertiesDictionaryReader.jinja2" %}
{%-     endif %}
{%- endfor %}
{%- endfilter %}

{%- if vt_ports is defined and vt_ports|length > 0  %}
      NodeCollectionDatum vt_datum;
      if ( updateValue< NodeCollectionDatum >( d, names::volume_transmitter, vt_datum ) )
      {
{%- if not (nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2") or nest_version.startswith("v3.3") or nest_version.startswith("v3.4")) %}
        const size_t tid = kernel().vp_manager.get_thread_id();
{%- else %}
        const thread tid = kernel().vp_manager.get_thread_id();
{%- endif %}
{%- if nest_version.startswith("v2") %}
        Node* vt = kernel().node_manager.get_node( ( *vt_datum )[ 0 ], tid );
{%- else %}
        Node* vt = kernel().node_manager.get_node_or_proxy( ( *vt_datum )[ 0 ], tid );
{%- endif %}
        vt_ = dynamic_cast< volume_transmitter* >( vt );
        if ( vt_ == nullptr )
        {
          throw BadProperty( "Neuromodulatory source must be volume transmitter" );
        }
      }
{%- endif %}
    }

    // N.B.: we define all parameters as public for easy reference conversion later on.
    // This may or may not benefit performance (TODO: compare with inline getters/setters)

{%- for parameter in synapse.get_parameter_symbols() %}
{%-     set isHomogeneous = PyNestMLLexer["DECORATOR_HOMOGENEOUS"] in parameter.get_decorators() %}
{%-     if (isHomogeneous) %}
{%-         set parameterName = printer.print(utils.get_parameter_variable_by_name(astnode, parameter.get_symbol_name())) %}
    {{declarations.print_variable_type(parameter)}} {{parameter.get_symbol_name()}};
{%-     endif %}
{%- endfor %}

{%- if vt_ports is defined and vt_ports|length > 0  %}
    volume_transmitter* vt_ = nullptr;

    inline long get_vt_node_id() const
    {
      if ( vt_ != nullptr )
      {
{%- if nest_version.startswith("v2") %}
        return vt_->get_gid();
{%- else %}
        return vt_->get_node_id();
{%- endif %}
      }
      else
      {
        return -1;
      }
    }

{%- endif %}
};

template < typename targetidentifierT >
class {{synapseName}} : public Connection< targetidentifierT >
{
{%- if paired_neuron_name | length > 0 %}
  typedef {{ paired_neuron_name }} post_neuron_t;

{% endif %}
{%- if vt_ports is defined and vt_ports|length > 0  %}
public:
{%- if not (nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2") or nest_version.startswith("v3.3") or nest_version.startswith("v3.4")) %}
  void trigger_update_weight( size_t t,
    const std::vector< spikecounter >& vt_spikes,
    double t_trig,
    const {{synapseName}}CommonSynapseProperties& cp );
{%- else %}
  void trigger_update_weight( thread t,
    const std::vector< spikecounter >& vt_spikes,
    double t_trig,
    const {{synapseName}}CommonSynapseProperties& cp );
{%- endif %}
{%- endif %}
private:
  double t_lastspike_;
{%- if vt_ports is defined and vt_ports|length > 0  %}
  // time of last update, which is either time of last presyn. spike or time-driven update
  double t_last_update_;

  // vt_spikes_idx_ refers to the vt spike that has just been processed after trigger_update_weight
  // a pseudo vt spike at t_trig is stored at index 0 and vt_spikes_idx_ = 0
{%- if not (nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2") or nest_version.startswith("v3.3") or nest_version.startswith("v3.4")) %}
  size_t vt_spikes_idx_;
{%- else %}
  index vt_spikes_idx_;
{%- endif %}
{%- endif %}

  /**
   * Dynamic state of the synapse.
   *
   * These are the state variables that are advanced in time by calls to
   * send(). In many models, some or all of them can be set by the user
   * through ``SetStatus()``.
   *
   * @note State_ need neither copy constructor nor @c operator=(), since
   *       all its members are copied properly by the default copy constructor
   *       and assignment operator. Important:
   *       - If State_ contained @c Time members, you need to define the
   *         assignment operator to recalibrate all members of type @c Time . You
   *         may also want to define the assignment operator.
   *       - If State_ contained members that cannot copy themselves, such
   *         as C-style arrays, you need to define the copy constructor and
   *         assignment operator to copy those members.
  **/
  struct State_{
{%- if not uses_numeric_solver %}
{%-     filter indent(4,True) %}
{%-     for variable_symbol in synapse.get_state_symbols() %}
{%-         set variable = utils.get_state_variable_by_name(astnode, variable_symbol.get_symbol_name()) %}
{%-         include "directives_cpp/MemberDeclaration.jinja2" %}
{%-     endfor %}
{%-     endfilter %}
{%- else %}
    //! Symbolic indices to the elements of the state vector y
    enum StateVecElems
    {
{# N.B. numeric solver contains all state variables, including those that will be solved by analytic solver#}
{%-     if uses_numeric_solver %}
      // numeric solver state variables
{%-         for variable_name in numeric_state_variables %}
      {{variable_name}},
{%-         endfor %}
{%-     endif %}
      STATE_VEC_SIZE
    };
    //! state vector, must be C-array for GSL solver
    double ode_state[STATE_VEC_SIZE];

    // state variables from state block
{%-     filter indent(4,True) %}
{%-     for variable_symbol in synapse.get_state_symbols() %}
{%-         set variable = utils.get_state_variable_by_name(astnode, variable_symbol.get_symbol_name()) %}
{%-         include "directives_cpp/MemberDeclaration.jinja2" %}
{%-     endfor %}
{%-     endfilter %}
{%- endif %}

    State_() {};
  };

  /**
   * Free parameters of the synapse.
   *
{% for block in synapse.get_parameters_blocks() %}
{{ block.print_comment() }}
{%- endfor %}
   *
   * These are the parameters that can be set by the user through @c SetStatus.
   * Parameters do not change during calls to ``send()`` and are not reset by
   * @c ResetNetwork.
   *
   * @note Parameters_ need neither copy constructor nor @c operator=(), since
   *       all its members are copied properly by the default copy constructor
   *       and assignment operator. Important:
   *       - If Parameters_ contained @c Time members, you need to define the
   *         assignment operator to recalibrate all members of type @c Time . You
   *         may also want to define the assignment operator.
   *       - If Parameters_ contained members that cannot copy themselves, such
   *         as C-style arrays, you need to define the copy constructor and
   *         assignment operator to copy those members.
  */
  struct Parameters_{
{%- filter indent(4,True) %}
{%- for variable_symbol in synapse.get_parameter_symbols() %}
{%-     set isHomogeneous = PyNestMLLexer["DECORATOR_HOMOGENEOUS"] in variable_symbol.get_decorators() %}
{%-     set variable = utils.get_parameter_variable_by_name(astnode, variable_symbol.get_symbol_name()) %}
{%-     if not isHomogeneous  and not (variable_symbol.get_symbol_name() == nest_codegen_opt_delay_variable) %}
{%-         include 'directives_cpp/MemberDeclaration.jinja2' %}
{%-     elif isHomogeneous %}
    // N.B. the parameter `{{ printer.print(variable) }}` is defined in the common properties class
{%-     endif %}
{%- endfor %}
{%- endfilter %}

    /** Initialize parameters to their default values. */
    Parameters_() {};
  };

  /**
   * Internal variables of the synapse.
   *
{%- for internals_block in synapse.get_internals_blocks() %}
   {{ internals_block.print_comment('*') }}
{%- endfor %}
   * These variables must be initialized by recompute_internal_variables().
  **/
  struct Variables_
  {
{%- for variable_symbol in synapse.get_internal_symbols() %}
{%-     set variable = utils.get_internal_variable_by_name(astnode, variable_symbol.get_symbol_name()) %}
{%-     filter indent(4,True) %}
{%-         include "directives_cpp/MemberDeclaration.jinja2" %}
{%-     endfilter %}
{%- endfor %}
  };

  Parameters_ P_;  //!< Free parameters.
  State_      S_;  //!< Dynamic state.
  Variables_  V_;  //!< Internal Variables
{%- if synapse.get_state_symbols()|length > 0 or synapse.get_parameter_symbols()|length > 0 %}
  // -------------------------------------------------------------------------
  //   Getters/setters for parameters and state variables
  // -------------------------------------------------------------------------
{%  filter indent(2, True) -%}
{%- for variable_symbol in synapse.get_state_symbols() + synapse.get_parameter_symbols() %}
{%-     set variable = utils.get_variable_by_name(astnode, variable_symbol.get_symbol_name()) %}
{%-     set isHomogeneous = PyNestMLLexer["DECORATOR_HOMOGENEOUS"] in variable_symbol.get_decorators() %}
{%-     if not isHomogeneous %}
{%-         if variable.get_name() != nest_codegen_opt_delay_variable and variable.get_name() != synapse_weight_variable %}
{%-             include "directives_cpp/MemberVariableGetterSetter.jinja2" %}
{%          elif variable.get_name() == synapse_weight_variable and variable.get_name() != "weight" %}
{#              weight is its own special case in NEST #}
inline {{ declarations.print_variable_type(variable_symbol) }} get_{{ variable.get_name() }}() const
{
  return {{ printer.print(variable) }};
}

inline void set_{{ variable.get_name() }}(const {{ declarations.print_variable_type(variable_symbol) }} __v)
{
  set_weight(__v);
}
{%-         endif %}
{%-     endif %}
{%- endfor %}
{%- endfilter %}
{%- endif %}

  // -------------------------------------------------------------------------
  //   Getters/setters for inline expressions
  // -------------------------------------------------------------------------

{% filter indent(2, True) -%}
{%- for equations_block in synapse.get_equations_blocks() %}
{%-     for inline_expr in equations_block.get_inline_expressions() %}
{%-         set variable = ast_node_factory.create_ast_variable(inline_expr.get_variable_name(), differential_order=0, scope=inline_expr.scope) %}
{%-         set variable_symbol = equations_block.get_scope().resolve_to_symbol(variable.get_complete_name(), SymbolKind.VARIABLE) %}
{%-         include "directives_cpp/MemberVariableGetterSetter.jinja2" %}
{%-     endfor %}
{%- endfor %}
{%- endfilter %}

  // -------------------------------------------------------------------------
  //   Function declarations
  // -------------------------------------------------------------------------

{% filter indent(2) -%}
{% for function in synapse.get_functions() %}
{{ function_declaration.FunctionDeclaration(function, "") }};
{% endfor %}
{%- endfilter %}

  /**
   * Update internal state (``S_``) of the synapse according to the dynamical equations defined in the model and the statements in the ``update`` block.
  **/
  inline void
  update_internal_state_(double t_start, double timestep, const {{synapseName}}CommonSynapseProperties& cp);

  void recompute_internal_variables();

public:
  // this line determines which common properties to use
  typedef {{synapseName}}CommonSynapseProperties CommonPropertiesType;

  typedef Connection< targetidentifierT > ConnectionBase;

{%- if not (nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2") or nest_version.startswith("v3.3") or nest_version.startswith("v3.4")) %}
  static constexpr ConnectionModelProperties properties = ConnectionModelProperties::HAS_DELAY
    | ConnectionModelProperties::IS_PRIMARY | ConnectionModelProperties::SUPPORTS_HPC
    | ConnectionModelProperties::SUPPORTS_LBL;
{%- endif %}

  /**
  * Default constructor.
  *
  * Sets default values for all parameters (skipping common properties).
  *
  * Needed by GenericConnectorModel.
  */
  {{synapseName}}();

  /**
  * Copy constructor from a property object.
  *
  * Sets default values for all parameters (skipping common properties).
  *
  * Needs to be defined properly in order for GenericConnector to work.
  */
  {{synapseName}}( const {{synapseName}}& rhs );

{%- if vt_ports is defined and vt_ports|length > 0  %}
{%-     set vt_port = vt_ports[0] %}
  void process_{{ vt_port }}_spikes_( const std::vector< spikecounter >& vt_spikes,
      double t0,
      double t1,
      const {{synapseName}}CommonSynapseProperties& cp );
{%- endif %}

  // Explicitly declare all methods inherited from the dependent base
  // ConnectionBase. This avoids explicit name prefixes in all places these
  // functions are used. Since ConnectionBase depends on the template parameter,
  // they are not automatically found in the base class.
  using ConnectionBase::get_delay_steps;
  using ConnectionBase::set_delay_steps;
  using ConnectionBase::get_delay;
  using ConnectionBase::set_delay;
  using ConnectionBase::get_rport;
  using ConnectionBase::get_target;


  class ConnTestDummyNode : public ConnTestDummyNodeBase
  {
  public:
    // Ensure proper overriding of overloaded virtual functions.
    // Return values from functions are ignored.
    using ConnTestDummyNodeBase::handles_test_event;
{%- if not (nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2") or nest_version.startswith("v3.3") or nest_version.startswith("v3.4")) %}
    size_t
    handles_test_event( SpikeEvent&, size_t ) override
{%- else %}
    port
    handles_test_event( SpikeEvent&, rport ) override
{%- endif %}
    {
{%- if nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2") or nest_version.startswith("v3.3") %}
      return invalid_port_;
{%- else %}
      return invalid_port;
{%- endif %}
    }
{%- if not (nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2") or nest_version.startswith("v3.3") or nest_version.startswith("v3.4")) %}
    size_t
    handles_test_event( RateEvent&, size_t ) override
{%- else %}
    port
    handles_test_event( RateEvent&, rport ) override
{%- endif %}
    {
{%- if nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2") or nest_version.startswith("v3.3") %}
      return invalid_port_;
{%- else %}
      return invalid_port;
{%- endif %}    }
{%- if not (nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2") or nest_version.startswith("v3.3") or nest_version.startswith("v3.4")) %}
    size_t
    handles_test_event( DataLoggingRequest&, size_t ) override
{%- else %}
    port
    handles_test_event( DataLoggingRequest&, rport ) override
{%- endif %}
    {
{%- if nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2") or nest_version.startswith("v3.3") %}
      return invalid_port_;
{%- else %}
      return invalid_port;
{%- endif %}    }
{%- if not (nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2") or nest_version.startswith("v3.3") or nest_version.startswith("v3.4")) %}
    size_t
    handles_test_event( CurrentEvent&, size_t ) override
{%- else %}
    port
    handles_test_event( CurrentEvent&, rport ) override
{%- endif %}
    {
{%- if nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2") or nest_version.startswith("v3.3") %}
      return invalid_port_;
{%- else %}
      return invalid_port;
{%- endif %}    }
{%- if not (nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2") or nest_version.startswith("v3.3") or nest_version.startswith("v3.4")) %}
    size_t
    handles_test_event( ConductanceEvent&, size_t ) override
{%- else %}
    port
    handles_test_event( ConductanceEvent&, rport ) override
{%- endif %}
    {
{%- if nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2") or nest_version.startswith("v3.3") %}
      return invalid_port_;
{%- else %}
      return invalid_port;
{%- endif %}    }
{%- if not (nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2") or nest_version.startswith("v3.3") or nest_version.startswith("v3.4")) %}
    size_t
    handles_test_event( DoubleDataEvent&, size_t ) override
{%- else %}
    port
    handles_test_event( DoubleDataEvent&, rport ) override
{%- endif %}
    {
{%- if nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2") or nest_version.startswith("v3.3") %}
      return invalid_port_;
{%- else %}
      return invalid_port;
{%- endif %}    }
{%- if not (nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2") or nest_version.startswith("v3.3") or nest_version.startswith("v3.4")) %}
    size_t
    handles_test_event( DSSpikeEvent&, size_t ) override
{%- else %}
    port
    handles_test_event( DSSpikeEvent&, rport ) override
{%- endif %}
    {
{%- if nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2") or nest_version.startswith("v3.3") %}
      return invalid_port_;
{%- else %}
      return invalid_port;
{%- endif %}    }
{%- if not (nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2") or nest_version.startswith("v3.3") or nest_version.startswith("v3.4")) %}
    size_t
    handles_test_event( DSCurrentEvent&, size_t ) override
{%- else %}
    port
    handles_test_event( DSCurrentEvent&, rport ) override
{%- endif %}
    {
{%- if nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2") or nest_version.startswith("v3.3") %}
      return invalid_port_;
{%- else %}
      return invalid_port;
{%- endif %}    }
  };
{%- if synapse_weight_variable | length > 0 and synapse_weight_variable != "weight" %}
{%-     set variable = utils.get_variable_by_name(astnode, synapse_weight_variable) %}
{%-    set variable_symbol = variable.get_scope().resolve_to_symbol(variable.get_complete_name(), SymbolKind.VARIABLE) %}
{%-     set isHomogeneous = PyNestMLLexer["DECORATOR_HOMOGENEOUS"] in variable_symbol.get_decorators() %}

  /**
   *  special case for weights in NEST: only in case a NESTML state variable was specified in code generation options as ``weight_variable``
  **/
  inline void set_weight(double w)
  {
{%-    if isHomogeneous %}
    throw BadProperty(
      "Setting of individual weights is not possible! The common weights can "
      "be changed via "
      "CopyModel()." );
{%-     else %}
    {{ printer.print(variable) }} = w;
{%-     endif %}
  }

{%-    if not isHomogeneous %}
  /**
   *  special case for weights in NEST: only in case a NESTML state variable was specified in code generation options as ``weight_variable``
  **/
  inline double get_weight() const
  {
    return {{ printer.print(variable) }};
  }
{%-     endif %}
{%- endif %}

{%- if not (nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2") or nest_version.startswith("v3.3") or nest_version.startswith("v3.4")) %}
  void
  check_connection( Node& s,
    Node& t,
    size_t receptor_type,
    const CommonPropertiesType& cp )
{%- else %}
  void
  check_connection( Node& s,
    Node& t,
    rport receptor_type,
    const CommonPropertiesType& cp )
{%- endif %}
  {
    ConnTestDummyNode dummy_target;
    ConnectionBase::check_connection_( dummy_target, s, t, receptor_type );

{%- if paired_neuron_name is defined %}
    try {
      dynamic_cast< {{ paired_neuron_name }}& >(t);
    }
    catch (std::bad_cast &exp) {
      std::cerr << "wrong type of neuron connected! Synapse '{{synapseName}}' will only work with neuron '{{ paired_neuron_name }}'.\n";
      exit(1);
    }
{%- endif %}
{%- if vt_ports is defined and vt_ports|length > 0  %}

    if ( cp.vt_ == nullptr )
    {
      throw BadProperty( "No volume transmitter has been assigned to the dopamine synapse." );
    }
{%- endif %}

    t.register_stdp_connection( t_lastspike_ - get_delay(), get_delay() );
  }

{%- if paired_neuron_name is not none and paired_neuron_name|length > 0 and paired_neuron.state_vars_that_need_continuous_buffering | length > 0 %}
{%-     if continuous_state_buffering_method == "continuous_time_buffer" %}
void get_entry_from_continuous_variable_history(double t,
                                                std::deque< continuous_variable_histentry_{{ paired_neuron_name }} >::iterator& start,
                                                std::deque< continuous_variable_histentry_{{ paired_neuron_name }} >::iterator& finish,
                                                continuous_variable_histentry_{{ paired_neuron_name }}& histentry)
{
  std::deque< continuous_variable_histentry_{{ paired_neuron_name }} >::iterator runner;
  if ( start == finish or t < 0.0 )
  {
    // return initial value
    histentry.t_ = 0.;
{%-         for state_var in paired_neuron.state_vars_that_need_continuous_buffering %}
    histentry.{{ state_var }} = {{ state_vars_that_need_continuous_buffering_transformed_iv[state_var] }};
{%-         endfor %}
    return;
  }
  else
  {
    runner = start;
    while ( runner != finish )
    {
      if ( fabs( t - runner->t_ ) < nest::kernel().connection_manager.get_stdp_eps() )
      {
        histentry = *runner;
        return;
      }
      ++runner;
    }
  }

  // if we get here, there is no entry at time t -- something is wrong!
  assert(0);
}
{%-     endif %}
{%- endif %}

{%- if not (nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2") or nest_version.startswith("v3.3") or nest_version.startswith("v3.4")) %}
{%-   if not (nest_version.startswith("v3.5") or nest_version.startswith("v3.6")) %}
  bool
  send( Event& e, const size_t tid, const {{synapseName}}CommonSynapseProperties& cp )
{%-   else %}
  void
  send( Event& e, const size_t tid, const {{synapseName}}CommonSynapseProperties& cp )
{%-   endif %}
{%- else %}
  void
  send( Event& e, const thread tid, const {{synapseName}}CommonSynapseProperties& cp )
{%- endif %}
  {
    const double __timestep = nest::Time::get_resolution().get_ms();  // do not remove, this is necessary for the timestep() function

    auto get_thread = [tid]()
    {
        return tid;
    };

    const double __t_spike = e.get_stamp().get_ms();    // time of the presynaptic spike [ms]
#ifdef DEBUG
    std::cout << "[synapse " << this << "] {{ synapseName }}::send(): handling pre spike at t = " << __t_spike << std::endl;
#endif

{%- if vt_ports is defined and vt_ports|length > 0  %}
  // get history of volume transmitter spikes
  const std::vector< spikecounter >& vt_spikes = cp.vt_->deliver_spikes();

{%- endif %}
    // use accessor functions (inherited from Connection< >) to obtain delay and target
{%- if paired_neuron_name is not none and paired_neuron_name|length > 0 %}
    {{ paired_neuron_name }}* __target = static_cast< {{ paired_neuron_name }}* >(get_target(tid));
    assert(__target);
{%- else %}
    Node* __target = get_target( tid );
{%- endif %}
    const double __dendritic_delay = get_delay();
    const bool pre_before_post_update = {{pre_before_post_update}};
    bool pre_before_post_flag = false;

    if (t_lastspike_ < 0.)
    {
        // this is the first presynaptic spike to be processed
        t_lastspike_ = 0.;
    }

{%- if paired_neuron_name is not none and paired_neuron_name|length > 0 %}
    double timestep = 0;
    std::deque< histentry__{{ paired_neuron_name }} >::iterator start;
    std::deque< histentry__{{ paired_neuron_name }} >::iterator finish;
    {
      /**
       * Updates due to post-synaptic spikes since last pre-synaptic spike.
       *
       * t_lastspike_ contains the point in time of the last spike. So we read the history (t_last_spike - dendritic_delay, ..., __t_spike - dendritic_delay]
       *
       * Note that this also increases the access counter for these entries which is used to prune the history.
      **/

{%- if vt_ports is defined and vt_ports|length > 0  %}
      double t0 = t_last_update_;
{%- endif %}

{%- if paired_neuron_name is not none and paired_neuron_name|length > 0 and paired_neuron.state_vars_that_need_continuous_buffering | length > 0 and continuous_state_buffering_method == "continuous_time_buffer" %}
      // get continuous-time history entries in relevant range (t1, t2] from post-synaptic neuron
      std::deque< continuous_variable_histentry_{{ paired_neuron_name }} >::iterator continuous_history_start;
      std::deque< continuous_variable_histentry_{{ paired_neuron_name }} >::iterator continuous_history_finish;

      ((post_neuron_t*)(__target))->get_continuous_variable_history( t_lastspike_ ,
        __t_spike,
        &continuous_history_start,
        &continuous_history_finish );
{%- endif %}

      // get spike history entries in relevant range (t1, t2] from post-synaptic neuron
      __target->get_history__( t_lastspike_ - __dendritic_delay,
        __t_spike - __dendritic_delay,
        &start,
        &finish );
      while ( start != finish )
      {
#ifdef DEBUG
        std::cout << "\tprocessing post spike at t = " << start->t_ + __dendritic_delay << std::endl;
#endif

        const double minus_dt = t_lastspike_ - ( start->t_ + __dendritic_delay );
        // get_history() should make sure that ``start->t_ > t_lastspike_ - dendritic_delay``, i.e. minus_dt < 0
        assert( minus_dt < -kernel().connection_manager.get_stdp_eps() );

{%- if paired_neuron_name is not none and paired_neuron_name|length > 0 and paired_neuron.state_vars_that_need_continuous_buffering | length > 0 %}
        /**
         * grab state variables from the postsynaptic neuron at the time of the post spike
        **/

{#- post spike based: grab the entry from the post spiking history buffer #}
{%-     if continuous_state_buffering_method == "post_spike_based" %}
{%-         for var_name in paired_neuron.state_vars_that_need_continuous_buffering %}
{%-         set var_name_post = utils.get_var_name_tuples_of_neuron_synapse_pair(continuous_post_ports, var_name) %}
        const double __{{ var_name }} = start->{{ var_name }}_;
{%-         endfor %}
{%-     endif %}

{#- continuous time based: grab the entry from the postsynaptic continuous time history buffer #}
{%-     if continuous_state_buffering_method == "continuous_time_buffer" %}
#ifdef DEBUG
      std::cout << "Grabbing continuous_variable_history at t = " << start->t_ + __dendritic_delay << "\n";
#endif
        continuous_variable_histentry_{{ paired_neuron_name }} histentry(0.,
{%-         for state_var in paired_neuron.state_vars_that_need_continuous_buffering %}
      {{ state_vars_that_need_continuous_buffering_transformed_iv[state_var] }}{% if not loop.last %},{% endif %}
{%-         endfor %});
        get_entry_from_continuous_variable_history(start->t_ + __dendritic_delay, continuous_history_start, continuous_history_finish, histentry);

{%-         for var_name in paired_neuron.state_vars_that_need_continuous_buffering %}
{%-             set var = utils.get_parameter_variable_by_name(astnode, var_name) %}
        const double __{{ var_name }} = histentry.{{ var_name }};
{%-         endfor %}
{%-     endif %}
{%- endif %}

{%  if vt_ports is defined and vt_ports|length > 0  %}
{%-     set vt_port = vt_ports[0] %}
        process_{{vt_port}}_spikes_( vt_spikes, t0, start->t_ + __dendritic_delay, cp );
        t0 = start->t_ + __dendritic_delay;
{%- endif %}

        /**
         * pre before post update and t_post == t_pre?
        **/

        if (pre_before_post_update and start->t_ == __t_spike - __dendritic_delay)
        {
          pre_before_post_flag = true;
          break;  // this would in any case have been the last post spike to be processed
        }

#ifdef DEBUG
        std::cout << "\tprocessing post spike at t = " << start->t_ << std::endl;
#endif

        /**
         * update synapse internal state from `t_lastspike_` to `start->t_`
        **/

        update_internal_state_(t_lastspike_, (start->t_ + __dendritic_delay) - t_lastspike_, cp);

        timestep += (start->t_ + __dendritic_delay) - t_lastspike_;

        const double t_hist_entry_ms = start->t_;
        auto get_t = [t_hist_entry_ms](){ return t_hist_entry_ms; };   // do not remove, this is in case the predefined time variable ``t`` is used in the NESTML model

{%  filter indent(8, True) %}
{%- if post_ports is defined %}
{%-     for post_port in spiking_post_ports %}
/**
 *  NESTML generated onReceive code block for postsynaptic port "{{post_port}}" begins here!
**/
{%          if synapse.get_on_receive_block(post_port) %}
{%-             set dynamics = synapse.get_on_receive_block(post_port) %}
{%-             with ast = dynamics.get_stmts_body() %}
{%-                 include "directives_cpp/StmtsBody.jinja2" %}
{%-             endwith %}
{%-         endif %}
{%-     endfor %}
{%- endif %}
{%- endfilter %}

        /**
         * internal state has now been fully updated to `start->t_ + __dendritic_delay`
        **/

        t_lastspike_ = start->t_ + __dendritic_delay;
        ++start;
      }
    }
{%- endif %}

    /**
     * update synapse internal state from `t_lastspike_` to `__t_spike`
    **/
{%- if vt_ports is defined and vt_ports|length > 0  %}
{%- set vt_port = vt_ports[0] %}
    process_{{vt_port}}_spikes_( vt_spikes, t_lastspike_, __t_spike, cp );
{%- endif %}

    update_internal_state_(t_lastspike_, __t_spike - t_lastspike_, cp);

    const double _tr_t = __t_spike - __dendritic_delay;

    {
      auto get_t = [__t_spike](){ return __t_spike; };    // do not remove, this is in case the predefined time variable ``t`` is used in the NESTML model

{%- if paired_neuron_name is not none and paired_neuron_name|length > 0 and paired_neuron.state_vars_that_need_continuous_buffering | length > 0 %}

        /**
         * grab state variables from the postsynaptic neuron (at the "current" simulation time, which corresponds to t_pre)
        **/
{%      for var_name in paired_neuron.state_vars_that_need_continuous_buffering %}
{%-         set var_name_post = utils.get_var_name_tuples_of_neuron_synapse_pair(continuous_post_ports, var_name) %}
        const double __{{ var_name }} = ((post_neuron_t*)(__target))->get_{{ var_name_post }}();
{%-     endfor %}
{%- endif %}

{{ printer._expression_printer._simple_expression_printer._variable_printer.set_getter_string("((post_neuron_t*)(__target))->get_%s(_tr_t)") }} {# XXX: TODO: see https://github.com/nest/nestml/issues/1163 #}
{%- for pre_port in pre_ports %}

      /**
       *  NESTML generated onReceive code block for presynaptic port "{{ pre_port }}" begins here!
      **/
{%      if synapse.get_on_receive_block(pre_port) %}
{%-         set dynamics = synapse.get_on_receive_block(pre_port) %}
{%-         with ast = dynamics.get_stmts_body() %}
{%-             filter indent(6, True) %}
{%-                 include "directives_cpp/StmtsBody.jinja2" %}
{%-             endfilter %}
{%-         endwith %}
{%-     endif %}
{%- endfor %}
{{ printer._expression_printer._simple_expression_printer._variable_printer.set_getter_string("((post_neuron_t*)(__target))->get_%s(get_t())") }} {# XXX: TODO: see https://github.com/nest/nestml/issues/1163 #}
    }

    /**
     *  update all convolutions with pre spikes
    **/

{%  for spike_updates_for_port in spike_updates.values() %}
{%-     for spike_update in spike_updates_for_port %}
    {{ printer.print(spike_update.get_variable()) }} += 1.;    // XXX: TODO: increment with initial value instead of 1
{%-     endfor %}
{%- endfor %}


    /**
     *  in case pre and post spike time coincide and pre update takes priority
    **/

    if (pre_before_post_flag)
    {
      auto get_t = [__t_spike](){ return __t_spike; };    // do not remove, this is in case the predefined time variable ``t`` is used in the NESTML model

{%  if paired_neuron_name is not none and paired_neuron_name|length > 0 and paired_neuron.state_vars_that_need_continuous_buffering | length > 0 %}

        /**
         * grab state variables from the postsynaptic neuron (at the "current" simulation time, which corresponds to t_pre)
        **/
{%      for var_name in paired_neuron.state_vars_that_need_continuous_buffering %}
{%-         set var_name_post = utils.get_var_name_tuples_of_neuron_synapse_pair(continuous_post_ports, var_name) %}
        const double __{{ var_name }} = ((post_neuron_t*)(__target))->get_{{ var_name_post }}();
{%-     endfor %}
{%- endif %}

{%- filter indent(6, True) %}
{%- if post_ports is defined %}
{%-     for post_port in spiking_post_ports %}
/**
 *  NESTML generated onReceive code block for postsynaptic port "{{ post_port }}" begins here!
**/
{%-         if synapse.get_on_receive_block(post_port) %}
{%-             set dynamics = synapse.get_on_receive_block(post_port) %}
{%-             with ast = dynamics.get_stmts_body() %}
{%-                 include "directives_cpp/StmtsBody.jinja2" %}
{%-             endwith %}
{%-         endif %}
{%-     endfor %}
{%- endif %}
{%- endfilter %}
    }

    /**
     *  synapse internal state has now been fully updated to `__t_spike`
    **/

    t_lastspike_ = __t_spike;

{%- if not (nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2")
        or nest_version.startswith("v3.3") or nest_version.startswith("v3.4") or nest_version.startswith("v3.5") or nest_version.startswith("v3.6")) %}
    return true;
{%- endif %}
  }

  void get_status( DictionaryDatum& d ) const;

  void set_status( const DictionaryDatum& d, ConnectorModel& cm );

{%- if norm_rng %}
{%- if nest_version.startswith("v2") %}
  librandom::NormalRandomDev normal_dev_; //!< random deviate generator
{%- else %}
  nest::normal_distribution normal_dev_; //!< random deviate generator
{%- endif %}
{%- endif %}
};

{%- if not (nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2")
        or nest_version.startswith("v3.3") or nest_version.startswith("v3.4") or nest_version.startswith("v3.5") or nest_version.startswith("v3.6")) %}
void
register_{{ synapseName }}( const std::string& name )
{
  nest::register_connection_model< {{ synapseName }} >( name );
}
{%- endif %}

{%- if not (nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2") or nest_version.startswith("v3.3") or nest_version.startswith("v3.4")) %}
template < typename targetidentifierT >
constexpr ConnectionModelProperties {{synapseName}}< targetidentifierT >::properties;

{%- endif %}
{%- if vt_ports is defined and vt_ports|length > 0  %}
{%- set vt_port = vt_ports[0] %}
template < typename targetidentifierT >
void
{{synapseName}}< targetidentifierT >::process_{{vt_port}}_spikes_( const std::vector< spikecounter >& vt_spikes,
    double t0,
    double t1,
    const {{synapseName}}CommonSynapseProperties& cp )
{
#ifdef DEBUG
  std::cout << "[synapse " << this << "] {{ synapseName }}::process_{{ vt_port }}_spikes_(): t0 = " << t0 << ", t1 = " << t1 << "\n";
#endif
  // process dopa spikes in (t0, t1]
  // propagate weight from t0 to t1
  if ( ( vt_spikes.size() > vt_spikes_idx_ + 1 )
    and ( t1 - vt_spikes[ vt_spikes_idx_ + 1 ].spike_time_ > -1.0 * kernel().connection_manager.get_stdp_eps() ) )
  {
    // there is at least 1 dopa spike in (t0, t1]
    // propagate up to first dopa spike
    update_internal_state_(t0, vt_spikes[ vt_spikes_idx_ + 1 ].spike_time_ - t0, cp );
    ++vt_spikes_idx_;

    /**
     *  NESTML generated onReceive code block for volume transmitter synaptic port "{{vt_port}}" begins here!
    **/

{%- filter indent(4, True) %}
{%-     set dynamics = synapse.get_on_receive_block(vt_port) %}
{%-     with ast = dynamics.get_stmts_body() %}
{%-         include "directives_cpp/StmtsBody.jinja2" %}
{%-     endwith %}
{%- endfilter %}
    // process remaining dopa spikes in (t0, t1]
    double cd;
    while ( ( vt_spikes.size() > vt_spikes_idx_ + 1 )
      and ( t1 - vt_spikes[ vt_spikes_idx_ + 1 ].spike_time_ > -1.0 * kernel().connection_manager.get_stdp_eps() ) )
    {
      // propagate up to next dopa spike
      update_internal_state_(vt_spikes[ vt_spikes_idx_ ].spike_time_,
                             vt_spikes[ vt_spikes_idx_ + 1 ].spike_time_ - vt_spikes[ vt_spikes_idx_ ].spike_time_,
                             cp );
      ++vt_spikes_idx_;

      /**
       *  Begin NESTML generated onReceive code block for volume transmitter synaptic port "{{vt_port}}"
      **/
{%- filter indent(6, True) %}
{%-     set dynamics = synapse.get_on_receive_block(vt_port) %}
{%-     with ast = dynamics.get_stmts_body() %}
{%-         include "directives_cpp/StmtsBody.jinja2" %}
{%-     endwith %}
{%- endfilter %}

      /**
       *  End NESTML generated onReceive code block for volume transmitter synaptic port "{{vt_port}}"
      **/
    }

    // propagate up to t1
    update_internal_state_(vt_spikes[ vt_spikes_idx_ ].spike_time_,
                           t1 - vt_spikes[ vt_spikes_idx_ ].spike_time_,
                           cp );
  }
  else
  {
    // no dopamine spikes in (t0, t1]
    update_internal_state_( t0, t1 - t0, cp );
  }
}
{%- endif %}


template < typename targetidentifierT >
void
{{synapseName}}< targetidentifierT >::get_status( DictionaryDatum& __d ) const
{
  ConnectionBase::get_status( __d );
  def< long >( __d, names::size_of, sizeof( *this ) );

  // parameters and state variables
{%- filter indent(2,True) %}
{%- for variable_symbol in synapse.get_parameter_symbols() + synapse.get_state_symbols() %}
{%-     set variable = utils.get_variable_by_name(astnode, variable_symbol.get_symbol_name()) %}
{%-     set isHomogeneous = PyNestMLLexer["DECORATOR_HOMOGENEOUS"] in variable_symbol.get_decorators() %}
{%-     if not isHomogeneous %}
{%-         if variable.get_name() == nest_codegen_opt_delay_variable %}
{#-             special case for NEST special variable delay #}
def< {{ declarations.print_variable_type(variable_symbol) }} >( __d, names::delay, {{ printer.print(variable) }} );    // NEST special case for delay variable
def<double>(__d, nest::{{ synapseName }}_names::_{{ nest_codegen_opt_delay_variable }}, {{ printer.print(variable) }});
{#-             special case for NEST special variable weight #}
{%-         elif variable.get_name() == synapse_weight_variable %}
def< {{ declarations.print_variable_type(variable_symbol) }} >( __d, names::weight, {{ printer.print(variable) }} );    // NEST special case for weight variable
def< {{ declarations.print_variable_type(variable_symbol) }} >( __d, nest::{{ synapseName }}_names::_{{ synapse_weight_variable }}, {{ printer.print(variable) }} );    // NEST special case for weight variable
{%-         else %}
{%-             include "directives_cpp/WriteInDictionary.jinja2" %}
{%-         endif %}
{%-     endif %}
{%- endfor %}
{%- endfilter %}
}

template < typename targetidentifierT >
void
{{synapseName}}< targetidentifierT >::set_status( const DictionaryDatum& __d,
  ConnectorModel& cm )
{
{%- if synapse_weight_variable|length > 0 and synapse_weight_variable != "weight" %}
  if (__d->known(nest::{{ synapseName }}_names::_{{ synapse_weight_variable }}) and __d->known(nest::names::weight))
  {
    throw BadProperty( "To prevent inconsistencies, please set either 'weight' or '{{ synapse_weight_variable }}' variable; not both at the same time." );
  }
{%- endif %}

{%- if nest_codegen_opt_delay_variable != "delay" %}
  if (__d->known(nest::{{ synapseName }}_names::_{{ nest_codegen_opt_delay_variable }}) and __d->known(nest::names::delay))
  {
    throw BadProperty( "To prevent inconsistencies, please set either 'delay' or '{{ nest_codegen_opt_delay_variable }}' variable; not both at the same time." );
  }
{%- endif %}

  // call parent class method
  ConnectionBase::set_status( __d, cm );

  // state variables and parameters
{%- filter indent(2,True) %}
{%- for variable_symbol in synapse.get_state_symbols() + synapse.get_parameter_symbols() %}
{%-     set isHomogeneous = PyNestMLLexer["DECORATOR_HOMOGENEOUS"] in variable_symbol.get_decorators() %}
{%-     set variable = utils.get_variable_by_name(astnode, variable_symbol.get_symbol_name()) %}
{%-     if not isHomogeneous and not is_delta_kernel(synapse.get_kernel_by_name(variable_symbol.name)) and not variable_symbol.is_inline_expression %}
{%-         if variable.get_name() == nest_codegen_opt_delay_variable %}
// special treatment of NEST delay
double tmp_{{ nest_codegen_opt_delay_variable }} = get_delay();
updateValue<double>(__d, nest::{{ synapseName }}_names::_{{ nest_codegen_opt_delay_variable }}, tmp_{{nest_codegen_opt_delay_variable}});
{%-         elif variable.get_name() == synapse_weight_variable %}
// special treatment of NEST weight
double tmp_{{ synapse_weight_variable }} = get_weight();
if (__d->known(nest::{{ synapseName }}_names::_{{ synapse_weight_variable }}))
{
  updateValue<double>(__d, nest::{{ synapseName }}_names::_{{ synapse_weight_variable }}, tmp_{{synapse_weight_variable}});
}
if (__d->known(nest::names::weight))
{
  updateValue<double>(__d, nest::names::weight, tmp_{{synapse_weight_variable}});
}
{%-         else %}
{%-             include "directives_cpp/ReadFromDictionaryToTmp.jinja2" %}
{%-         endif %}
{%-     endif %}
{%- endfor %}
{%- endfilter %}

  // We now know that (ptmp, stmp) are consistent. We do not
  // write them back to (P_, S_) before we are also sure that
  // the properties to be set in the parent class are internally
  // consistent.
  ConnectionBase::set_status( __d, cm );

  // if we get here, temporaries contain consistent set of properties

  // set state and parameters
{%- filter indent(2,True) %}
{%- for variable_symbol in synapse.get_state_symbols() + synapse.get_parameter_symbols() %}
{%-     set variable = utils.get_variable_by_name(astnode, variable_symbol.get_symbol_name()) %}
{%-     set isHomogeneous = PyNestMLLexer["DECORATOR_HOMOGENEOUS"] in variable_symbol.get_decorators() %}
{%-     if not isHomogeneous and not is_delta_kernel(synapse.get_kernel_by_name(variable_symbol.name)) %}
{%-         if variable.get_name() == nest_codegen_opt_delay_variable %}
// special treatment of NEST delay
set_delay(tmp_{{ nest_codegen_opt_delay_variable }});
{%-         else %}
{%-             include "directives_cpp/AssignTmpDictionaryValue.jinja2" %}
{%-         endif %}
{%-     endif %}
{%- endfor %}
{%- endfilter %}

{%- if synapse.get_parameter_invariants() | length > 0 %}
  // check invariants
{% for invariant in synapse.get_parameter_invariants() %}
  if ( !({{printer.print(invariant)}}) )
  {
    throw nest::BadProperty("The constraint '{{nestml_printer.print(invariant)}}' is violated!");
  }
{%- endfor %}
{%- endif %}

  // recompute internal variables in case they are dependent on parameters or state that might have been updated in this call to set_status()
  V_.__h = nest::Time::get_resolution().get_ms();
  recompute_internal_variables();
}

/**
 * NESTML internals block symbols initialisation
**/
template < typename targetidentifierT >
void {{synapseName}}< targetidentifierT >::recompute_internal_variables()
{
  const double __timestep = nest::Time::get_resolution().get_ms();  // do not remove, this is necessary for the timestep() function

{% filter indent(2) %}
{%- for variable_symbol in synapse.get_internal_symbols() %}
{%-     set variable = utils.get_internal_variable_by_name(astnode, variable_symbol.get_symbol_name()) %}
{%-     if not variable_symbol.get_symbol_name() == "__h" %}
{%-         include "directives_cpp/MemberInitialization.jinja2" %}
{%-     endif %}
{%- endfor %}
{%- endfilter %}
}

/**
 * constructor
**/
template < typename targetidentifierT >
{{synapseName}}< targetidentifierT >::{{synapseName}}() : ConnectionBase()
{
  const double __timestep = nest::Time::get_resolution().get_ms();  // do not remove, this is necessary for the timestep() function

  // initial values for parameters
{%- filter indent(2, True) %}
{%- for variable_symbol in synapse.get_parameter_symbols() %}
{%-     set variable = utils.get_parameter_variable_by_name(astnode, variable_symbol.get_symbol_name()) %}
{%-     set isHomogeneous = PyNestMLLexer["DECORATOR_HOMOGENEOUS"] in variable_symbol.get_decorators() %}
{%-     if not isHomogeneous %}
{%-         if variable.get_name() != nest_codegen_opt_delay_variable %}
{%-             include "directives_cpp/MemberInitialization.jinja2" %}
{%-         endif %}
{%-     endif %}
{%- endfor %}
{%- endfilter %}

  V_.__h = nest::Time::get_resolution().get_ms();
  recompute_internal_variables();

  // initial values for state variables
{%- filter indent(2, True) %}
{%- for variable_symbol in synapse.get_state_symbols() %}
{%-     set variable = utils.get_state_variable_by_name(astnode, variable_symbol.get_symbol_name()) %}
{%-     if variable.get_name() != synapse_weight_variable and variable.get_name() != nest_codegen_opt_delay_variable %}
{%-         include "directives_cpp/MemberInitialization.jinja2" %}
{%-     endif %}
{%- endfor %}
{%- endfilter %}

{%- if synapse_weight_variable | length > 0 %}
{%-     set variable_symbol = synapse.get_scope().resolve_to_symbol(synapse_weight_variable, SymbolKind.VARIABLE) %}
{%-     set isHomogeneous = PyNestMLLexer["DECORATOR_HOMOGENEOUS"] in variable_symbol.get_decorators() %}
{%-     if not isHomogeneous %}
  // special treatment of NEST weight
  set_weight({{ printer.print(variable_symbol.get_initial_value()) }});
{%-     endif %}
{%- endif %}

  t_lastspike_ = 0.;
{%- if vt_ports is defined and vt_ports|length > 0  %}
  t_last_update_ = 0.;
{%- endif %}
}

/**
 * copy constructor
**/
template < typename targetidentifierT >
{{synapseName}}< targetidentifierT >::{{synapseName}}( const {{synapseName}}< targetidentifierT >& rhs )
: ConnectionBase( rhs )
{
  // parameters
{%- for variable_symbol in synapse.get_parameter_symbols() %}
{%-     set isHomogeneous = PyNestMLLexer["DECORATOR_HOMOGENEOUS"] in variable_symbol.get_decorators() %}
{%-     set variable = utils.get_parameter_variable_by_name(astnode, variable_symbol.get_symbol_name()) %}
{%-     if not isHomogeneous and variable.get_name() != synapse_weight_variable and variable.get_name() != nest_codegen_opt_delay_variable %}
  {{ printer.print(variable) }} = rhs.{{ printer.print(variable) }};
{%-     endif %}
{%- endfor %}

  // state variables
{%- for variable_symbol in synapse.get_state_symbols() %}
{%-     set variable = utils.get_state_variable_by_name(astnode, variable_symbol.get_symbol_name()) %}
{%-     if variable.get_name() != synapse_weight_variable and variable.get_name() != nest_codegen_opt_delay_variable %}
  {{ printer.print(variable) }} = rhs.{{ printer.print(variable) }};
{%-     endif %}
{%- endfor %}

{%- if vt_ports is defined and vt_ports|length > 0  %}
  t_last_update_ = rhs.t_last_update_;
{%- endif %}
  t_lastspike_ = rhs.t_lastspike_;

  // special treatment of NEST delay
  set_delay(rhs.get_delay());
{%- if synapse_weight_variable | length > 0 %}
{%-     set variable_symbol = synapse.get_scope().resolve_to_symbol(variable.get_complete_name(), SymbolKind.VARIABLE) %}
{%-     set isHomogeneous = PyNestMLLexer["DECORATOR_HOMOGENEOUS"] in variable_symbol.get_decorators() %}
{%-     if not isHomogeneous %}
  // special treatment of NEST weight
  set_weight(rhs.get_weight());
{%-     endif %}
{%- endif %}
}

template < typename targetidentifierT >
inline void
{{synapseName}}< targetidentifierT >::update_internal_state_(double t_start, double timestep, const {{synapseName}}CommonSynapseProperties& cp)
{
    if (timestep < 1E-12)
    {
#ifdef DEBUG
        std::cout << "[synapse " << this << "] {{ synapseName }}: update_internal_state_() called with dt < 1E-12; skipping update" << std::endl;
#endif
        return;
    }

    const double __timestep = timestep;  // do not remove, this is necessary for the timestep() function
    auto get_t = [t_start](){ return t_start; };   // do not remove, this is in case the predefined time variable ``t`` is used in the NESTML model

#ifdef DEBUG
    std::cout << "[synapse " << this << "] {{ synapseName }}: Updating internal state: t_start = " << t_start << ", dt = " << timestep << std::endl;
#endif

    V_.__h = timestep;
    assert(V_.__h > 0);
    recompute_internal_variables();

    /**
     * Begin NESTML generated code for the update block
    **/

{%- if synapse.get_update_blocks() %}
{%-     filter indent(2) %}
{%-         for block in synapse.get_update_blocks() %}
{%-             set ast = block.get_stmts_body() %}
{%-             if ast.print_comment('*')|length > 1 %}
/*
 {{ast.print_comment('*')}}
 */
{%-             endif %}
{%-             include "directives_cpp/StmtsBody.jinja2" %}
{%-         endfor %}
{%-     endfilter %}
{%- endif %}

    /**
     * End NESTML generated code for the update block
    **/

{%- if vt_ports is defined and vt_ports|length > 0  %}
    t_last_update_ = t_start + timestep;
{%- endif %}
}

{%- if vt_ports is defined and vt_ports|length > 0  %}
/**
 * Update to end of timestep ``t_trig``, while processing vt spikes and post spikes
**/
template < typename targetidentifierT >
{%- if not (nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2") or nest_version.startswith("v3.3") or nest_version.startswith("v3.4")) %}
inline void
{{synapseName}}< targetidentifierT >::trigger_update_weight( size_t t,
  const std::vector< spikecounter >& vt_spikes,
  const double t_trig,
  const CommonPropertiesType& cp )
{%- else %}
{{synapseName}}< targetidentifierT >::trigger_update_weight( thread t,
  const std::vector< spikecounter >& vt_spikes,
  const double t_trig,
  const CommonPropertiesType& cp )
{%- endif %}
{
  // propagate all state variables in the synapse to time t_trig

#ifdef DEBUG
    std::cout << "[synapse " << this << "] {{ synapseName }}::trigger_update_weight(): t = " << t_trig << std::endl;
#endif

  // purely dendritic delay
  double dendritic_delay = get_delay();

  // get spike history in relevant range (t_last_update, t_trig] from postsyn. neuron
  std::deque< histentry__{{ paired_neuron_name }} >::iterator start;
  std::deque< histentry__{{ paired_neuron_name }} >::iterator finish;
  static_cast<{{ paired_neuron_name }}*>(get_target(t))->get_history__( t_last_update_ - dendritic_delay, t_trig - dendritic_delay, &start, &finish );

  // facilitation due to postsyn. spikes since last update
  double t0 = t_last_update_;
  // double minus_dt;
  double timestep = 0;

  while ( start != finish )
  {
{%- for vt_port in vt_ports %}
{%- set vt_port = vt_ports[0] %}
    process_{{vt_port}}_spikes_( vt_spikes, t0, start->t_ + dendritic_delay, cp );
{%- endfor %}

#ifdef DEBUG
    std::cout << "[synapse " << this << "] {{ synapseName }}: processing post spike from " << t_last_update_ << " to " << start->t_ + dendritic_delay << std::endl;
#endif

    /**
     * update synapse internal state from `t_last_update_` to `start->t_ + dendritic_delay`
    **/

    update_internal_state_(t_last_update_,
                           (start->t_ + dendritic_delay) - t_last_update_,
                           cp);

    const double _tr_t = start->t_;
{%- filter indent(6, True) %}
{%- if post_ports is defined %}
{%-     for post_port in spiking_post_ports %}
    /**
     *  NESTML generated onReceive code block for postsynaptic port "{{post_port}}" begins here!
    **/

{%-         set dynamics = synapse.get_on_receive_block(post_port) %}
{%-         with ast = dynamics.get_stmts_body() %}
{%-             include "directives_cpp/StmtsBody.jinja2" %}
{%-         endwith %}
{%-     endfor %}
{%- endif %}
{%- endfilter %}

    /**
     * internal state has now been fully updated to `start->t_ + dendritic_delay`
    **/

    t0 = start->t_ + dendritic_delay;
    // minus_dt = t_last_update_ - t0;
    t_lastspike_ = start->t_ + dendritic_delay;
    ++start;
  }

  /**
    * update synapse internal state from `t_lastspike_` to `t_trig`
  **/

{%- for vt_port in vt_ports %}
{%- set vt_port = vt_ports[0] %}
  process_{{vt_port}}_spikes_( vt_spikes, t_lastspike_, t_trig, cp );
{%- endfor %}
#ifdef DEBUG
  std::cout << "[synapse " << this << "] {{ synapseName }}::trigger_update_weight(): \tupdating from " << t_lastspike_ << " to " << t_trig + dendritic_delay << std::endl;
#endif

  // N.B. call to update_internal state is not needed; this is already taken care of by ``process_{{vt_port}}_spikes_()``
  // update_internal_state_(t_lastspike_,
  //                        t_trig - t_lastspike_,
  //                        cp);

  vt_spikes_idx_ = 0;
  t_lastspike_ = t_trig;
}


{%- endif %}

} // namespace

#endif /* #ifndef {{synapseName.upper()}}_H */
