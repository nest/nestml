
{#
NeuronClass.jinja2

This file is part of NEST.

Copyright (C) 2004 The NEST Initiative

NEST is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 2 of the License, or
(at your option) any later version.

NEST is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with NEST.  If not, see <http://www.gnu.org/licenses/>.
#}
{%- import 'directives_cpp/BufferInitialization.jinja2' as buffer_initialization with context %}
{%- import 'directives_cpp/FunctionDeclaration.jinja2' as function_declaration with context %}
{%- import 'directives_cpp/VectorSizeParameter.jinja2' as vector_size_parameter with context %}
{%- import 'directives_cpp/RportToBufferIndexEntry.jinja2' as rport_to_port_map_entry with context %}

{%- if tracing %}/* generated by {{self._TemplateReference__context.name}} */{% endif -%}
/*
 *  {{ neuronName }}.cpp
 *
 *  This file is part of NEST.
 *
 *  Copyright (C) 2004 The NEST Initiative
 *
 *  NEST is free software: you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License as published by
 *  the Free Software Foundation, either version 2 of the License, or
 *  (at your option) any later version.
 *
 *  NEST is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU General Public License for more details.
 *
 *  You should have received a copy of the GNU General Public License
 *  along with NEST.  If not, see <http://www.gnu.org/licenses/>.
 *
 *  Generated from NESTML at time: {{now}}
**/

// C++ includes:
#include <limits>

// Includes from libnestutil:
#include "numerics.h"

// Includes from nestkernel:
#include "exceptions.h"
#include "kernel_manager.h"
#include "nest_impl.h"
#include "universal_data_logger_impl.h"

// Includes from sli:
#include "dict.h"
#include "dictutils.h"
#include "doubledatum.h"
#include "integerdatum.h"
#include "lockptrdatum.h"

#include "{{ neuronName }}.h"

// uncomment the next line to enable printing of detailed debug information
// #define DEBUG

{%- if state_vars_that_need_continuous_buffering | length > 0 %}
{%-     if continuous_state_buffering_method == "continuous_time_buffer" %}
continuous_variable_histentry_{{ neuronName }}::continuous_variable_histentry_{{ neuronName }}( double t,
{%-         for state_var in state_vars_that_need_continuous_buffering %}
      double {{ state_var }}{% if not loop.last %}, {% endif %}
{%-         endfor %} )
: t_( t )
, access_counter_ ( 0 )
{%-         for state_var in state_vars_that_need_continuous_buffering %}
, {{ state_var }}( {{ state_var }} )
{%-         endfor %}
{
}
{%-     endif %}
{%- endif %}

{%- if not (nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2")
        or nest_version.startswith("v3.3") or nest_version.startswith("v3.4") or nest_version.startswith("v3.5") or nest_version.startswith("v3.6")) %}
void
register_{{ neuronName }}( const std::string& name )
{
  nest::register_node_model< {{ neuronName }} >( name );
}
{%- endif %}

// ---------------------------------------------------------------------------
//   Recordables map
// ---------------------------------------------------------------------------
{%- if not has_state_vectors %}
nest::RecordablesMap<{{ neuronName }}> {{ neuronName }}::recordablesMap_;
{%- endif %}
namespace nest
{

  // Override the create() method with one call to RecordablesMap::insert_()
  // for each quantity to be recorded.
{%- if has_state_vectors %}
template <> void DynamicRecordablesMap<{{ neuronName }}>::create({{ neuronName }}& host)
{%- else %}
template <> void RecordablesMap<{{ neuronName }}>::create()
{%- endif %}
  {

{%- if recordable_state_variables|length > 0 %}
{%- if has_state_vectors %}
{%-   for variable in recordable_state_variables %}
{%-     if not variable.has_vector_parameter() %}
    insert("{{variable.get_complete_name()}}", host.get_data_access_functor( {{ neuronName }}::State_::{{ printer_no_origin.print(utils.get_state_variable_by_name(astnode, variable.get_complete_name())).upper() }} ));
{%-     endif %}
{%-   endfor %}
{%- else %}
    // add state variables to recordables map
{%-   for variable in recordable_state_variables %}
   insert_({{names_namespace}}::_{{ variable.get_complete_name() }}, &{{ neuronName }}::get_{{ printer_no_origin.print(variable) }});
{%-   endfor %}
{%- endif %}
{%- endif %}

{%- if recordable_inline_expressions|length > 0 %}
    // add recordable inline expressions to recordables map
{%- for variable_symbol in recordable_inline_expressions %}
{%-     set variable = utils.get_variable_by_name(astnode, variable_symbol.get_symbol_name()) %}
	insert_({{ names_namespace }}::_{{ variable_symbol.get_symbol_name() }}, &{{ neuronName }}::{{ printer_no_origin.print(variable)[:-2] }});
{%- endfor %}
{%- endif %}

    // Add vector variables
{%- filter indent(2,True) %}
{%- if has_state_vectors %}
    host.insert_recordables();
{%- endif %}
{%- endfilter %}
  }
}

{%- if neuron.get_spike_input_ports()|length > 1 or neuron.is_multisynapse_spikes() %}
std::vector< std::tuple< int, int > > {{ neuronName }}::rport_to_nestml_buffer_idx =
{
{%-   for key, ports in utils.get_spike_input_ports_in_pairs(neuron).items() %}
{%-   set ns = namespace(rport=key) %}
{%-     if ports[0].has_vector_parameter() %}
{%-     set size = utils.get_numeric_vector_size(ports[0]) %}
{%-         for i in range(size) %}
  {{ rport_to_port_map_entry.RportToBufferIndexEntry(ports, ns.rport, index=i) }}
{%-         set ns.rport = ns.rport + 1 %}
{%-         endfor %}
{%-     else %}
  {{ rport_to_port_map_entry.RportToBufferIndexEntry(ports, ns.rport) }}
{%-     endif %}
{%-   endfor %}
};
{%- endif %}

{%- if has_state_vectors %}
  std::string {{ neuronName }}::get_var_name(size_t elem, std::string var_name)
  {
    std::stringstream n;
    n << var_name << elem;
    return n.str();
  }

  void {{ neuronName }}::insert_recordables(size_t first)
  {
{%- for variable in neuron.get_vector_state_symbols() %}
{%-   set node = utils.get_state_variable_by_name(astnode, variable.get_symbol_name()) %}
      for (size_t i = 0; i < {{vector_size_parameter.VectorSizeParameter(variable, true)}}; i++)
      {
        size_t elem = {{ neuronName }}::State_::{{ printer_no_origin.print(node).upper() }} + i;
        recordablesMap_.insert(get_var_name(i, "{{ printer_no_origin.print(node).upper() }}_"), this->get_data_access_functor(elem));
      }
{%- endfor %}
  }

  nest::DataAccessFunctor< {{ neuronName }} >
  {{ neuronName }}::get_data_access_functor( size_t elem )
  {
    return nest::DataAccessFunctor< {{ neuronName }} >( *this, elem );
  }
{%- endif %}

// ---------------------------------------------------------------------------
//   Default constructors defining default parameters and state
//   Note: the implementation is empty. The initialization is of variables
//   is a part of {{ neuronName }}'s constructor.
// ---------------------------------------------------------------------------

{{ neuronName }}::Parameters_::Parameters_()
{
}

{{ neuronName }}::State_::State_()
{
}

// ---------------------------------------------------------------------------
//   Parameter and state extractions and manipulation functions
// ---------------------------------------------------------------------------

{{ neuronName }}::Buffers_::Buffers_({{ neuronName }} &n):
  logger_(n)
{%- if neuron.get_spike_input_ports()|length > 0 %}
  , spike_inputs_( std::vector< nest::RingBuffer >( NUM_SPIKE_RECEPTORS ) )
  , spike_inputs_grid_sum_( std::vector< double >( NUM_SPIKE_RECEPTORS ) )
  , spike_input_received_( std::vector< nest::RingBuffer >( NUM_SPIKE_RECEPTORS ) )
  , spike_input_received_grid_sum_( std::vector< double >( NUM_SPIKE_RECEPTORS ) )
{%- endif %}
{%- if uses_numeric_solver %}
{%-     if numeric_solver == "rk45" %}
  , __s( nullptr ), __c( nullptr ), __e( nullptr )
{%-     endif %}
{%- endif %}
{
  // Initialization of the remaining members is deferred to init_buffers_().
}

{{ neuronName }}::Buffers_::Buffers_(const Buffers_ &, {{ neuronName }} &n):
  logger_(n)
{%- if neuron.get_spike_input_ports()|length > 0 %}
  , spike_inputs_( std::vector< nest::RingBuffer >( NUM_SPIKE_RECEPTORS ) )
  , spike_inputs_grid_sum_( std::vector< double >( NUM_SPIKE_RECEPTORS ) )
  , spike_input_received_( std::vector< nest::RingBuffer >( NUM_SPIKE_RECEPTORS ) )
  , spike_input_received_grid_sum_( std::vector< double >( NUM_SPIKE_RECEPTORS ) )
{%- endif %}
{%- if uses_numeric_solver %}
{%-     if numeric_solver == "rk45" %}
  , __s( nullptr ), __c( nullptr ), __e( nullptr )
{%-     endif %}
{%- endif %}
{
  // Initialization of the remaining members is deferred to init_buffers_().
}

// ---------------------------------------------------------------------------
//   Default constructor for node
// ---------------------------------------------------------------------------

{{ neuronName }}::{{ neuronName }}():{{neuron_parent_class}}(), P_(), S_(), B_(*this)
{
  init_state_internal_();

{%- if has_state_vectors %}
  recordablesMap_.create(*this);
{%- else %}
  recordablesMap_.create();
{%- endif %}

{%- if nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2") or nest_version.startswith("v3.3") %}
  calibrate();
{%- else %}
  pre_run_hook();
{%- endif %}
}

// ---------------------------------------------------------------------------
//   Copy constructor for node
// ---------------------------------------------------------------------------

{{ neuronName }}::{{ neuronName }}(const {{ neuronName }}& __n):
  {{neuron_parent_class}}(), P_(__n.P_), S_(__n.S_), B_(__n.B_, *this)
{
  // copy parameter struct P_
{%- for parameter in neuron.get_parameter_symbols() %}
{%-   set node = utils.get_parameter_variable_by_name(astnode, parameter.get_symbol_name()) %}
  P_.{{ printer_no_origin.print(node) }} = __n.P_.{{ printer_no_origin.print(node) }};
{%- endfor %}

  // copy state struct S_
{%- for init in neuron.get_state_symbols() %}
{%-   if not is_delta_kernel(neuron.get_kernel_by_name(init.name)) %}
{%-   set node = utils.get_state_variable_by_name(astnode, init.get_symbol_name()) %}
  {{ nest_codegen_utils.print_symbol_origin(init, node) % printer_no_origin.print(node) }} = __n.{{ nest_codegen_utils.print_symbol_origin(init, node) % printer_no_origin.print(node) }};
{%-   endif %}
{%- endfor %}

  // copy internals V_
{%- for internal in neuron.get_internal_symbols() %}
{%-   set node = utils.get_internal_variable_by_name(astnode, internal.get_symbol_name()) %}
  V_.{{ printer_no_origin.print(node) }} = __n.V_.{{ printer_no_origin.print(node) }};
{%- endfor %}

{%- if has_state_vectors %}
  recordablesMap_.create(*this);
{%- endif %}
{%- if paired_synapse is defined %}
  n_incoming_ = __n.n_incoming_;
  max_delay_ = __n.max_delay_;
  last_spike_ = __n.last_spike_;

  // cache initial values
{%- for var_name in transferred_variables %}
{%-     set var = utils.get_variable_by_name(astnode, var_name) %}
{%-     set variable_symbol = transferred_variables_syms[var_name] %}
{%-     if not var_name == variable_symbol.get_symbol_name() %}
{{ raise('Error in resolving variable to symbol') }}
{%-     endif %}
  {{var_name}}__iv = {{printer.print(var)}};
{%- endfor %}
{%- endif %}
}

// ---------------------------------------------------------------------------
//   Destructor for node
// ---------------------------------------------------------------------------

{{ neuronName }}::~{{ neuronName }}()
{
{%- if uses_numeric_solver %}
{%-     if numeric_solver == "rk45" %}
  // GSL structs may not have been allocated, so we need to protect destruction

  if (B_.__s)
  {
    gsl_odeiv_step_free( B_.__s );
  }

  if (B_.__c)
  {
    gsl_odeiv_control_free( B_.__c );
  }

  if (B_.__e)
  {
    gsl_odeiv_evolve_free( B_.__e );
  }
{%-     endif %}
{%- endif %}
}

// ---------------------------------------------------------------------------
//   Node initialization functions
// ---------------------------------------------------------------------------

{%- if nest_version.startswith("v2") %}
void {{ neuronName }}::init_state_(const Node& proto)
{
  const {{ neuronName }}& pr = downcast<{{ neuronName }}>(proto);
  S_ = pr.S_;
}
{%- endif %}

{%- if not nest_version.startswith("v2") %}
void {{ neuronName }}::calibrate_time( const nest::TimeConverter& tc )
{
  LOG( nest::M_WARNING,
    "{{ neuronName }}",
    "Simulation resolution has changed. Internal state and parameters of the model have been reset!" );

  init_state_internal_();
}

{%- endif %}
void {{ neuronName }}::init_state_internal_()
{
#ifdef DEBUG
  std::cout << "[neuron " << this << "] {{ neuronName }}::init_state_internal_()" << std::endl;
#endif

  const double __timestep = nest::Time::get_resolution().get_ms();  // do not remove, this is necessary for the timestep() function

{%- if numeric_solver == "rk45" %}
  // by default, integrate all variables with a conservative tolerance, in the sense that we err on the side of integrating very precisely at the expense of extra computation
  P_.__gsl_abs_error_tol = 1e-6;
  P_.__gsl_rel_error_tol = 1e-6;
{%- endif %}

{%- if parameter_vars_with_iv|length > 0 %}
  // initial values for parameters
{%- filter indent(2) %}
{%- for variable in parameter_vars_with_iv %}
{%-     set variable_symbol = variable.get_scope().resolve_to_symbol(variable.get_complete_name(), SymbolKind.VARIABLE) %}
{%-     include "directives_cpp/MemberInitialization.jinja2" %}
{%- endfor %}
{%- endfilter %}
{%- endif %}

  V_.__h = nest::Time::get_resolution().get_ms();
  recompute_internal_variables();

{%- if neuron.get_state_symbols()|length > 0 %}
  // initial values for state variables
{%- filter indent(2) %}
{%- for variable_symbol in neuron.get_state_symbols() %}
{%-     set variable = utils.get_state_variable_by_name(astnode, variable_symbol.get_symbol_name()) %}
{%-     include "directives_cpp/MemberInitialization.jinja2" %}
{%- endfor %}
{%- endfilter %}
{%- endif %}

{%- if paired_synapse is defined %}
  // state variables for archiving state for paired synapse
  n_incoming_ = 0;
  max_delay_ = 0;
  last_spike_ = -1.;

  // cache initial values
{%- for var_name in transferred_variables %}
{%-     set var = utils.get_variable_by_name(astnode, var_name) %}
{%-     set variable_symbol = transferred_variables_syms[var_name] %}
{%-     if not var_name == variable_symbol.get_symbol_name() %}
{{ raise('Error in resolving variable to symbol') }}
{%-     endif %}
  {{var_name}}__iv = {{printer.print(var)}};
{%- endfor %}
{%- endif %}
}

void {{ neuronName }}::init_buffers_()
{
#ifdef DEBUG
  std::cout << "[neuron " << this << "] {{ neuronName }}::init_buffers_()" << std::endl;
#endif

{%- if use_gap_junctions %}
  // resize interpolation_coefficients depending on interpolation order
  const size_t buffer_size =
    nest::kernel().connection_manager.get_min_delay() * ( nest::kernel().simulation_manager.get_wfr_interpolation_order() + 1 );

  B_.interpolation_coefficients.resize( buffer_size, 0.0 );

  B_.last_y_values.resize( nest::kernel().connection_manager.get_min_delay(), 0.0 );

  B_.sumj_g_ij_ = 0.0;
{%- endif %}

{%- if neuron.get_spike_input_ports() | length > 0 %}
  // spike input buffers
  get_spike_inputs_().clear();
  get_spike_inputs_grid_sum_().clear();
  get_spike_input_received_().clear();
  get_spike_input_received_grid_sum_().clear();
{% endif %}
{%- if neuron.get_continuous_input_ports() | length > 0 %}

  // continuous time input buffers
{%-   filter indent(2,True) %}
{%-     for inputPort in neuron.get_continuous_input_ports() %}
{{ buffer_initialization.BufferInitialization(inputPort) }}
{%-     endfor %}
{%-   endfilter %}
{%- endif %}

  B_.logger_.reset();

{% if has_delay_variables %}

  // Initialize helper variables for delay-based variables
{%-   for variable in neuron.get_state_symbols() %}
{%-     if variable.has_delay_parameter() %}
{%-       include "directives_cpp/DelayVariablesInitialization.jinja2" %}
{%-     endif %}
{%-   endfor %}
{%- endif %}
{%- if paired_neuron is defined %}

  clear_history();
{%- endif %}
{%- if uses_numeric_solver %}
{%-     if numeric_solver == "rk45" %}

  if ( not B_.__s )
  {
    B_.__s = gsl_odeiv_step_alloc( gsl_odeiv_step_rkf45, State_::STATE_VEC_SIZE );
  }
  else
  {
    gsl_odeiv_step_reset( B_.__s );
  }

  if ( not B_.__c )
  {
{%- if gsl_adaptive_step_size_controller == "with_respect_to_solution" %}
    B_.__c = gsl_odeiv_control_y_new( P_.__gsl_abs_error_tol, P_.__gsl_rel_error_tol );
{%- elif gsl_adaptive_step_size_controller == "with_respect_to_derivative" %}
    B_.__c = gsl_odeiv_control_yp_new( P_.__gsl_abs_error_tol, P_.__gsl_rel_error_tol );
{%- else %}
{{ raise('Unknown GSL adaptive step size controller value') }}
{%- endif %}
  }
  else
  {
{%- if gsl_adaptive_step_size_controller == "with_respect_to_solution" %}
    gsl_odeiv_control_init( B_.__c, P_.__gsl_abs_error_tol, P_.__gsl_rel_error_tol, 1.0, 0.0 );
{%- elif gsl_adaptive_step_size_controller == "with_respect_to_derivative" %}
    gsl_odeiv_control_init( B_.__c, P_.__gsl_abs_error_tol, P_.__gsl_rel_error_tol, 0.0, 1.0 );
{%- else %}
{{ raise('Unknown GSL adaptive step size controller value') }}
{%- endif %}

  }

  if ( not B_.__e )
  {
    B_.__e = gsl_odeiv_evolve_alloc( State_::STATE_VEC_SIZE );
  }
  else
  {
    gsl_odeiv_evolve_reset( B_.__e );
  }

  // B_.__sys.function = {{ neuronName }}_dynamics; // will be set just prior to the call to gsl_odeiv_evolve_apply()
  B_.__sys.jacobian = nullptr;
  B_.__sys.dimension = State_::STATE_VEC_SIZE;
  B_.__sys.params = reinterpret_cast< void* >( this );
  B_.__step = nest::Time::get_resolution().get_ms();
  B_.__integration_step = nest::Time::get_resolution().get_ms();
{%-     endif %}
{%- endif %}
}

void {{ neuronName }}::recompute_internal_variables(bool exclude_timestep)
{
  const double __timestep = nest::Time::get_resolution().get_ms();  // do not remove, this is necessary for the timestep() function

  if (exclude_timestep)
  {
{%- filter indent(4,True) %}
{%- for internals_block in neuron.get_internals_blocks() %}
{%-     for decl in internals_block.get_declarations() %}
{%-         for variable in decl.get_variables() %}
{%-             if variable.get_complete_name() != "__h" %}
{%-                 set variable_symbol = variable.get_scope().resolve_to_symbol(variable.get_complete_name(), SymbolKind.VARIABLE) %}
{%-                 include "directives_cpp/MemberInitialization.jinja2" %}
{%-             endif %}
{%-         endfor %}
{%-     endfor %}
{%- endfor %}
{%- endfilter %}
  }
  else {
{%- filter indent(4,True) %}
{%- for internals_block in neuron.get_internals_blocks() %}
{%-     for decl in internals_block.get_declarations() %}
{%-         for variable in decl.get_variables() %}
{%-             set variable_symbol = variable.get_scope().resolve_to_symbol(variable.get_complete_name(), SymbolKind.VARIABLE) %}
{%-             include "directives_cpp/MemberInitialization.jinja2" %}
{%-         endfor %}
{%-     endfor %}
{%- endfor %}
{%- endfilter %}
  }
}

{%- if nest_version.startswith("v2") or nest_version.startswith("v3.0") or nest_version.startswith("v3.1") or nest_version.startswith("v3.2") or nest_version.startswith("v3.3") %}
void {{ neuronName }}::calibrate()
{%- else %}
void {{ neuronName }}::pre_run_hook()
{%- endif %}
{
#ifdef DEBUG
  std::cout << "[neuron " << this << "] {{ neuronName }}::pre_run_hook()" << std::endl;
#endif

  B_.logger_.init();

  // parameters might have changed -- recompute internals
  V_.__h = nest::Time::get_resolution().get_ms();
  recompute_internal_variables();

  // buffers B_
{%- if ((neuron.get_spike_input_ports())|length > 0) %}
  B_.spike_inputs_.resize(NUM_SPIKE_RECEPTORS);
  B_.spike_inputs_grid_sum_.resize(NUM_SPIKE_RECEPTORS);
  B_.spike_input_received_.resize(NUM_SPIKE_RECEPTORS);
  B_.spike_input_received_grid_sum_.resize(NUM_SPIKE_RECEPTORS);
{%-   endif %}
}
{%- if neuron.get_functions()|length > 0 %}

// ---------------------------------------------------------------------------
//   Functions defined in the NESTML model
// ---------------------------------------------------------------------------

{%-     for function in neuron.get_functions() %}
{{ function_declaration.FunctionDeclaration(function, neuronName + "::") }}
{
{%-         filter indent(2,True) %}
{%-             with ast = function.get_block() %}
{%-                 include "directives_cpp/Block.jinja2" %}
{%-             endwith %}
{%-         endfilter %}
}
{%-     endfor %}
{%- endif %}

// ---------------------------------------------------------------------------
//   Update and spike handling functions
// ---------------------------------------------------------------------------

{%  if uses_numeric_solver %}
{%-     for ast in utils.get_all_integrate_odes_calls_unique(neuron) %}
{%-         include "directives_cpp/GSLDifferentiationFunction.jinja2" %}
{%-     endfor %}

{%-     if paired_synapse is defined and (purely_numeric_state_variables_moved + analytic_state_variables_moved) | length > 0 %}
{#          for neuron-synapse co-generation: separate integrator for the emulated integrate_odes() calls below in this template #}
{%-         set args = utils.resolve_variables_to_expressions(astnode, purely_numeric_state_variables_moved + analytic_state_variables_moved) %}
{%-         set ast = ASTNodeFactory.create_ast_function_call("integrate_odes", args) %}
{%-         include "directives_cpp/GSLDifferentiationFunction.jinja2" %}
{%-     endif %}

{%- endif %}

{%- if has_delay_variables %}
void {{ neuronName }}::update_delay_variables()
{
#ifdef DEBUG
  std::cout << "[neuron " << this << "] {{ neuronName }}::update_delay_variables()" << std::endl;
#endif
{%-   for variable_symbol in neuron.get_state_symbols() %}
{%-       if variable_symbol.has_delay_parameter() %}
{%-           set variable = utils.get_variable_by_name(astnode, variable_symbol.get_symbol_name()) %}
{%-           include "directives_cpp/UpdateDelayVariables.jinja2" %}
{%-       endif %}
{%-   endfor %}
}

{%-   for variable in neuron.get_state_symbols() %}
{%-     if variable.has_delay_parameter() %}
double {{ neuronName }}::get_delayed_{{variable.get_symbol_name()}}() const
{
    return DV_.delayed_{{variable.get_symbol_name()}}[ DV_.delayed_{{variable.get_symbol_name()}}_idx ];
}
{%-     endif %}
{%-   endfor %}

{%- endif %}

{%- if use_gap_junctions %}
bool {{ neuronName }}::update_( nest::Time const& origin, const long from, const long to, const bool called_from_wfr_update )
{%- else %}
void {{ neuronName }}::update(nest::Time const & origin, const long from, const long to)
{%- endif %}
{
  const double __timestep = nest::Time::get_resolution().get_ms();  // do not remove, this is necessary for the timestep() function

{%- if use_gap_junctions %}
  const size_t interpolation_order = nest::kernel().simulation_manager.get_wfr_interpolation_order();
  const double wfr_tol = nest::kernel().simulation_manager.get_wfr_tol();
  bool wfr_tol_exceeded = false;

  // allocate memory to store the new interpolation coefficients
  // to be sent by gap event
  const size_t buffer_size = nest::kernel().connection_manager.get_min_delay() * ( interpolation_order + 1 );
  std::vector< double > new_coefficients( buffer_size, 0.0 );

  // parameters needed for piecewise interpolation
  double y_i = 0.0, y_ip1 = 0.0, hf_i = 0.0, hf_ip1 = 0.0;
{%- endif %}

  for ( long lag = from ; lag < to ; ++lag )
  {
{% if propagators_are_state_dependent %}
  // the propagators are state dependent; update them!
  V_.__h = nest::Time::get_resolution().get_ms();
  recompute_internal_variables();

{% endif %}

    auto get_t = [origin, lag](){ return nest::Time( nest::Time::step( origin.get_steps() + lag + 1) ).get_ms(); };

#ifdef DEBUG
    std::cout << "[neuron " << this << "] {{ neuronName }}::update{% if use_gap_junctions %}_{% endif %}: handling post spike at t = " << get_t() << std::endl;
#endif

{%- if use_gap_junctions %}
    // B_.lag is needed by GSL stepping function to determine the current section
    B_.lag_ = lag;

{%- if not gap_junction_membrane_potential_variable_is_numeric %}
{#      in case V_m is solved analytically, need to compute __I_gap here so dV_m/dt can be computed below #}
  // set I_gap depending on interpolation order
  double __I_gap = 0.0;

  const double __t_gap = gap_junction_step / nest::Time::get_resolution().get_ms();

  switch ( nest::kernel().simulation_manager.get_wfr_interpolation_order() )
  {
  case 0:
    __I_gap = -B_.sumj_g_ij_ * {{ gap_junction_membrane_potential_variable_cpp }} + B_.interpolation_coefficients[ B_.lag_ ];
    break;

  case 1:
    __I_gap = -B_.sumj_g_ij_ * {{ gap_junction_membrane_potential_variable_cpp }} + B_.interpolation_coefficients[ B_.lag_ * 2 + 0 ]
      + B_.interpolation_coefficients[ B_.lag_ * 2 + 1 ] * __t_gap;
    break;

  case 3:
    __I_gap = -B_.sumj_g_ij_ * {{ gap_junction_membrane_potential_variable_cpp }} + B_.interpolation_coefficients[ B_.lag_ * 4 + 0 ]
      + B_.interpolation_coefficients[ B_.lag_ * 4 + 1 ] * __t_gap
      + B_.interpolation_coefficients[ B_.lag_ * 4 + 2 ] * __t_gap * __t_gap
      + B_.interpolation_coefficients[ B_.lag_ * 4 + 3 ] * __t_gap * __t_gap * __t_gap;
    break;

  default:
    throw nest::BadProperty( "Interpolation order must be 0, 1, or 3." );
  }
{%- endif %}

    if ( called_from_wfr_update )
    {
      y_i = {{ gap_junction_membrane_potential_variable_cpp }};
      if ( interpolation_order == 3 )
      {
        // find dV_m/dt
        gap_junction_step = 0;
        double __I_gap = 0;
{%-     if gap_junction_membrane_potential_variable_is_numeric %}
{#          solved using a numeric solver #}
        double f_temp[ State_::STATE_VEC_SIZE ];
        {{ neuronName }}_dynamics( get_t(), S_.ode_state, f_temp, reinterpret_cast< void* >( this ) );
        hf_i = nest::Time::get_resolution().get_ms() * f_temp[ State_::{{ gap_junction_membrane_potential_variable }} ];
{%-     else %}
{#        solved using an analytic solver #}

{#        the following statements will only assign to temporary variables and not edit the internal state of the neuron #}
{%-         with analytic_state_variables_ = analytic_state_variables %}
{%-             include "directives_cpp/AnalyticIntegrationStep_begin.jinja2" %}
{%-         endwith %}

        hf_i = ({{ gap_junction_membrane_potential_variable }}__tmp - y_i) / nest::Time::get_resolution().get_ms();
{%-     endif %}
      }
    }

{%- endif %}
    /**
     * buffer spikes from spiking input ports
    **/

    for (long i = 0; i < NUM_SPIKE_RECEPTORS; ++i)
    {
      get_spike_inputs_grid_sum_()[i] = get_spike_inputs_()[i].get_value(lag);
      get_spike_input_received_grid_sum_()[i] = get_spike_input_received_()[i].get_value(lag);
    }

{%- if has_delay_variables %}
    /**
     * delay variables
    **/

    update_delay_variables();
{%- endif %}

    /**
     * subthreshold updates of the convolution variables
     *
     * step 1: regardless of whether and how integrate_odes() will be called, update variables due to convolutions
    **/
{% if uses_analytic_solver %}
{%-     for variable_name in analytic_state_variables: %}
{%-         if "__X__" in variable_name %}
{%-             set update_expr = update_expressions[variable_name] %}
{%-             set var_ast = utils.get_variable_by_name(astnode, variable_name)%}
{%-             set var_symbol = var_ast.get_scope().resolve_to_symbol(variable_name, SymbolKind.VARIABLE)%}
{%-             if use_gap_junctions %}
    const {{ type_symbol_printer.print(var_symbol.type_symbol) }} {{variable_name}}__tmp_ = {{ printer.print(update_expr) | replace("B_." + gap_junction_port + "_grid_sum_", "(B_." + gap_junction_port + "_grid_sum_ + __I_gap)") }};
{%-             else %}
    const {{ type_symbol_printer.print(var_symbol.type_symbol) }} {{variable_name}}__tmp_ = {{ printer.print(update_expr) }};
{%-             endif %}
{%-         endif %}
{%-     endfor %}
{%- endif %}


    /**
     * Begin NESTML generated code for the update block(s)
    **/
{%  if neuron.get_update_blocks() %}
{%-     filter indent(2) %}
{%-         for block in neuron.get_update_blocks() %}
{%-             set ast = block.get_block() %}
{%-             if ast.print_comment('*')|length > 1 %}
/*
 {{ast.print_comment('*')}}
 */
{%-             endif %}
{%-             include "directives_cpp/Block.jinja2" %}
{%-         endfor %}
{%-     endfilter %}
{%- endif %}

    /**
     * Begin NESTML generated code for the onReceive block(s)
    **/
{%  for blk in neuron.get_on_receive_blocks() %}
{%-     set inport = blk.get_port_name() %}
    if (B_.spike_input_received_grid_sum_[{{ inport.upper() }} - MIN_SPIKE_RECEPTOR])
    {
      // B_.spike_input_received_[{{ inport.upper() }} - MIN_SPIKE_RECEPTOR] = false;  // no need to reset the flag -- reading from the RingBuffer into the "grid_sum" variables resets the RingBuffer entries
      on_receive_block_{{ blk.get_port_name() }}();
    }
{%- endfor %}

    /**
     * subthreshold updates of the convolution variables
     *
     * step 2: regardless of whether and how integrate_odes() was called, update variables due to convolutions. Set to the updated values at the end of the timestep.
    **/
{%  if uses_analytic_solver %}
{%-     for variable_name in analytic_state_variables: %}
{%-         if "__X__" in variable_name %}
{%-             set update_expr = update_expressions[variable_name] %}
{%-             set var_ast = utils.get_variable_by_name(astnode, variable_name)%}
{%-             set var_symbol = var_ast.get_scope().resolve_to_symbol(variable_name, SymbolKind.VARIABLE)%}
    {{ printer.print(var_ast) }} = {{variable_name}}__tmp_;
{%-         endif %}
{%-     endfor %}
{%- endif %}


{%- if purely_numeric_state_variables_moved %}
  /**
   * numeric state variables moved from synapse to neuron should always be integrated, regardless of calls to integrate_odes() in the neuron
  **/

  // restore the original values
{%-     for variable_name in purely_numeric_state_variables_moved %}
{%-         set var_ast = utils.get_variable_by_name(astnode, variable_name)%}
{{ printer.print(var_ast) }} = {{ variable_name }}__tmp;
{%-     endfor %}

{#          emulate a call to ``integrate_odes(purely_numeric_state_variables_moved)`` #}
{%-         set args = utils.resolve_variables_to_expressions(astnode, purely_numeric_state_variables_moved) %}
{%-         set ast = ASTNodeFactory.create_ast_function_call("integrate_odes", args) %}
{%-         include "directives_cpp/PredefinedFunction_integrate_odes.jinja2" %}
{%- endif %}

    /**
     * spike updates due to convolutions
    **/
{%  filter indent(4) %}
{%-     include "directives_cpp/ApplySpikesFromBuffers.jinja2" %}
{%- endfilter %}

    /**
     * Begin NESTML generated code for the onCondition block(s)
    **/
{% if neuron.get_on_condition_blocks() %}
{%-     for block in neuron.get_on_condition_blocks() %}
    if ({{ printer.print(block.get_cond_expr()) }})
    {
{%-         set ast = block.get_block() %}
{%-         if ast.print_comment('*') | length > 1 %}
/*
 {{ast.print_comment('*')}}
 */
{%-         endif %}
{%-         filter indent(6) %}
{%-             include "directives_cpp/Block.jinja2" %}
{%-         endfilter %}
    }
{%-     endfor %}
{%- endif %}

    /**
     * handle continuous input ports
    **/
{%  for inputPort in neuron.get_continuous_input_ports() %}
{%-     if use_gap_junctions %}
    if ( called_from_wfr_update )
    {
      B_.{{ inputPort.name }}_grid_sum_ = get_{{ inputPort.name }}().get_value_wfr_update(lag);
    }
    else
    {
      B_.{{ inputPort.name }}_grid_sum_ = get_{{ inputPort.name }}().get_value(lag);
    }
{%-     else %}
    B_.{{ inputPort.name }}_grid_sum_ = get_{{ inputPort.name }}().get_value(lag);
{%-     endif %}
{%- endfor %}

{%- if use_gap_junctions %}
    if ( called_from_wfr_update )
    {
      // check if deviation from last iteration exceeds wfr_tol
      wfr_tol_exceeded = wfr_tol_exceeded or fabs( {{ gap_junction_membrane_potential_variable_cpp }} - B_.last_y_values[ lag ] ) > wfr_tol;
      B_.last_y_values[ lag ] = {{ gap_junction_membrane_potential_variable_cpp }};

      // update different interpolations

      // constant term is the same for each interpolation order
      new_coefficients[ lag * ( interpolation_order + 1 ) + 0 ] = y_i;

      switch ( interpolation_order )
      {
        case 0:
          break;

        case 1:
          y_ip1 = {{ gap_junction_membrane_potential_variable_cpp }};

          new_coefficients[ lag * ( interpolation_order + 1 ) + 1 ] = y_ip1 - y_i;
          break;

        case 3:
          // find dV_m/dt
          {
            gap_junction_step = __timestep;
            y_ip1 = {{ gap_junction_membrane_potential_variable_cpp }};
{%-     if gap_junction_membrane_potential_variable_is_numeric %}
            double f_temp[ State_::STATE_VEC_SIZE ];
            {{ neuronName }}_dynamics( get_t(), S_.ode_state, f_temp, reinterpret_cast< void* >( this ) );
            hf_ip1 = nest::Time::get_resolution().get_ms() * f_temp[ State_::{{ gap_junction_membrane_potential_variable }} ];
{%-     else %}
{#          solved using an analytic solver #}

            const double __t_gap = gap_junction_step / nest::Time::get_resolution().get_ms();

            double __I_gap = -B_.sumj_g_ij_ * {{ gap_junction_membrane_potential_variable_cpp }} + B_.interpolation_coefficients[ B_.lag_ * 4 + 0 ]
              + B_.interpolation_coefficients[ B_.lag_ * 4 + 1 ] * __t_gap
              + B_.interpolation_coefficients[ B_.lag_ * 4 + 2 ] * __t_gap * __t_gap
              + B_.interpolation_coefficients[ B_.lag_ * 4 + 3 ] * __t_gap * __t_gap * __t_gap;

{#        the following statements will only assign to temporary variables and not edit the internal state of the neuron #}
{%-         with analytic_state_variables_ = analytic_state_variables %}
{%-             include "directives_cpp/AnalyticIntegrationStep_begin.jinja2" %}
{%-         endwith %}

            hf_ip1 = ({{ gap_junction_membrane_potential_variable }}__tmp - y_ip1) / nest::Time::get_resolution().get_ms();
{%-     endif %}
          }

          new_coefficients[ lag * ( interpolation_order + 1 ) + 1 ] = hf_i;
          new_coefficients[ lag * ( interpolation_order + 1 ) + 2 ] = -3 * y_i + 3 * y_ip1 - 2 * hf_i - hf_ip1;
          new_coefficients[ lag * ( interpolation_order + 1 ) + 3 ] = 2 * y_i - 2 * y_ip1 + hf_i + hf_ip1;
          break;

        default:
          throw nest::BadProperty( "Interpolation order must be 0, 1, or 3." );
      }
    }

    if ( not called_from_wfr_update )
    {
      // voltage logging
      B_.logger_.record_data(origin.get_steps() + lag);
    }
{%-     else %}
    // voltage logging
    B_.logger_.record_data(origin.get_steps() + lag);
{%-     endif %}

{%- if state_vars_that_need_continuous_buffering | length > 0 %}
{%-     if continuous_state_buffering_method == "continuous_time_buffer" %}
#ifdef DEBUG
std::cout << "[neuron " << this << "] Writing history at time " << nest::Time(nest::Time::step( origin.get_steps() + lag + 1 )).get_ms() << "\n";
#endif
    write_continuous_variable_history( nest::Time(nest::Time::step( origin.get_steps() + lag + 1 )),
{%-         for state_var_name in state_vars_that_need_continuous_buffering_transformed %}
{%-             set state_var = utils.get_variable_by_name(astnode, state_var_name) %}
      {{ printer.print(state_var) }}{% if not loop.last %}, {% endif %}
{%-         endfor %}
    );
{%-     endif %}
{%- endif %}
  }

{%- if use_gap_junctions %}
  // if not called_from_wfr_update perform constant extrapolation and reset last_y_values
  if ( not called_from_wfr_update )
  {
    for ( long temp = from; temp < to; ++temp )
    {
      new_coefficients[ temp * ( interpolation_order + 1 ) + 0 ] = {{ gap_junction_membrane_potential_variable_cpp }};
    }

    std::vector< double >( nest::kernel().connection_manager.get_min_delay(), 0.0 ).swap( B_.last_y_values );
  }

  // Send gap-event
  nest::GapJunctionEvent ge;
  ge.set_coeffarray( new_coefficients );
  nest::kernel().event_delivery_manager.send_secondary( *this, ge );

  // Reset variables
  B_.sumj_g_ij_ = 0.0;
  std::vector< double >( buffer_size, 0.0 ).swap( B_.interpolation_coefficients );

  return wfr_tol_exceeded;
{%- endif %}
}

{%- if state_vars_that_need_continuous_buffering | length > 0 %}
{%-     if continuous_state_buffering_method == "continuous_time_buffer" %}
void {{neuronName}}::get_continuous_variable_history( double t1,
  double t2,
  std::deque< continuous_variable_histentry_{{ neuronName }} >::iterator* start,
  std::deque< continuous_variable_histentry_{{ neuronName }} >::iterator* finish )
{
#ifdef DEBUG
  std::cout << "{{neuronName}}::get_continuous_variable_history()" << std::endl;
#endif
  *finish = continuous_variable_history_.end();
  if ( continuous_variable_history_.empty() )
  {
    *start = *finish;
    return;
  }
  else
  {
    std::deque< continuous_variable_histentry_{{ neuronName }} >::iterator runner = continuous_variable_history_.begin();

    // To have a well defined discretization of the integral, we make sure
    // that we exclude the entry at t1 but include the one at t2 by subtracting
    // a small number so that runner->t_ is never equal to t1 or t2.
    while ( ( runner != continuous_variable_history_.end() ) and runner->t_ - 1.0e-6 < t1 )
    {
      ++runner;
    }
    *start = runner;
    while ( ( runner != continuous_variable_history_.end() ) and runner->t_ - 1.0e-6 < t2 )
    {
      ( runner->access_counter_ )++;
      ++runner;
    }
    *finish = runner;
  }
}

void {{neuronName}}::write_continuous_variable_history(nest::Time const &t,
{%-         for state_var in state_vars_that_need_continuous_buffering %}
      const double {{ state_var }}{% if not loop.last %}, {% endif %}
{%-         endfor %})
{
#ifdef DEBUG
  std::cout << "{{neuronName}}::write_continuous_variable_history()" << std::endl;
#endif
  const double t_ms = t.get_ms();

  // prune all entries from history which are no longer needed except the penultimate one. we might still need it.
  auto orig = continuous_variable_history_.size();
  while ( continuous_variable_history_.size() > 1 )
  {
    if ( continuous_variable_history_.front().access_counter_ >= n_incoming_ )
    {
      continuous_variable_history_.pop_front();
    }
    else
    {
      break;
    }
  }

#ifdef DEBUG
  if (continuous_variable_history_.size() != orig) {
    std::cout << "[neuron " << this << "] \tpruning history, original size = " << orig << ", new size = " <<  continuous_variable_history_.size() << "\n";
  }
#endif

  continuous_variable_history_.push_back( continuous_variable_histentry_{{ neuronName }}( t_ms,
{%-         for state_var in state_vars_that_need_continuous_buffering %}
      {{ state_var }}{% if not loop.last %}, {% endif %}
{%-         endfor %}) );
#ifdef DEBUG
  std::cout << "[neuron " << this << "] \thistory size = " << continuous_variable_history_.size() << std::endl;
#endif
}
{%-     endif %}
{%- endif %}

// Do not move this function as inline to h-file. It depends on
// universal_data_logger_impl.h being included here.
void {{ neuronName }}::handle(nest::DataLoggingRequest& e)
{
  B_.logger_.handle(e);
}

{% if has_spike_input %}
void {{ neuronName }}::handle(nest::SpikeEvent &e)
{
#ifdef DEBUG
  std::cout << "[neuron " << this << "] {{ neuronName }}::handle(SpikeEvent)" << std::endl;
#endif

  assert(e.get_delay_steps() > 0);
  assert( e.get_rport() < B_.spike_inputs_.size() );

  double weight = e.get_weight();
  size_t nestml_buffer_idx = 0;
{%- if neuron.get_spike_input_ports()|length > 1 or neuron.is_multisynapse_spikes() %}
  if ( weight >= 0.0 )
  {
    nestml_buffer_idx = std::get<0>(rport_to_nestml_buffer_idx[e.get_rport()]);
  }
  else
  {
    nestml_buffer_idx = std::get<1>(rport_to_nestml_buffer_idx[e.get_rport()]);
    if ( nestml_buffer_idx == {{ neuronName }}::PORT_NOT_AVAILABLE )
    {
      nestml_buffer_idx = std::get<0>(rport_to_nestml_buffer_idx[e.get_rport()]);
    }
    weight = -weight;
  }
{%- endif %}
  B_.spike_inputs_[ nestml_buffer_idx - MIN_SPIKE_RECEPTOR ].add_value(
    e.get_rel_delivery_steps( nest::kernel().simulation_manager.get_slice_origin() ),
    weight * e.get_multiplicity() );
  B_.spike_input_received_[ nestml_buffer_idx - MIN_SPIKE_RECEPTOR ].add_value(
    e.get_rel_delivery_steps( nest::kernel().simulation_manager.get_slice_origin() ),
    1. );
}
{%- endif %}

{%- if has_continuous_input %}

void {{ neuronName }}::handle(nest::CurrentEvent& e)
{
#ifdef DEBUG
  std::cout << "[neuron " << this << "] {{neuronName}}::handle(CurrentEvent)" << std::endl;
#endif
  assert(e.get_delay_steps() > 0);

  const double current = e.get_current();     // we assume that in NEST, this returns a current in pA
  const double weight = e.get_weight();

{%- for port in neuron.get_continuous_input_ports() %}
  get_{{port.get_symbol_name()}}().add_value(
               e.get_rel_delivery_steps( nest::kernel().simulation_manager.get_slice_origin()),
               weight * current );
{%- endfor %}
}
{%- endif %}

// -------------------------------------------------------------------------
//   Methods corresponding to event handlers
// -------------------------------------------------------------------------

{%- for blk in neuron.get_on_receive_blocks() %}
{%-     set ast = blk.get_block() %}
void
{{ neuronName }}::on_receive_block_{{ blk.get_port_name() }}()
{
  const double __timestep = nest::Time::get_resolution().get_ms();  // do not remove, this is necessary for the timestep() function

{%-     filter indent(2, True) -%}
{%-         include "directives_cpp/Block.jinja2" %}
{%-     endfilter %}
}

{% endfor %}

{%- if paired_synapse is defined %}
// -------------------------------------------------------------------------
//   Methods for neuron/synapse co-generation
// -------------------------------------------------------------------------

inline double
{{ neuronName }}::get_spiketime_ms() const
{
  return last_spike_;
}

void
{{ neuronName }}::register_stdp_connection( double t_first_read, double delay )
{
#ifdef DEBUG
  std::cout << "[neuron " << this << "] {{ neuronName }}::register_stdp_connection()" << std::endl;
#endif

  // Mark all entries in the deque, which we will not read in future as read by
  // this input input, so that we safely increment the incoming number of
  // connections afterwards without leaving spikes in the history.
  // For details see bug #218. MH 08-04-22

  for ( std::deque< histentry__{{ neuronName }} >::iterator runner = history_.begin();
        runner != history_.end() and ( t_first_read - runner->t_ > -1.0 * nest::kernel().connection_manager.get_stdp_eps() );
        ++runner )
  {
    ( runner->access_counter_ )++;
  }

  n_incoming_++;

  max_delay_ = std::max( delay, max_delay_ );
}


void
{{ neuronName }}::get_history__( double t1,
  double t2,
  std::deque< histentry__{{ neuronName }} >::iterator* start,
  std::deque< histentry__{{ neuronName }} >::iterator* finish )
{
#ifdef DEBUG
  std::cout << "[neuron " << this << "] {{ neuronName }}::register_stdp_connection()" << std::endl;
#endif

  *finish = history_.end();
  if ( history_.empty() )
  {
    *start = *finish;
    return;
  }
  std::deque< histentry__{{ neuronName }} >::reverse_iterator runner = history_.rbegin();
  const double t2_lim = t2 + nest::kernel().connection_manager.get_stdp_eps();
  const double t1_lim = t1 + nest::kernel().connection_manager.get_stdp_eps();
  while ( runner != history_.rend() and runner->t_ >= t2_lim )
  {
    ++runner;
  }
  *finish = runner.base();
  while ( runner != history_.rend() and runner->t_ >= t1_lim )
  {
    runner->access_counter_++;
    ++runner;
  }
  *start = runner.base();
}

void
{{ neuronName }}::set_spiketime( nest::Time const& t_sp, double offset )
{
#ifdef DEBUG
  std::cout << "[neuron " << this << "] {{ neuronName }}::set_spiketime()" << std::endl;
#endif

  {{ neuron_parent_class }}::set_spiketime( t_sp, offset );

  const double t_sp_ms = t_sp.get_ms() - offset;

  if ( n_incoming_ )
  {
    // Prune all spikes from history which are no longer needed. Only remove a spike if:
    // - its access counter indicates it has been read out by all connected STDP synapses, and
    // - there is another, later spike, that is strictly more than ``min_global_delay + max_delay_ + eps`` away from the new spike (at ``t_sp_ms``)
    // for access counter logic, see https://www.frontiersin.org/files/Articles/1382/fncom-04-00141-r1/image_m/fncom-04-00141-g003.jpg (Potjans et al. 2010)
    while ( history_.size() > 1 )
    {
      const double next_t_sp = history_[ 1 ].t_;
      // Note that ``access_counter`` is compared to the number of incoming synapses (``n_incoming_``), so that spikes are removed from the history only after they have been read out by each synapse
      // see https://www.frontiersin.org/files/Articles/1382/fncom-04-00141-r1/image_m/fncom-04-00141-g003.jpg (Potjans et al. 2010)

      if ( history_.front().access_counter_ >= n_incoming_
           and t_sp_ms - next_t_sp > max_delay_ + nest::Time::delay_steps_to_ms(nest::kernel().connection_manager.get_min_delay()) + nest::kernel().connection_manager.get_stdp_eps() )
      {
        history_.pop_front();
      }
      else
      {
        break;
      }
    }

    if (history_.size() > 0)
    {
      assert(history_.back().t_ == last_spike_);

{%-     for var in purely_numeric_state_variables_moved|sort %}
      {{ printer.print(utils.get_state_variable_by_name(astnode, var)) }} = history_.back().{{ var }}_;
{%-     endfor %}
{%-     for var in analytic_state_variables_moved|sort %}
      {{ printer.print(utils.get_state_variable_by_name(astnode, var)) }} = history_.back().{{ var }}_;
{%-     endfor %}
    }
    else
    {
{%-     for var in purely_numeric_state_variables_moved|sort %}
      {{ printer.print(utils.get_state_variable_by_name(astnode, var)) }} = {{ utils.initial_value_or_zero(astnode, var) }}; // initial value for convolution is always 0
{%-     endfor %}
{%-     for var in analytic_state_variables_moved|sort %}
      {{ printer.print(utils.get_state_variable_by_name(astnode, var)) }} = {{ utils.initial_value_or_zero(astnode, var) }}; // initial value for convolution is always 0
{%-     endfor %}
    }
{%      if paired_synapse is defined and (purely_numeric_state_variables_moved + analytic_state_variables_moved) | length > 0 %}
    /**
     * update state variables transferred from synapse from `last_spike_` to `t_sp_ms`
     *
     * variables that will be integrated: {{ purely_numeric_state_variables_moved + analytic_state_variables_moved }}
    **/

    const double old___h = V_.__h;
    V_.__h = t_sp_ms - last_spike_;
    if (V_.__h > 1E-12)
    {
      recompute_internal_variables(true);
{%-         filter indent(6, True) -%}
{#              emulate a call to ``integrate_odes(purely_numeric_state_variables_moved + analytic_state_variables_moved)`` #}
{%-             set args = utils.resolve_variables_to_expressions(astnode, purely_numeric_state_variables_moved + analytic_state_variables_moved) %}
{%-             set ast = ASTNodeFactory.create_ast_function_call("integrate_odes", args) %}
{%-             include "directives_cpp/PredefinedFunction_integrate_odes.jinja2" %}
{%-         endfilter %}

      V_.__h = old___h;
      recompute_internal_variables(true);
    }
{%- endif %}
    /**
     * print extra on-emit statements transferred from synapse
    **/

{%-     filter indent(4, True) %}
{%-     for stmt in extra_on_emit_spike_stmts_from_synapse %}
{%-         include "directives_cpp/Statement.jinja2" %}
{%-     endfor %}
{%-     endfilter %}

    /**
     * updates due to convolutions
    **/

{%-     for _, spike_update in post_spike_updates.items() %}
    {{ printer.print(utils.get_variable_by_name(astnode, spike_update.get_variable().get_complete_name())) }} += 1.;
{%-     endfor %}

    /**
     * push back history
    **/

    last_spike_ = t_sp_ms;
    history_.push_back( histentry__{{ neuronName }}( last_spike_
        , 0 // access counter
{%- for var in purely_numeric_state_variables_moved|sort %}
        , get_{{var}}()
{%- endfor %}
{%- for var in analytic_state_variables_moved | sort %}
        , get_{{ var }}()
{%- endfor %}
{%- if state_vars_that_need_continuous_buffering | length > 0 and continuous_state_buffering_method == "post_spike_based" %}
{%-     for var_name_post in state_vars_that_need_continuous_buffering | sort %}
{%-         set var_name = utils.get_var_name_tuples_of_neuron_synapse_pair(continuous_post_ports, var_name_post) %}
        , get_{{ var_name }}()
{%-     endfor %}
{%- endif %}
    ) );
  }
  else
  {
    last_spike_ = t_sp_ms;
  }
}


void
{{ neuronName }}::clear_history()
{
  last_spike_ = -1.0;
  history_.clear();
}


{#
	generate getter functions for the transferred variables
#}

{%- for var in transferred_variables %}
{%- with variable_symbol = transferred_variables_syms[var] %}

{%- if not variable_symbol %}
{%-     set variable_symbol = astnode.get_scope().resolve_to_symbol(var, SymbolKind.VARIABLE) %}
{%- endif %}

{%- if not var == variable_symbol.get_symbol_name() %}
{{ raise('Error in resolving variable to symbol') }}
{%- endif %}

double
{{ neuronName }}::get_{{ var }}( double t, const bool before_increment )
{
#ifdef DEBUG
  std::cout << "[neuron " << this << "] {{ neuronName }}::get_{{var}}: getting value at t = " << t << std::endl;
#endif

  // case when the neuron has not yet spiked
  if ( history_.empty() )
  {
#ifdef DEBUG
    std::cout << "[neuron " << this << "] {{ neuronName }}::get_{{var}}: \thistory empty, returning initial value = " << {{var}}__iv << std::endl;
#endif

    // return initial value
    return {{ var }}__iv;
  }

  // search for the latest post spike in the history buffer that came strictly before `t`
  int i = history_.size() - 1;
  double eps = 0.;
  if ( before_increment )
  {
   eps = nest::kernel().connection_manager.get_stdp_eps();
  }
  while ( i >= 0 )
  {
    if ( t - history_[ i ].t_ >= eps )
    {
#ifdef DEBUG
      std::cout<<"{{ neuronName }}::get_{{ var }}: \tspike occurred at history[i].t_ = " << history_[i].t_ << std::endl;
#endif

{%- for var_ in purely_numeric_state_variables_moved %}
      {{ printer.print(utils.get_variable_by_name(astnode, var_)) }} = history_[ i ].{{var_}}_;
{%- endfor %}
{%- for var_ in analytic_state_variables_moved %}
      {{ printer.print(utils.get_variable_by_name(astnode, var_)) }} = history_[ i ].{{var_}}_;
{%- endfor %}
{%  if paired_synapse is defined and (purely_numeric_state_variables_moved + analytic_state_variables_moved) | length > 0 %}
      /**
       * update state variables transferred from synapse from `history[i].t_` to `t`
       *
       * variables that will be integrated: {{ purely_numeric_state_variables_moved + analytic_state_variables_moved }}
      **/

      if ( t - history_[ i ].t_ >= nest::kernel().connection_manager.get_stdp_eps() )
      {
        const double old___h = V_.__h;
        V_.__h = t - history_[i].t_;
        assert(V_.__h > 0);
        recompute_internal_variables(true);

{#      emulate a call to ``integrate_odes(purely_numeric_state_variables_moved + analytic_state_variables_moved)`` #}
{%- filter indent(8, True) %}
{%-     set args = utils.resolve_variables_to_expressions(astnode, purely_numeric_state_variables_moved + analytic_state_variables_moved) %}
{%-     set ast = ASTNodeFactory.create_ast_function_call("integrate_odes", args) %}
{%-     include "directives_cpp/PredefinedFunction_integrate_odes.jinja2" %}
{%- endfilter %}

        V_.__h = old___h;
        recompute_internal_variables(true);
      }
{%- endif %}
#ifdef DEBUG
      std::cout << "[neuron " << this << "] {{ neuronName }}::get_{{var}}: \treturning " << {{ printer.print(utils.get_variable_by_name(astnode, var)) }} << std::endl;
#endif

      return {{ printer.print(utils.get_variable_by_name(astnode, var)) }};       // type: {{declarations.print_variable_type(variable_symbol)}}
    }
    --i;
  }

  // this case occurs when the trace was requested at a time precisely at that of the first spike in the history
  if ( (!before_increment) and t == history_[ 0 ].t_)
  {
{%- for var_ in purely_numeric_state_variables_moved %}
    {{ printer.print(utils.get_state_variable_by_name(astnode, var_)) }} = history_[ 0 ].{{var_}}_;
{%- endfor %}
{%- for var_ in analytic_state_variables_moved %}
    {{ printer.print(utils.get_state_variable_by_name(astnode, var_)) }} = history_[ 0 ].{{var_}}_;
{%- endfor %}

#ifdef DEBUG
    std::cout << "[neuron " << this << "] {{ neuronName }}::get_{{var}}: \ttrace requested at exact time of history entry 0, returning " << {{ printer.print(utils.get_variable_by_name(astnode, variable_symbol.get_symbol_name())) }} << std::endl;
#endif

    return {{ printer.print(utils.get_variable_by_name(astnode, variable_symbol.get_symbol_name())) }};
  }

  // this case occurs when the trace was requested at a time before the first spike in the history
  // return initial value propagated in time

#ifdef DEBUG
  std::cout << "[neuron " << this << "] {{ neuronName }}::get_{{var}}: \tfall-through, returning initial value = " << {{var}}__iv << std::endl;
#endif

  if (t == 0.)
  {
    return 0.;  // initial value for convolution is always 0
  }

  // set to initial value
{%- for var_ in purely_numeric_state_variables_moved %}
  {{ printer.print(utils.get_state_variable_by_name(astnode, var_)) }} = 0.;  // initial value for convolution is always 0
{%- endfor %}
{%- for var_ in analytic_state_variables_moved %}
  {{ printer.print(utils.get_state_variable_by_name(astnode, var_)) }} = 0.;  // initial value for convolution is always 0
{%- endfor %}
{%-     if paired_synapse is defined and (purely_numeric_state_variables_moved + analytic_state_variables_moved) | length > 0 %}
  /**
   * update state variables transferred from synapse from initial condition to `t`
   *
   * variables that will be integrated: {{ purely_numeric_state_variables_moved + analytic_state_variables_moved }}
  **/

  if (t > 1E-12)
  {
    const double old___h = V_.__h;
    V_.__h = t;   // from time 0 to the requested time
    recompute_internal_variables(true);

{# emulate a call to ``integrate_odes(purely_numeric_state_variables_moved + analytic_state_variables_moved)`` #}
{%- filter indent(8, True) %}
{%-     set args = utils.resolve_variables_to_expressions(astnode, purely_numeric_state_variables_moved + analytic_state_variables_moved) %}
{%-     set ast = ASTNodeFactory.create_ast_function_call("integrate_odes", args) %}
{%-     include "directives_cpp/PredefinedFunction_integrate_odes.jinja2" %}
{%- endfilter %}

    V_.__h = old___h;
    recompute_internal_variables(true);
  }
{%- endif %}
  return {{ printer.print(utils.get_variable_by_name(astnode, var)) }};
}
{%- endwith -%}
{%- endfor %}

{%- endif %}


{%- if use_gap_junctions %}
void {{ neuronName }}::handle( nest::GapJunctionEvent& e )
{
#ifdef DEBUG
  std::cout << "[neuron " << this << "] {{ neuronName }}::handle(GapJunctionEvent)" << std::endl;
#endif

  const double weight = e.get_weight();

  B_.sumj_g_ij_ += weight;

  size_t i = 0;
  std::vector< unsigned int >::iterator it = e.begin();
  // The call to get_coeffvalue( it ) in this loop also advances the iterator it
  while ( it != e.end() )
  {
    B_.interpolation_coefficients[ i ] += weight * e.get_coeffvalue( it );
    ++i;
  }
}
{%- endif %}

{# leave this comment here to ensure newline is generated at end of file -#}
