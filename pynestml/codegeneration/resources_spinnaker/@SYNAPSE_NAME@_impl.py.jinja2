#
#  {{ synapseName }}_impl.py
#
#  This file is part of NEST.
#
#  Copyright (C) 2004 The NEST Initiative
#
#  NEST is free software: you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation, either version 2 of the License, or
#  (at your option) any later version.
#
#  NEST is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
#  You should have received a copy of the GNU General Public License
#  along with NEST.  If not, see <http://www.gnu.org/licenses/>.
#
#  Generated from NESTML {{ nestml_version }} at time: {{ now }}

# Copyright (c) 2015 The University of Manchester
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

from typing import Any, Iterable, List, Optional, Tuple, TYPE_CHECKING

import math

from math import exp # XXX how to support all NESTML functions that can be used in the initiialisation of parameters and internals?

import numpy 
import numpy as np
from numpy import floating, integer, uint8, uint16, uint32
from numpy.typing import NDArray

from pyNN.standardmodels.synapses import StaticSynapse

from spinn_utilities.overrides import overrides

from spinn_front_end_common.interface.ds import DataType
from spinn_front_end_common.interface.ds import DataSpecificationBase
from spinn_front_end_common.utilities.constants import (
    BYTES_PER_WORD, BYTES_PER_SHORT)
from spynnaker.pyNN.data import SpynnakerDataView
from spynnaker.pyNN.exceptions import (
    SynapticConfigurationException, InvalidParameterType)
from spynnaker.pyNN.models.neural_projections.connectors import (
    AbstractConnector)
from spynnaker.pyNN.types import Weight_Types
from spynnaker.pyNN.types import Weight_Delay_In_Types as _In_Types
from spynnaker.pyNN.utilities.utility_calls import get_n_bits
from spynnaker.pyNN.models.neuron.synapse_dynamics.types import (
    NUMPY_CONNECTORS_DTYPE)
from spynnaker.pyNN.models.neuron.synapse_dynamics.abstract_plastic_synapse_dynamics import AbstractPlasticSynapseDynamics
from spynnaker.pyNN.models.neuron.synapse_dynamics.abstract_synapse_dynamics_structural import (
    AbstractSynapseDynamicsStructural)
from spynnaker.pyNN.models.neuron.synapse_dynamics.abstract_generate_on_machine import (
    AbstractGenerateOnMachine, MatrixGeneratorID)
from spynnaker.pyNN.models.neuron.synapse_dynamics.synapse_dynamics_neuromodulation import SynapseDynamicsNeuromodulation
from spynnaker.pyNN.models.neuron.synapse_dynamics.synapse_dynamics_weight_changable import SynapseDynamicsWeightChangable
from spynnaker.pyNN.models.neuron.synapse_dynamics.synapse_dynamics_weight_changer import SynapseDynamicsWeightChanger

if TYPE_CHECKING:
    from spynnaker.pyNN.models.neural_projections import (
        ProjectionApplicationEdge, SynapseInformation)
    from spynnaker.pyNN.models.neuron.synapse_dynamics.types import (
        ConnectionsArray)
    from spynnaker.pyNN.models.neuron.synapse_io import MaxRowInfo
    from .abstract_synapse_dynamics import AbstractSynapseDynamics

# How large are the time-stamps stored with each event
TIME_STAMP_BYTES = BYTES_PER_WORD



def float_to_s16_15(x: float) -> int:
    """Converts a float to a Q16.15 fixed-point 32-bit integer.

    The S16.15 format uses 1 sign bit, 16 integer bits, and 15
    fractional bits.

    Args:
        float_val: The floating-point number to convert.

    Returns:
        The 32-bit signed integer representing the fixed-point number.
    """

    sign_bit: int = 1 if np.sign(x) == -1 else 0
    integer_bits: int = int(np.abs(x))
    fractional_bits: int = int((np.abs(x) - int(np.abs(x))) * 2**15)

    fixp_x: int = (sign_bit << 31) + (integer_bits << 15) + fractional_bits

    return fixp_x




class {{ synapseName }}Dynamics(AbstractPlasticSynapseDynamics):
    """
    The dynamics of a synapse that changes over time using a
    Spike Timing Dependent Plasticity (STDP) rule.
    """

    """__slots__ = (
        # Fraction of delay that is dendritic (instead of axonal or synaptic)
        "__dendritic_delay_fraction",
        # The neuromodulation instance if enabled
        "__neuromodulation",
        # padding to add to a synaptic row for synaptic rewiring
        "__pad_to_length",
        # Whether to use back-propagation delay or not
        "__backprop_delay")"""


    def _init_nestml_model_variables(self):
        timestep = 1.   # XXX: hard-coded for now!
        self._nestml_model_variables = {}
{%- for sym in synapse.get_parameter_symbols() + synapse.get_internal_symbols() %}
        self._nestml_model_variables["{{ sym.get_symbol_name() }}"] = {{ printer.print(sym.get_declaring_expression()) }}  # type: {{ sym.get_type_symbol().print_symbol() }}
{%- endfor %}

    def __init__(
            self, 
            voltage_dependence: None = None,
            dendritic_delay_fraction: float = 1.0,
            weight: _In_Types = StaticSynapse.default_parameters['weight'],
            delay: _In_Types = None, pad_to_length: Optional[int] = None,
            backprop_delay: bool = True):
        """
        :param None voltage_dependence: not supported
        :param float dendritic_delay_fraction: must be 1.0!
        :param float weight:
        :param delay: Use ``None`` to get the simulator default minimum delay.
        :type delay: float or None
        :param pad_to_length:
        :type pad_to_length: int or None
        :param bool backprop_delay:
        """
        print("Initialising the synapse with weight = " + str(weight))
        if voltage_dependence is not None:
            raise NotImplementedError(
                "Voltage dependence has not been implemented")
        super().__init__(delay=delay, weight=weight)
        self.__dendritic_delay_fraction = float(dendritic_delay_fraction)
        self.__pad_to_length = pad_to_length
        self.__backprop_delay = backprop_delay
        self.__neuromodulation: Optional[SynapseDynamicsNeuromodulation] = None

        self._init_nestml_model_variables()

        if self.__dendritic_delay_fraction != 1.0:
            raise NotImplementedError("All delays must be dendritic!")

    def _merge_neuromodulation(
            self, neuromodulation: SynapseDynamicsNeuromodulation) -> None:
        if self.__neuromodulation is None:
            self.__neuromodulation = neuromodulation
        elif not self.__neuromodulation.is_neuromodulation_same_as(
                neuromodulation):
            raise SynapticConfigurationException(
                "Neuromodulation must match exactly when using multiple"
                " edges to the same Population")

    @overrides(AbstractPlasticSynapseDynamics.merge)
    def merge(self, synapse_dynamics: AbstractSynapseDynamics
              ) -> AbstractSynapseDynamics:

        # If dynamics is Neuromodulation, merge with other neuromodulation,
        # and then return ourselves, as neuromodulation can't be used by
        # itself
        if isinstance(synapse_dynamics, SynapseDynamicsNeuromodulation):
            self._merge_neuromodulation(synapse_dynamics)
            return self

        """# If dynamics is STDP, test if same as
        if isinstance(synapse_dynamics, SynapseDynamicsSTDP):
            if not self.is_same_as(synapse_dynamics):
                raise SynapticConfigurationException(
                    "Synapse dynamics must match exactly when using multiple"
                    " edges to the same population")

            if self.__neuromodulation is not None:
                # pylint: disable=protected-access
                synapse_dynamics._merge_neuromodulation(self.__neuromodulation)

            # If STDP part matches, return the other, as it might also be
            # structural
            return synapse_dynamics

        # If dynamics is structural but not STDP (as here), merge
        # NOTE: Import here as otherwise we get a circular dependency
        # pylint: disable=import-outside-toplevel
        from .synapse_dynamics_structural_stdp import (
            SynapseDynamicsStructuralSTDP)
        if isinstance(synapse_dynamics, AbstractSynapseDynamicsStructural):
            assert False, "Not supported yet"
        """

        # Otherwise, it is static or neuromodulation, so return ourselves
        return self

    @overrides(AbstractPlasticSynapseDynamics.get_value)
    def get_value(self, key: str) -> Any:
        for obj in [self]:
            if hasattr(obj, key):
                return getattr(obj, key)
        raise InvalidParameterType(
            f"Type {type(self)} does not have parameter {key}")

    @overrides(AbstractPlasticSynapseDynamics.set_value)
    def set_value(self, key: str, value: Any) -> None:
        for obj in [self]:
            if hasattr(obj, key):
                setattr(obj, key, value)
                SpynnakerDataView.set_requires_mapping()
                return
        raise InvalidParameterType(
            f"Type {type(self)} does not have parameter {key}")

    @property
    def dendritic_delay_fraction(self) -> float:
        """
        Settable.

        :rtype: float
        """
        return self.__dendritic_delay_fraction

    @dendritic_delay_fraction.setter
    def dendritic_delay_fraction(self, new_value: float) -> None:
        self.__dendritic_delay_fraction = new_value

    @property
    def backprop_delay(self) -> bool:
        """
        Settable.

        :rtype: bool
        """
        return self.__backprop_delay

    @backprop_delay.setter
    def backprop_delay(self, backprop_delay: bool) -> None:
        self.__backprop_delay = bool(backprop_delay)

    @property
    def neuromodulation(self) -> Optional[SynapseDynamicsNeuromodulation]:
        """
        :rtype: SynapseDynamicsNeuromodulation
        """
        return self.__neuromodulation

    @overrides(AbstractPlasticSynapseDynamics.is_same_as)
    def is_same_as(self, synapse_dynamics: AbstractSynapseDynamics) -> bool:
        raise NotImplementedError()

        if not isinstance(synapse_dynamics, SynapseDynamicsSTDP):
            return False
        return self.__dendritic_delay_fraction == synapse_dynamics.dendritic_delay_fraction

    def get_vertex_executable_suffix(self) -> str:
        """
        :rtype: str
        """
        name = ""

        return name

    def get_parameters_sdram_usage_in_bytes(
            self, n_neurons: int, n_synapse_types: int) -> int:
        """
        :param int n_neurons:
        :param int n_synapse_types:
        :rtype: int
        """
        n_parameters = {{ synapse.get_parameter_symbols() | length }}
        size = n_parameters * BYTES_PER_WORD * n_synapse_types

        if self.__neuromodulation:
            size += self.__neuromodulation.get_parameters_sdram_usage_in_bytes(
                n_neurons, n_synapse_types)

        print("[NESTML synapse] get_parameters_sdram_usage_in_bytes(n_neurons = " + str(n_neurons) + ", n_synapse_types = " + str(n_synapse_types) + ") = " + str(size) + " bytes")

        return size

    @overrides(AbstractPlasticSynapseDynamics.write_parameters)
    def write_parameters(
            self, spec: DataSpecificationBase, region: int,
            global_weight_scale: float,
            synapse_weight_scales: NDArray[floating]) -> None:


        i = 0


        spec.comment("Writing Plastic Parameters")

        # Switch focus to the region:
        spec.switch_write_focus(region)

{%- for sym in synapse.get_parameter_symbols() + synapse.get_internal_symbols() %}
        # write value for parameter "{{ sym.get_symbol_name() }}"
        print("XXXXXXXXX writing value for parameter {{ sym.get_symbol_name() }} ======= " + str(self._nestml_model_variables["{{ sym.get_symbol_name() }}"]) + " ==== encoded --> " + str(hex(float_to_s16_15(self._nestml_model_variables["{{ sym.get_symbol_name() }}"]))))
        spec.write_value(
                data=float_to_s16_15(self._nestml_model_variables["{{ sym.get_symbol_name() }}"]), # int(i),  # XXX should be actual data; use int(i) for testing
                data_type=DataType.INT32)
        i += 1
{%- endfor %}
                
    @overrides(AbstractPlasticSynapseDynamics.get_n_words_for_plastic_connections)
    def get_n_words_for_plastic_connections(self, n_connections: int) -> int:
        """
        Get the number of 32-bit words for `n_connections` in a single row.
        :param n_connections:
        """
        n_header_words_per_row = 2   # XXX timestamp and pre trace at that time?
        #n_header_words_per_row = 0
        
        n_state_variables = {{ synapse.get_state_symbols() | length }}    # number of state variables per connection (i.e. for a single synapse model)

        pp_size_words = n_state_variables * n_connections
        fp_size_words = n_connections
        print("[NESTML synapse] get_n_words_for_plastic_connections(n_connections = " + str(n_connections) + "): returning " + str(n_header_words_per_row + pp_size_words + fp_size_words))

        return n_header_words_per_row + pp_size_words + fp_size_words

    @overrides(AbstractPlasticSynapseDynamics.get_plastic_synaptic_data)
    def get_plastic_synaptic_data(
            self, connections: ConnectionsArray,
            connection_row_indices: NDArray[integer], n_rows: int,
            n_synapse_types: int,
            max_n_synapses: int, max_atoms_per_core: int) -> Tuple[
                List[NDArray[uint32]], List[NDArray[uint32]],
                NDArray[uint32], NDArray[uint32]]:
        """
        Convert from a list of tuples (source, target, weight, delay) into raw memory (``pp_data`` and ``fp_data``).

        The data is returned once for each row.

        Fixed data words are 16-bit ("half-word"/"short"). Typically: 3 bits of padding; 4 bits of delay; 1 bit of type; 8 bits of neuron ID.

        The row into which connection should go is given by `connection_row_indices`, and the total number of rows is given by `n_rows`.

        Lengths are returned as an array made up of an integer for each row, for each of the fixed-plastic and plastic-plastic regions.

        :param ~numpy.ndarray connections: The connections to get data for
        :param ~numpy.ndarray connection_row_indices:
            The row into which each connection should go
        :param int n_rows: The total number of rows
        :param int n_synapse_types: The number of synapse types
        :param int max_n_synapses: The maximum number of synapses to generate
        :param int max_atoms_per_core: The maximum number of atoms on a core
        :return: (fp_data (2D), pp_data (2D), fp_size (1D), pp_size (1D))
        :rtype:
            tuple(~numpy.ndarray, ~numpy.ndarray, ~numpy.ndarray,
            ~numpy.ndarray)
        """
        n_connections = len(connections)

        print("[NESTML synapse] get_plastic_synaptic_data(n_connections = " + str(n_connections) + ")")

        n_synapse_type_bits = get_n_bits(n_synapse_types)
        n_neuron_id_bits = get_n_bits(max_atoms_per_core)
        neuron_id_mask = (1 << n_neuron_id_bits) - 1

        #
        # Get the fixed data words
        #

        fixed_plastic = (
            (connections["delay"].astype(uint16) << (n_neuron_id_bits + n_synapse_type_bits))
            | (connections["synapse_type"].astype(uint16) << n_neuron_id_bits)
            | (connections["target"].astype(uint16) & neuron_id_mask))
        fixed_plastic_rows = self.convert_per_connection_data_to_rows(
            connection_row_indices, n_rows,
            fixed_plastic.view(dtype=uint8).reshape((-1, BYTES_PER_SHORT)),
            max_n_synapses)
        fp_size = self.get_n_items(fixed_plastic_rows, BYTES_PER_SHORT)
        if self.__pad_to_length is not None:
            # Pad the data
            fixed_plastic_rows = self._pad_row(fixed_plastic_rows, BYTES_PER_SHORT)

        fp_data = self.get_words(fixed_plastic_rows)

        #
        # Get the plastic-plastic data words
        #

        n_state_variables = {{ synapse.get_state_symbols() | length }}    # number of state variables per connection (i.e. for a single synapse model)

        # If neuromodulation...
        if self.__neuromodulation:
            raise Exception("Not supported!")

        plastic_plastic = numpy.zeros(n_connections * n_state_variables, dtype=uint32)
        for connection_idx in range(len(connections)):
{%- for state_var_sym in synapse.get_state_symbols() %}
{%-     set state_var_idx = loop.index0 %}
{%-     if state_var_sym.name == "w" %}  {# XXXX use codegen opts for "w" instead #}
            plastic_plastic[connection_idx * n_state_variables + {{ state_var_idx }}] = connections["weight"][connection_idx]
{%-     else %}
            # XXX: what about other initial values, can they be put into/extracted from ConnectionsArray? Otherwise, do we use printing initial values directly from the NESTML model?
            plastic_plastic[connection_idx * n_state_variables + {{ state_var_idx }}] = 0
{%-     endif %}
{%- endfor %}

        # Convert the plastic data into groups of bytes per connection and then into rows
        plastic_plastic_bytes = plastic_plastic.view(dtype=uint8).reshape((-1, 4 * n_state_variables))    # 4 bytes per word
        plastic_plastic_row_data = self.convert_per_connection_data_to_rows(connection_row_indices, n_rows, plastic_plastic_bytes, max_n_synapses)

        # pp_size = fp_size in words => fp_size * no_bytes / 4 (bytes)
        if self.__pad_to_length is not None:
            # Pad the data
            print("[NESTML synapse] Exception: padding rows not supported!")
            raise Exception("Padding rows not supported!") # XXX can we get away with this?
            #plastic_plastic_row_data = self._pad_row(
            #    plastic_plastic_row_data, bytes_per_connection=4 * n_state_variables)

        plastic_headers: np.ndarray[uint8] = numpy.zeros((n_rows, 8), dtype=uint8)   # timestamp and trace at that time -- measured in bytes (4 bytes per word!)
        plastic_plastic_rows: np.ndarray[uint8] = [numpy.concatenate((plastic_headers[i, :], plastic_plastic_row_data[i])) for i in range(n_rows)]
        pp_size = self.get_n_items(plastic_plastic_rows, BYTES_PER_WORD)
        pp_data = self.get_words(plastic_plastic_rows)

        print("[NESTML synapse] \treturning fp_data = " + str(fp_data) + ", pp_data = " + str(pp_data) + ", fp_size = " + str(fp_size) + ", pp_size = " + str(pp_size))
        #import pdb;pdb.set_trace()

        return fp_data, pp_data, fp_size, pp_size

    def _pad_row(self, rows: List[NDArray],
                 no_bytes_per_connection: int) -> List[NDArray]:
        """
        :param list(~numpy.ndarray) rows:
        :param int no_bytes_per_connection:
        :rtype: list(~numpy.ndarray)
        """
        raise NotImplementedException
        pad_len = self.__pad_to_length or 1
        # Row elements are (individual) bytes
        return [
            numpy.concatenate((
                row, numpy.zeros(
                    numpy.clip(
                        no_bytes_per_connection * pad_len - row.size,
                        0, None)).astype(dtype=uint8))
                ).view(dtype=uint8)
            for row in rows]

    @overrides(
        AbstractPlasticSynapseDynamics.get_n_plastic_plastic_words_per_row)
    def get_n_plastic_plastic_words_per_row(
            self, pp_size: NDArray[uint32]) -> NDArray[integer]:
        # pp_size is in words, so return
        return pp_size

    @overrides(
        AbstractPlasticSynapseDynamics.get_n_fixed_plastic_words_per_row)
    def get_n_fixed_plastic_words_per_row(
            self, fp_size: NDArray[uint32]) -> NDArray[integer]:
        # fp_size is in half-words
        return numpy.ceil(fp_size / 2.0).astype(dtype=uint32)

    @overrides(AbstractPlasticSynapseDynamics.get_n_synapses_in_rows)
    def get_n_synapses_in_rows(self, pp_size: NDArray[uint32],
                               fp_size: NDArray[uint32]) -> NDArray[integer]:
        # Each fixed-plastic synapse is a half-word and fp_size is in half
        # words so just return it
     #3333   return fp_size
         print("[NESTML synapse] get_n_synapses_in_rows()")
         raise NotImplementedError()

    @overrides(AbstractPlasticSynapseDynamics.read_plastic_synaptic_data)
    def read_plastic_synaptic_data(
            self,
            n_synapse_types: int,
            pp_size: NDArray[uint32],
            pp_data: List[NDArray[uint32]],
            fp_size: NDArray[uint32],
            fp_data: List[NDArray[uint32]],
            max_atoms_per_core: int) -> ConnectionsArray:
        """
        Convert from raw memory (``pp_data`` and ``fp_data``) into a list of tuples (source, target, weight, delay).
        """
        print("[NESTML synapse] read_plastic_synaptic_data")
        n_rows = len(fp_size)

        n_synapse_type_bits = get_n_bits(n_synapse_types)
        n_neuron_id_bits = get_n_bits(max_atoms_per_core)
        neuron_id_mask = (1 << n_neuron_id_bits) - 1

        # the header size
        n_header_words = 2

        data_fixed = numpy.concatenate([fp_data[i].view(dtype=uint16)[0:fp_size[i]] for i in range(n_rows)])
        pp_without_headers = [row.view(dtype=uint8)[(4*n_header_words):] for row in pp_data]

        if self.__neuromodulation:
            raise Exception("Neuromodulation not supported!")

        """pp_half_words = numpy.concatenate([
            pp[:size * n_half_words * BYTES_PER_SHORT].view(uint16)[
                half_word::n_half_words]
            for pp, size in zip(pp_without_headers, fp_size)])"""

        n_state_variables = {{ synapse.get_state_symbols() | length }}    # number of state variables per connection (i.e. for a single synapse model)

        connections = numpy.zeros(data_fixed.size, dtype=NUMPY_CONNECTORS_DTYPE)
        connections["source"] = numpy.concatenate([numpy.repeat(i, fp_size[i]) for i in range(len(fp_size))])
        connections["target"] = data_fixed & neuron_id_mask
        connections["weight"] = numpy.concatenate([pp[:size * 4 * n_state_variables].view(uint32)[::n_state_variables] for pp, size in zip(pp_without_headers, fp_size)])
        connections["delay"] = data_fixed >> (n_neuron_id_bits + n_synapse_type_bits)

        #import pdb;pdb.set_trace()
        print("[NESTML synapse] Returning connections: " + str(connections))

        return connections

    @overrides(AbstractPlasticSynapseDynamics.get_weight_mean)
    def get_weight_mean(self, connector: AbstractConnector,
                        synapse_info: SynapseInformation) -> float:
        # Because the weights could all be changed to the maximum, the mean
        # has to be given as the maximum for scaling
        return self.get_weight_maximum(connector, synapse_info)

    @overrides(AbstractPlasticSynapseDynamics.get_weight_variance)
    def get_weight_variance(
           self, connector: AbstractConnector, weights: Weight_Types,
            synapse_info: SynapseInformation) -> float:
        # Because the weights could all be changed to the maximum, the variance
        # has to be given as no variance
        return 0.0

    @overrides(AbstractPlasticSynapseDynamics.get_weight_maximum)
    def get_weight_maximum(self, connector: AbstractConnector,
                           synapse_info: SynapseInformation) -> float:
        w_max = super().get_weight_maximum(connector, synapse_info)
        # The maximum weight is the largest that it could be set to from
        # the weight dependence
        return max(w_max, 9999)  # XXX ???

    @overrides(AbstractPlasticSynapseDynamics.get_parameter_names)
    def get_parameter_names(self) -> Iterable[str]:
{%- for variable_symbol in synapse.get_parameter_symbols() %}
{%-     set variable = utils.get_parameter_variable_by_name(astnode, variable_symbol.get_symbol_name()) %}
{%-     set isHomogeneous = PyNestMLLexer["DECORATOR_HOMOGENEOUS"] in variable_symbol.get_decorators() %}
{%-     if not isHomogeneous and variable.get_name() != synapse_weight_variable and variable.get_name() != synapse_delay_variable %}
        yield '{{ variable.name }}'
{%-     endif %}
{%- endfor %}

    @overrides(AbstractPlasticSynapseDynamics.get_max_synapses)
    def get_max_synapses(self, n_words: int) -> int:
        """
        Get the maximum number of synapses that can be held in the given number of words.

        :param n_words: The number of words the synapses must fit in
        """
        # Subtract the header size that will always exist
        n_header_words = 2

        # Get plastic plastic size per connection
        n_state_variables = {{ synapse.get_state_symbols() | length }}    # number of state variables per connection (i.e. for a single synapse model)

        if self.__neuromodulation:
            raise Exception("Neuromodulation not supported!")

        # The fixed plastic size per connection is 2 bytes
        bytes_per_fp = BYTES_PER_SHORT

        # n_words = 2 + n_connections * (.5 + n_state_variables)

        # Maximum possible connections, ignoring word alignment
        import math
        n_connections = int(math.floor((n_words - 2) / (.5 + n_state_variables)))

        #check_length_padded = False

        # Reduce until correct
        #while (self.__get_n_connections(n_connections, check_length_padded) > n_words):
        #    n_connections -= 1

        return n_connections

    @property
    @overrides(AbstractPlasticSynapseDynamics.changes_during_run)
    def changes_during_run(self) -> bool:
        return True

    @property
    @overrides(AbstractPlasticSynapseDynamics.is_combined_core_capable)
    def is_combined_core_capable(self) -> bool:
        return self.__neuromodulation is None

    @property
    @overrides(AbstractPlasticSynapseDynamics.is_split_core_capable)
    def is_split_core_capable(self) -> bool:
        return False

    @property
    @overrides(AbstractPlasticSynapseDynamics.pad_to_length)
    def pad_to_length(self) -> Optional[int]:
        return self.__pad_to_length

    @property
    @overrides(AbstractPlasticSynapseDynamics.synapses_per_second)
    def synapses_per_second(self) -> int:
        # From Synapse-Centric Mapping of Cortical Models to the SpiNNaker
        # Neuromorphic Architecture
        return 1400000
